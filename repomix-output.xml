This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
datasets/
  __init__.py
  base.py
  ns2d.py
docs/
  DESIGN_SPEC.md
forecastors/
  __init__.py
  base.py
models/
  base/
    __init__.py
    attention.py
    embedding.py
    mlp.py
    utils.py
  fno/
    __init__.py
    fno.py
    spectral_conv.py
  galerkin_transformer/
    __init__.py
    basic.py
    galerkin_transformer.py
  gnot/
    __init__.py
    basic.py
    gnot.py
  lsm/
    __init__.py
    basic.py
    geo_projection.py
    lsm.py
    neural_spectral_block.py
    unet_block.py
  m2no/
    __init__.py
    grid_operator.py
    m2no1d.py
    m2no2d.py
    utils.py
  mlp/
    __init__.py
    mlp.py
  ono/
    __init__.py
    basic.py
    embedding.py
    ono.py
  swin_transformer/
    __init__.py
    swin_mlp.py
    swin_transformer_v2.py
  transformer/
    __init__.py
    transformer.py
  transolver/
    __init__.py
    basic.py
    physics_attention.py
    transolver.py
  unet/
    __init__.py
    unet1d.py
    unet2d.py
    unet3d.py
  __init__.py
template/
  config/
    base.yaml
    ns2d.yaml
trainers/
  __init__.py
  base.py
utils/
  __init__.py
  ddp.py
  helper.py
  loss.py
  metrics.py
  normalizer.py
  rollout.py
  vis.py
.gitignore
config.py
main.py
README.md
train_ddp.sh
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="docs/DESIGN_SPEC.md">
# NeuralFramework Design Specification (Agent Contract)

Version: 1.1  
Audience: Codex/Agent contributors  
Scope: PDE-centric learning framework (one-step, rollout, SR, diffusion; grid/point/graph modalities)

This document defines **non-negotiable constraints** for the codebase.  
All changes MUST comply with this spec unless the spec itself is updated.

---

## 0. Terminology

- **BNC**: tensor layout `(B, N, C)`
  - `B`: batch size
  - `N`: number of spatial samples (flattened)
  - `C`: channels/features (history packed into channels)
- **Spatial shape**: original grid/geometry shape (e.g., `(H, W)` or `(D, H, W)`), stored as metadata.
- **Coords**: geometric coordinates, logically per-sample; physically may be shared across batch.
- **Geom**: geometry metadata dictionary (mesh/graph/masks/etc.).
- **Task**: one-step / rollout / SR / diffusion, each with an explicit contract.

---

## 1. Core Design Principles (MUST)

### 1.1 Canonical layout for learning signals
All processed learning signals MUST be **BNC**:
- dataset outputs: `x`, `y` are BNC after collation
- trainer forward/loss: consumes BNC
- metrics input: consumes BNC (may unflatten internally)

Any other layout (e.g., `BCHW`, `BHWC`) is allowed ONLY **inside a model** or inside metric/visualization utilities, and MUST be converted back to BNC at the boundary.

### 1.2 Explicit contracts over implicit inference
- No “guessing” semantics by slicing channels (e.g., assuming coords are last channels).
- No reliance on heuristics such as `y.ndim` to select task logic.
- All feature packing and task semantics MUST be declared via `meta`.

### 1.3 Minimal coupling
- Trainer MUST NOT contain dataset-/model-specific slicing or reshaping logic (except standardized canonicalization utilities in §3).
- Dataset MUST NOT embed model-specific hacks.
- Metrics/visualization MUST be pure functions (no side effects).

### 1.4 Type safety (Pylance strict)
All code MUST be Pylance-compliant under strict settings:
- no implicit `Any` propagation
- no missing methods / protocol violations
- no unsafe `Optional` access
- no dynamic attribute injection

### 1.5 Documentation rules
- All docstrings/comments MUST be **English**.
- Docstrings MUST be **concise** (purpose + args/returns only).
- Inline comments MUST be rare and only for non-obvious reasoning.

---

## 2. Data Contract (MUST)

### 2.1 Dataset output format (per batch)
Each dataset batch MUST be a dictionary:

```python
from typing import Any, TypedDict
import torch

class Batch(TypedDict):
    x: torch.Tensor                     # (B, N, Cx)
    y: torch.Tensor                     # (B, N, Cy)
    coords: torch.Tensor | None         # see §2.4
    geom: dict[str, Any] | None         # see §2.5
    meta: dict[str, Any]                # required, see §2.2
````

Single-sample `__getitem__` may omit the batch dimension, but the collate MUST output:

* `x` as `(B, N, Cx)`
* `y` as `(B, N, Cy)`

### 2.2 Required metadata fields

`meta` MUST include:

* `layout: Literal["BNC"]` (always `"BNC"`)
* `spatial_shape: tuple[int, ...] | None`

  * grid: `(H,)`, `(H, W)`, `(D, H, W)`
  * point/graph: may be `None` if not applicable
* `in_t: int`, `out_t: int`
* `field_channels_x: int` (physical channels per timestep, excluding coords unless appended)
* `field_channels_y: int`
* `feature_packing: dict[str, Any]` (see §2.3)
* `coords_shared: bool` (see §2.4)

### 2.3 Feature packing rules (history into C)

Inputs MUST pack temporal history into channels:

* If the field has `F = field_channels_x` channels and history length is `in_t`,
  then the history-packed field part has `F * in_t` channels.

`meta["feature_packing"]` MUST encode:

* `field_dim: int` (F)
* `history: int` (in_t)
* `coords_in_x: bool` (default False, see §2.4)
* `order: list[str]` describing channel composition, e.g.:

  * `["field_history"]`
  * `["field_history", "extra_features"]`
  * `["field_history", "coords"]` (only if `coords_in_x = True`)

### 2.4 Coordinates handling (UPDATED)

Coords are logically per-sample but may be physically shared to reduce memory.

Allowed `coords` shapes:

* `(N, d)` shared across batch
* `(1, N, d)` shared across batch (explicit shared batch dimension)
* `(B, N, d)` per-sample coords

Rules:

* Default: `coords_in_x = False` and `coords` provided separately.
* If a dataset appends coords into `x`, it MUST set `coords_in_x = True` in `feature_packing`.
* Trainer and models MUST NOT assume coords are appended unless `coords_in_x = True`.

`meta["coords_shared"]` MUST be:

* `True` if coords are shared (`(N,d)` or `(1,N,d)`)
* `False` if coords are per-sample (`(B,N,d)`)

### 2.5 Geometry (`geom`) contract

`geom` is optional and modality-dependent (grid/mesh/graph). It MUST be a plain dictionary.

Allowed contents (examples):

* masks, boundary flags, land/sea masks
* mesh connectivity
* graph `edge_index` / `edge_attr`
* per-node/element attributes

Constraints:

* Keys MUST be strings.
* Values SHOULD be tensors or JSON-serializable objects.
* Datasets MUST document `geom` fields concisely in a docstring.

---

## 3. Canonicalization Utilities (MUST)

Centralized utilities MUST exist (no ad-hoc reshapes scattered across trainers/models).

### 3.1 Flatten / unflatten

Provide:

* `flatten_grid(u: Tensor, spatial_shape) -> Tensor` to produce BNC
* `unflatten_grid(u_bnc: Tensor, spatial_shape) -> Tensor` to restore grid layout for models/metrics/visualization

Constraints:

* Flattening order MUST be consistent (row-major/C-order).
* Unflatten MUST be the exact inverse.

### 3.2 Coords canonicalization (UPDATED)

Provide a single utility:

```python
def canonicalize_coords(
    coords: torch.Tensor | None,
    batch_size: int,
) -> torch.Tensor | None:
    """Normalize coords to shape (B, N, d) using expand when shared."""
```

Rules:

* `coords is None` -> `None`
* `(N, d)` -> `(1, N, d)` -> expand to `(B, N, d)`
* `(1, N, d)` -> expand to `(B, N, d)`
* `(B, N, d)` -> unchanged
* otherwise -> raise `ValueError` with clear expected/actual shapes

Important:

* Use `expand` (not `repeat`) for shared coords to avoid memory blow-up.

### 3.3 Model internal layout conversions

If a model needs `BCHW/BHWC`, it MUST:

* use shared utilities to unflatten/reshape
* validate `N == prod(spatial_shape)` when `spatial_shape` is not None
* return BNC after computation

---

## 4. Model Contract (MUST)

### 4.1 Forward signature (no signature introspection)

All models MUST implement:

```python
def forward(
    self,
    x: torch.Tensor,                           # (B, N, Cx)
    *,
    coords: torch.Tensor | None = None,         # canonicalized to (B, N, d) by Trainer
    geom: dict[str, Any] | None = None,
    meta: dict[str, Any] | None = None,
) -> torch.Tensor:                             # (B, N, Cy)
    ...
```

Constraints:

* `coords`, `geom`, `meta` MUST be keyword-only.
* Models MUST return BNC.
* Models MUST NOT rely on `inspect.signature`-based injection logic.

### 4.2 Config-driven behavior

* All optional behavior (e.g., `use_coords`) MUST be explicit via config.
* If `meta["feature_packing"]["coords_in_x"] == True`, models MUST NOT append coords again.

### 4.3 Internal reshape policy

Grid-based models (UNet/FNO/MG-style) MUST:

* read `spatial_shape` from `meta`
* validate compatibility
* unflatten -> compute -> flatten back

---

## 5. Task Semantics (MUST at boundary)

The trainer MUST separate task modes by an explicit flag, e.g. `data.task`.

### 5.1 One-step

* Input: `x` (B, N, Cx)
* Target: `y` (B, N, Cy)
* Output: `y_hat` (B, N, Cy)

### 5.2 Rollout (autoregressive)

Rollout MUST be defined by:

* `in_t`, `out_t`, and `rollout_steps` in config/meta
* explicit history update policy using `feature_packing`

Constraints:

* Rollout MUST NOT assume coords are embedded in `x`.
* History update MUST slice field-history channels using `field_dim` and `history`.
* Coords/extra features MUST remain static unless declared otherwise.

### 5.3 Super-resolution (SR)

SR MUST define:

* `spatial_shape_in`, `spatial_shape_out` in `meta`
* mapping strategy handled by task/model (not by Trainer heuristics)

---

## 6. Trainer Contract (MUST)

### 6.1 Responsibilities

Trainer MUST handle:

* DDP setup
* optimizer/scheduler
* checkpointing/logging
* calling model with standardized batch dict
* loss and metrics aggregation

Trainer MUST NOT:

* reshape `x/y` into BCHW/BHWC for model convenience
* contain dataset-specific slicing rules
* embed visualization

### 6.2 Batch-to-model call

Trainer MUST:

1. canonicalize `coords` via `canonicalize_coords`
2. call model as:

```python
coords = canonicalize_coords(batch["coords"], batch["x"].shape[0])
y_hat = model(batch["x"], coords=coords, geom=batch["geom"], meta=batch["meta"])
```

All shape mismatches MUST raise `ValueError` with:

* actual shape
* expected shape
* dataset/model name if available

---

## 7. Metrics and Visualization (MUST)

### 7.1 Metrics API

Metrics MUST accept:

* `pred: (B, N, C)`
* `target: (B, N, C)`
* `meta`

If spatial restoration is needed, metrics MUST use `unflatten_grid` with `meta["spatial_shape"]`.

### 7.2 No training-step visualization

Visualization utilities MUST be called outside the training step and MUST not mutate tensors/meta.

---

## 8. Code Quality and Pylance Rules (MUST)

### 8.1 Type hints everywhere

* All public functions/methods MUST be fully annotated.
* Avoid `Any`; allow it only for boundary dicts (`geom`, `meta`) and keep it localized.
* Prefer `TypedDict`, `Protocol`, `dataclass` for stable interfaces.

### 8.2 Disallowed patterns

* monkey patching
* dynamic attribute injection
* signature introspection to decide call arguments
* ambiguous containers mixing unrelated types without typing

### 8.3 Error handling

* No bare `except:`
* No silent fallbacks for shape/layout mismatches

---

## 9. Documentation Rules (MUST)

* English only.
* Concise docstrings (purpose + args/returns).
* Minimal inline comments.

---

## 10. Change Management (MUST)

If a change affects configs/checkpoints:

* update templates
* add a short migration note in `docs/MIGRATION.md`
* add at least a minimal smoke test:

  * 1 epoch on tiny subset
  * shape utility inverse test (flatten/unflatten)

---

## Appendix A: Recommended File Additions (SHOULD)

* `core/types.py` for `Batch` typing
* `core/shapes.py` for flatten/unflatten + canonicalize_coords
* `tasks/` for one-step / rollout / SR / diffusion task logic
* `tests/test_shapes.py` for shape invariants
</file>

<file path="forecastors/__init__.py">
# forecastors/__init__.py
from .base import BaseForecaster

__all__ = ["BaseForecaster"]
</file>

<file path="models/base/attention.py">
# models/base/attention.py
from typing import Optional, Tuple

import torch
from torch import nn, Tensor
import torch.nn.functional as F

from .embedding import RotaryEmbedding1D, rotary_pos_embedding


# ============================================================
# Rotary Positional Embedding (RoPE)
# ============================================================
class RoPE1DAdapter(nn.Module):
    """
    1D RoPE adapter for a single coordinate axis (e.g. x or t).

    Assumes head_dim is even:
        q, k: (B, num_heads, N, head_dim)
        coords: (B, N)
    """

    def __init__(self, head_dim: int, base: float = 10000.0, scale: float = 1.0) -> None:
        super().__init__()
        assert head_dim % 2 == 0, "head_dim must be even for 1D RoPE."

        self.head_dim = head_dim
        self.rope = RotaryEmbedding1D(head_dim, base=base, scale=scale)

    def forward(
        self,
        q: Tensor,
        k: Tensor,
        coords: Tensor,
    ) -> Tuple[Tensor, Tensor]:
        """
        Args:
            q, k:   (B, num_heads, N, head_dim)
            coords: (B, N)     1D coordinates in [0, 1] or physical range

        Returns:
            q_out, k_out: same shapes as q, k
        """
        # coords embedding: (B, N, head_dim)
        cos, sin = self.rope(coords)  # (B, N, head_dim)

        # broadcast over heads
        cos = cos.unsqueeze(1)  # (B, 1, N, head_dim)
        sin = sin.unsqueeze(1)  # (B, 1, N, head_dim)

        q_out = rotary_pos_embedding(q, cos, sin)
        k_out = rotary_pos_embedding(k, cos, sin)
        return q_out, k_out


class RoPE2DAdapter(nn.Module):
    """
    2D RoPE adapter for (x, y) coordinates.

    Assumes head_dim is split into two equal parts: D = 2 * D_axis, and D_axis is even.
    q, k: (B, num_heads, N, head_dim)
    coords_x, coords_y: (B, N)
    """

    def __init__(self, head_dim: int, base: float = 10000.0, scale: float = 1.0) -> None:
        super().__init__()
        assert head_dim % 2 == 0, "head_dim must be divisible by 2 for 2D RoPE."
        d_axis = head_dim // 2
        assert d_axis % 2 == 0, "Each axis dim must be even."

        self.d_axis = d_axis
        self.rope_x = RotaryEmbedding1D(d_axis, base=base, scale=scale)
        self.rope_y = RotaryEmbedding1D(d_axis, base=base, scale=scale)

    def forward(
        self,
        q: Tensor,
        k: Tensor,
        coords_x: Tensor,
        coords_y: Tensor,
    ) -> Tuple[Tensor, Tensor]:
        """
        q, k: (B, num_heads, N, head_dim)
        coords_x, coords_y: (B, N)
        """
        B, H, N, D = q.shape
        D_axis = self.d_axis

        qx, qy = q[..., :D_axis], q[..., D_axis:]
        kx, ky = k[..., :D_axis], k[..., D_axis:]

        cos_x, sin_x = self.rope_x(coords_x)  # (B, N, D_axis)
        cos_y, sin_y = self.rope_y(coords_y)  # (B, N, D_axis)

        cos_x = cos_x.unsqueeze(1)  # (B, 1, N, D_axis)
        sin_x = sin_x.unsqueeze(1)
        cos_y = cos_y.unsqueeze(1)
        sin_y = sin_y.unsqueeze(1)

        qx = rotary_pos_embedding(qx, cos_x, sin_x)
        kx = rotary_pos_embedding(kx, cos_x, sin_x)
        qy = rotary_pos_embedding(qy, cos_y, sin_y)
        ky = rotary_pos_embedding(ky, cos_y, sin_y)

        q_out = torch.cat([qx, qy], dim=-1)
        k_out = torch.cat([kx, ky], dim=-1)
        return q_out, k_out


class RoPE3DAdapter(nn.Module):
    """
    3D RoPE adapter for (x, y, z) coordinates.

    Assumes head_dim is split into 3 equal parts: D = 3 * D_axis, and D_axis is even.
    """

    def __init__(self, head_dim: int, base: float = 10000.0, scale: float = 1.0) -> None:
        super().__init__()
        assert head_dim % 3 == 0, "head_dim must be divisible by 3 for 3D RoPE."
        d_axis = head_dim // 3
        assert d_axis % 2 == 0, "Each axis dim must be even."

        self.d_axis = d_axis
        self.rope_x = RotaryEmbedding1D(d_axis, base=base, scale=scale)
        self.rope_y = RotaryEmbedding1D(d_axis, base=base, scale=scale)
        self.rope_z = RotaryEmbedding1D(d_axis, base=base, scale=scale)

    def forward(
        self,
        q: Tensor,
        k: Tensor,
        coords_x: Tensor,
        coords_y: Tensor,
        coords_z: Tensor,
    ) -> Tuple[Tensor, Tensor]:
        """
        q, k: (B, num_heads, N, head_dim)
        coords_{x,y,z}: (B, N)
        """
        B, H, N, D = q.shape
        D_axis = self.d_axis

        qx, qy, qz = q.split(D_axis, dim=-1)
        kx, ky, kz = k.split(D_axis, dim=-1)

        cos_x, sin_x = self.rope_x(coords_x)  # (B, N, D_axis)
        cos_y, sin_y = self.rope_y(coords_y)
        cos_z, sin_z = self.rope_z(coords_z)

        cos_x = cos_x.unsqueeze(1)
        sin_x = sin_x.unsqueeze(1)
        cos_y = cos_y.unsqueeze(1)
        sin_y = sin_y.unsqueeze(1)
        cos_z = cos_z.unsqueeze(1)
        sin_z = sin_z.unsqueeze(1)

        qx = rotary_pos_embedding(qx, cos_x, sin_x)
        kx = rotary_pos_embedding(kx, cos_x, sin_x)
        qy = rotary_pos_embedding(qy, cos_y, sin_y)
        ky = rotary_pos_embedding(ky, cos_y, sin_y)
        qz = rotary_pos_embedding(qz, cos_z, sin_z)
        kz = rotary_pos_embedding(kz, cos_z, sin_z)

        q_out = torch.cat([qx, qy, qz], dim=-1)
        k_out = torch.cat([kx, ky, kz], dim=-1)
        return q_out, k_out


# ============================================================
# Core Multi-Head Attention (Self / Cross, with optional Flash)
# ============================================================

class MultiHeadSelfAttention(nn.Module):
    """
    Standard multi-head self-attention with optional FlashAttention and RoPE.

    x: (B, N, d_model)
    """

    def __init__(
        self,
        d_model: int,
        num_heads: int,
        attn_dropout: float = 0.0,
        proj_dropout: float = 0.0,
        use_flash: bool = False,
        rope_adapter: Optional[nn.Module] = None,
    ) -> None:
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads."
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.scale = self.head_dim ** -0.5

        self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=True)
        self.out_proj = nn.Linear(d_model, d_model, bias=True)

        self.attn_dropout_p = attn_dropout
        self.attn_dropout = nn.Dropout(attn_dropout)
        self.proj_dropout = nn.Dropout(proj_dropout)

        self.use_flash = use_flash
        self.rope_adapter = rope_adapter  # e.g. RoPE2DAdapter / RoPE3DAdapter

    def forward(
        self,
        x: Tensor,
        attn_mask: Optional[Tensor] = None,
        key_padding_mask: Optional[Tensor] = None,
        rope_kwargs: Optional[dict] = None,
    ) -> Tensor:
        """
        Args:
            x: (B, N, d_model)
            attn_mask: optional (N, N) or (B, N, N), additive or bool mask
            key_padding_mask: optional (B, N) bool mask
            rope_kwargs: extra kwargs passed to rope_adapter, e.g.
                {"coords_x": ..., "coords_y": ...}

        Returns:
            out: (B, N, d_model)
        """
        B, N, _ = x.shape

        # Project to q, k, v
        qkv = self.qkv_proj(x)  # (B, N, 3*d_model)
        qkv = qkv.view(B, N, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)
        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (B, num_heads, N, head_dim)

        # Optional RoPE
        if self.rope_adapter is not None:
            rope_kwargs = rope_kwargs or {}
            q, k = self.rope_adapter(q, k, **rope_kwargs)

        # FlashAttention path (only if no masks for simplicity)
        if (
            self.use_flash
            and hasattr(F, "scaled_dot_product_attention")
            and attn_mask is None
            and key_padding_mask is None
        ):
            # F.scaled_dot_product_attention expects (B*num_heads, N, head_dim)
            q_ = q
            k_ = k
            v_ = v
            # (B, num_heads, N, head_dim) -> (B*num_heads, N, head_dim)
            q_ = q_.reshape(B * self.num_heads, N, self.head_dim)
            k_ = k_.reshape(B * self.num_heads, N, self.head_dim)
            v_ = v_.reshape(B * self.num_heads, N, self.head_dim)

            attn_out = F.scaled_dot_product_attention(
                q_,
                k_,
                v_,
                attn_mask=None,
                dropout_p=self.attn_dropout_p if self.training else 0.0,
                is_causal=False,
            )  # (B*num_heads, N, head_dim)

            attn_out = attn_out.view(B, self.num_heads, N, self.head_dim)
        else:
            # Manual attention
            # (B, num_heads, N, head_dim) -> (B, num_heads, N, N)
            attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale

            # key_padding_mask: (B, N) -> (B, 1, 1, N) broadcast
            if key_padding_mask is not None:
                mask = key_padding_mask[:, None, None, :]  # True = pad
                attn_scores = attn_scores.masked_fill(mask, float("-inf"))

            # attn_mask: (N, N) or (B, N, N)
            if attn_mask is not None:
                if attn_mask.dim() == 2:
                    attn_scores = attn_scores + attn_mask[None, None, :, :]
                elif attn_mask.dim() == 3:
                    attn_scores = attn_scores + attn_mask[:, None, :, :]

            attn_weights = F.softmax(attn_scores, dim=-1)
            attn_weights = self.attn_dropout(attn_weights)

            attn_out = torch.matmul(attn_weights, v)  # (B, num_heads, N, head_dim)

        # Merge heads
        attn_out = attn_out.transpose(1, 2).contiguous()  # (B, N, num_heads, head_dim)
        attn_out = attn_out.view(B, N, self.d_model)      # (B, N, d_model)
        out = self.out_proj(attn_out)
        out = self.proj_dropout(out)
        return out


class MultiHeadCrossAttention(nn.Module):
    """
    Multi-head cross-attention:
        Q来自 x，K/V 来自 context。

    x:       (B, N_q, d_model)
    context: (B, N_kv, d_model)
    """

    def __init__(
        self,
        d_model: int,
        num_heads: int,
        attn_dropout: float = 0.0,
        proj_dropout: float = 0.0,
        use_flash: bool = False,
        rope_adapter: Optional[nn.Module] = None,
    ) -> None:
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads."
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.scale = self.head_dim ** -0.5

        self.q_proj = nn.Linear(d_model, d_model, bias=True)
        self.k_proj = nn.Linear(d_model, d_model, bias=True)
        self.v_proj = nn.Linear(d_model, d_model, bias=True)
        self.out_proj = nn.Linear(d_model, d_model, bias=True)

        self.attn_dropout_p = attn_dropout
        self.attn_dropout = nn.Dropout(attn_dropout)
        self.proj_dropout = nn.Dropout(proj_dropout)

        self.use_flash = use_flash
        self.rope_adapter = rope_adapter

    def forward(
        self,
        x: Tensor,
        context: Tensor,
        attn_mask: Optional[Tensor] = None,
        key_padding_mask: Optional[Tensor] = None,
        rope_kwargs: Optional[dict] = None,
    ) -> Tensor:
        """
        Args:
            x: (B, N_q, d_model)      - queries
            context: (B, N_kv, d_model) - keys/values
        """
        B, N_q, _ = x.shape
        Bc, N_kv, _ = context.shape
        assert B == Bc, "Batch size mismatch between x and context."

        q = self.q_proj(x)
        k = self.k_proj(context)
        v = self.v_proj(context)

        q = q.view(B, N_q, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (B,H,N_q,Dh)
        k = k.view(B, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3) # (B,H,N_kv,Dh)
        v = v.view(B, N_kv, self.num_heads, self.head_dim).permute(0, 2, 1, 3) # (B,H,N_kv,Dh)

        if self.rope_adapter is not None:
            rope_kwargs = rope_kwargs or {}
            q, k = self.rope_adapter(q, k, **rope_kwargs)

        if (
            self.use_flash
            and hasattr(F, "scaled_dot_product_attention")
            and attn_mask is None
            and key_padding_mask is None
        ):
            q_ = q.reshape(B * self.num_heads, N_q, self.head_dim)
            k_ = k.reshape(B * self.num_heads, N_kv, self.head_dim)
            v_ = v.reshape(B * self.num_heads, N_kv, self.head_dim)

            attn_out = F.scaled_dot_product_attention(
                q_,
                k_,
                v_,
                attn_mask=None,
                dropout_p=self.attn_dropout_p if self.training else 0.0,
                is_causal=False,
            )  # (B*H, N_q, Dh)

            attn_out = attn_out.view(B, self.num_heads, N_q, self.head_dim)
        else:
            attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (B,H,N_q,N_kv)

            if key_padding_mask is not None:
                # key_padding_mask: (B, N_kv)
                mask = key_padding_mask[:, None, None, :]  # True = pad
                attn_scores = attn_scores.masked_fill(mask, float("-inf"))

            if attn_mask is not None:
                if attn_mask.dim() == 2:
                    attn_scores = attn_scores + attn_mask[None, None, :, :]
                elif attn_mask.dim() == 3:
                    attn_scores = attn_scores + attn_mask[:, None, :, :]

            attn_weights = F.softmax(attn_scores, dim=-1)
            attn_weights = self.attn_dropout(attn_weights)

            attn_out = torch.matmul(attn_weights, v)  # (B,H,N_q,Dh)

        attn_out = attn_out.transpose(1, 2).contiguous()  # (B, N_q, H, Dh)
        attn_out = attn_out.view(B, N_q, self.d_model)
        out = self.out_proj(attn_out)
        out = self.proj_dropout(out)
        return out


# ============================================================
# Channel Attention (SE-style)
# ============================================================

class ChannelAttention(nn.Module):
    """
    Simple channel attention (Squeeze-and-Excitation style).

    x: (B, N, d_model)
    Global average pool over N, then MLP to get channel-wise gates.
    """

    def __init__(self, d_model: int, reduction: int = 16) -> None:
        super().__init__()
        hidden = max(d_model // reduction, 1)
        self.mlp = nn.Sequential(
            nn.Linear(d_model, hidden, bias=True),
            nn.ReLU(inplace=True),
            nn.Linear(hidden, d_model, bias=True),
            nn.Sigmoid(),
        )

    def forward(self, x: Tensor) -> Tensor:
        B, N, C = x.shape
        # (B, C)
        pooled = x.mean(dim=1)
        weights = self.mlp(pooled)  # (B, C)
        return x * weights.unsqueeze(1)


# ============================================================
# Window Attention 2D (Swin-style)
# ============================================================

def window_partition(x: Tensor, window_size: int) -> Tensor:
    """
    x: (B, H, W, C)
    returns: (B * num_windows, window_size*window_size, C)
    """
    B, H, W, C = x.shape
    assert H % window_size == 0 and W % window_size == 0, \
        "H and W must be divisible by window_size."

    x = x.view(
        B,
        H // window_size, window_size,
        W // window_size, window_size,
        C,
    )  # (B, nH, Ws, nW, Ws, C)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()
    windows = x.view(-1, window_size * window_size, C)
    return windows


def window_reverse(
    windows: Tensor,
    window_size: int,
    H: int,
    W: int,
) -> Tensor:
    """
    windows: (B * num_windows, window_size*window_size, C)
    returns: (B, H, W, C)
    """
    B_ = windows.shape[0] // (H // window_size * W // window_size)
    C = windows.shape[-1]
    x = windows.view(
        B_,
        H // window_size, W // window_size,
        window_size, window_size, C,
    )  # (B_, nH, nW, Ws, Ws, C)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()
    x = x.view(B_, H, W, C)
    return x


class WindowAttention2D(nn.Module):
    """
    2D windowed self-attention.

    x: (B, N, d_model), where N = H * W
    You must pass H, W at forward time.
    """

    def __init__(
        self,
        d_model: int,
        num_heads: int,
        window_size: int,
        attn_dropout: float = 0.0,
        proj_dropout: float = 0.0,
        use_flash: bool = False,
        rope_adapter: Optional[nn.Module] = None,
    ) -> None:
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.window_size = window_size

        self.attn = MultiHeadSelfAttention(
            d_model=d_model,
            num_heads=num_heads,
            attn_dropout=attn_dropout,
            proj_dropout=proj_dropout,
            use_flash=use_flash,
            rope_adapter=rope_adapter,
        )

    def forward(
        self,
        x: Tensor,
        H: int,
        W: int,
        attn_mask: Optional[Tensor] = None,
        key_padding_mask: Optional[Tensor] = None,
        rope_kwargs: Optional[dict] = None,
    ) -> Tensor:
        """
        x: (B, N, d_model), N = H * W
        """
        B, N, C = x.shape
        assert N == H * W, "N must equal H * W."

        x = x.view(B, H, W, C)
        windows = window_partition(x, self.window_size)  # (B*nW, Ws*Ws, C)
        windows = self.attn(
            windows,
            attn_mask=attn_mask,
            key_padding_mask=key_padding_mask,
            rope_kwargs=rope_kwargs,
        )  # (B*nW, Ws*Ws, C)
        x = window_reverse(windows, self.window_size, H, W)  # (B, H, W, C)
        x = x.view(B, N, C)
        return x


# ============================================================
# Axial Attention 2D
# ============================================================

class AxialAttention2D(nn.Module):
    """
    Axial attention on 2D grid:
        - attention along H axis
        - then attention along W axis

    x: (B, N, d_model), N = H * W
    """

    def __init__(
        self,
        d_model: int,
        num_heads: int,
        attn_dropout: float = 0.0,
        proj_dropout: float = 0.0,
        use_flash: bool = False,
    ) -> None:
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads

        self.attn_height = MultiHeadSelfAttention(
            d_model=d_model,
            num_heads=num_heads,
            attn_dropout=attn_dropout,
            proj_dropout=proj_dropout,
            use_flash=use_flash,
        )
        self.attn_width = MultiHeadSelfAttention(
            d_model=d_model,
            num_heads=num_heads,
            attn_dropout=attn_dropout,
            proj_dropout=proj_dropout,
            use_flash=use_flash,
        )

    def forward(
        self,
        x: Tensor,
        H: int,
        W: int,
    ) -> Tensor:
        """
        x: (B, N, d_model), N = H * W
        """
        B, N, C = x.shape
        assert N == H * W, "N must equal H * W for AxialAttention2D."

        x = x.view(B, H, W, C)

        # Attention along height (H)
        # reshape to (B*W, H, C)
        x_h = x.permute(0, 2, 1, 3).contiguous().view(B * W, H, C)
        x_h = self.attn_height(x_h)  # (B*W, H, C)
        x_h = x_h.view(B, W, H, C).permute(0, 2, 1, 3).contiguous()  # (B, H, W, C)

        # Attention along width (W)
        x_w = x_h.view(B * H, W, C)
        x_w = self.attn_width(x_w)  # (B*H, W, C)
        x_w = x_w.view(B, H, W, C)

        out = x_w.view(B, N, C)
        return out
</file>

<file path="models/base/embedding.py">
# models/base/embedding.py
import math

from typing import Sequence, Optional, Tuple

import torch
from torch import nn, Tensor


def build_grid_coords(
    shape: Sequence[int],
    *,
    device: Optional[torch.device] = None,
    dtype: torch.dtype = torch.float32,
    coord_min: float = 0.0,
    coord_max: float = 1.0,
) -> Tensor:
    """
    Build a regular grid of coordinates in [coord_min, coord_max] for each axis.

    Args:
        shape: Spatial shape for each dimension, e.g. (H, W) or (H, W, D).
        device: Target device for the coordinates.
        dtype: Data type for the coordinates.
        coord_min: Lower bound for each coordinate axis.
        coord_max: Upper bound for each coordinate axis.

    Returns:
        coords: Tensor of shape (N, D), where
                N = prod(shape), D = len(shape).
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    dims = len(shape)
    # 1D list of coordinates for each axis
    axes = [
        torch.linspace(coord_min, coord_max, steps=s, device=device, dtype=dtype)
        for s in shape
    ]
    # Create meshgrid: each element has shape (s1, s2, ..., sD)
    mesh = torch.meshgrid(*axes, indexing="ij")  # list of length D
    # Stack last dimension -> (s1, s2, ..., sD, D)
    grid = torch.stack(mesh, dim=-1)
    # Flatten spatial dims -> (N, D)
    coords = grid.reshape(-1, dims)
    return coords


def unified_pos_embedding(
    shape_list: Sequence[int],
    ref: int,
    *,
    batch_size: int = 1,
    device: Optional[torch.device] = None,
    dtype: torch.dtype = torch.float32,
    coord_min: float = 0.0,
    coord_max: float = 1.0,
) -> Tensor:
    """
    Build a unified positional distance embedding between an input grid
    and a reference grid, for arbitrary spatial dimensions.

    This generalizes your original implementation for 1D/2D/3D to N-D.

    Args:
        shape_list: Spatial shape of the input grid, e.g.
                    [H] for 1D, [H, W] for 2D, [H, W, D] for 3D, etc.
        ref: Number of reference points per dimension
             (reference grid will have shape [ref, ref, ..., ref]).
        batch_size: Batch size (the distance matrix is simply broadcasted
                    along batch dimension).
        device: Target device for computations.
        dtype: Data type for coordinates and distances.
        coord_min: Lower bound for coordinate values (default 0.0).
        coord_max: Upper bound for coordinate values (default 1.0).

    Returns:
        pos: Positional distance tensor of shape (B, N, M), where
             B = batch_size,
             N = prod(shape_list)      (number of input grid points),
             M = ref ** len(shape_list) (number of reference grid points).
             Each entry pos[b, i, j] is the Euclidean distance between
             input point i and reference point j in coordinate space.
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Input grid coordinates: (N, D)
    coords_in = build_grid_coords(
        shape_list,
        device=device,
        dtype=dtype,
        coord_min=coord_min,
        coord_max=coord_max,
    )  # (N, D)

    # Reference grid coordinates: (M, D)
    dims = len(shape_list)
    ref_shape = [ref] * dims
    coords_ref = build_grid_coords(
        ref_shape,
        device=device,
        dtype=dtype,
        coord_min=coord_min,
        coord_max=coord_max,
    )  # (M, D)

    # Compute pairwise Euclidean distances
    # coords_in:  (N, D)
    # coords_ref: (M, D)
    # diff: (N, M, D)
    diff = coords_in.unsqueeze(1) - coords_ref.unsqueeze(0)
    # dist: (N, M)
    dist = torch.linalg.norm(diff, dim=-1)

    # Broadcast across batch: (B, N, M)
    pos = dist.unsqueeze(0).expand(batch_size, -1, -1).contiguous()
    return pos


class RotaryEmbedding1D(nn.Module):
    """
    1D Rotary positional embedding (RoPE) for a single coordinate axis.

    Typical use:
        rope = RotaryEmbedding1D(dim=32)  # dim per axis (must be even)
        cos_x, sin_x = rope(x_coords)     # x_coords: (B, N)

    In attention:
        # q, k: (B, H, N, D), with D = D_axis * num_axes (e.g., num_axes=2 for 2D)
        cos_x = cos_x.unsqueeze(1)  # -> (B, 1, N, D_axis) to broadcast over heads
        sin_x = sin_x.unsqueeze(1)
    """

    def __init__(
        self,
        dim: int,
        base: float = 10000.0,
        scale: float = 1.0,
    ) -> None:
        super().__init__()
        assert dim % 2 == 0, "RoPE dimension per axis must be even."

        self.dim = dim
        self.base = base
        self.scale = scale

        # frequencies for half of the dimensions: (dim/2,)
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))
        inv_freq = inv_freq * scale
        self.register_buffer("inv_freq", inv_freq)

    def forward(self, coords: Tensor) -> Tuple[Tensor, Tensor]:
        """
        Args:
            coords: tensor of shape (..., N)
                    e.g. (B, N) physical coordinates along one axis (x / y / z / t).

        Returns:
            cos, sin: both of shape (..., N, dim)
                      ready to be broadcast to q/k whose last dim is `dim` (per axis).
        """
        # coords: (..., N)
        coords = coords.to(self.inv_freq.device).to(self.inv_freq.dtype)

        # angles: (..., N, dim/2)
        angles = torch.einsum("... n, d -> ... n d", coords, self.inv_freq)

        cos = angles.cos()
        sin = angles.sin()

        # duplicate each frequency so last dim = dim
        cos = torch.repeat_interleave(cos, 2, dim=-1)  # (..., N, dim)
        sin = torch.repeat_interleave(sin, 2, dim=-1)  # (..., N, dim)
        return cos, sin


def rotate_half(x: Tensor) -> Tensor:
    """
    Rotate last dimension by 90 degrees in each pair:
        (x1, x2) -> (-x2, x1)

    Args:
        x: (..., D), where D is even.

    Returns:
        rotated x: (..., D)
    """
    D = x.shape[-1]
    assert D % 2 == 0, "Last dimension must be even for rotate_half."

    # view as (..., D/2, 2)
    x = x.view(*x.shape[:-1], D // 2, 2)
    x1, x2 = x.unbind(dim=-1)  # (..., D/2)
    return torch.cat([-x2, x1], dim=-1)  # (..., D)


def rotary_pos_embedding(x: Tensor, cos: Tensor, sin: Tensor) -> Tensor:
    """
    Apply RoPE on the last dimension of x.

    Args:
        x:   (..., N, D)
        cos: (..., N, D) - from RotaryEmbedding1D
        sin: (..., N, D)

    Returns:
        x_out: (..., N, D)
    """
    return x * cos + rotate_half(x) * sin


def rotary_2d_pos_embedding(
    x: Tensor,
    cos_x: Tensor,
    sin_x: Tensor,
    cos_y: Tensor,
    sin_y: Tensor,
) -> Tensor:
    """
    2D RoPE: channels are split into x-axis part and y-axis part.

    Args:
        x:      (..., N, D)  e.g. (B, H, N, D) or (B, N, D)
        cos_x:  (..., N, D_axis)
        sin_x:  (..., N, D_axis)
        cos_y:  (..., N, D_axis)
        sin_y:  (..., N, D_axis)

    Assumes:
        D = 2 * D_axis, and D_axis is even.

    Returns:
        x_out: same shape as x
    """
    D = x.shape[-1]
    assert D % 2 == 0, "Embedding dim must be divisible by 2 for 2D RoPE."
    D_axis = D // 2
    assert D_axis % 2 == 0, "Each axis dim must be even for RoPE."

    x_x, x_y = x[..., :D_axis], x[..., D_axis:]

    x_x_out = rotary_pos_embedding(x_x, cos_x, sin_x)
    x_y_out = rotary_pos_embedding(x_y, cos_y, sin_y)

    return torch.cat([x_x_out, x_y_out], dim=-1)


def rotary_3d_pos_embedding(
    x: Tensor,
    cos_x: Tensor,
    sin_x: Tensor,
    cos_y: Tensor,
    sin_y: Tensor,
    cos_z: Tensor,
    sin_z: Tensor,
) -> Tensor:
    """
    3D RoPE: channels are split into x / y / z parts.

    Args:
        x:      (..., N, D)
        cos_x:  (..., N, D_axis)
        sin_x:  (..., N, D_axis)
        cos_y:  (..., N, D_axis)
        sin_y:  (..., N, D_axis)
        cos_z:  (..., N, D_axis)
        sin_z:  (..., N, D_axis)

    Assumes:
        D = 3 * D_axis, and D_axis is even.

    Returns:
        x_out: same shape as x
    """
    D = x.shape[-1]
    assert D % 3 == 0, "Embedding dim must be divisible by 3 for 3D RoPE."
    D_axis = D // 3
    assert D_axis % 2 == 0, "Each axis dim must be even for RoPE."

    x_x, x_y, x_z = x.split(D_axis, dim=-1)

    x_x_out = rotary_pos_embedding(x_x, cos_x, sin_x)
    x_y_out = rotary_pos_embedding(x_y, cos_y, sin_y)
    x_z_out = rotary_pos_embedding(x_z, cos_z, sin_z)

    return torch.cat([x_x_out, x_y_out, x_z_out], dim=-1)


def timestep_embedding(
    timesteps: Tensor,
    dim: int,
    max_period: float = 10000.0,
    repeat_only: bool = False,
) -> Tensor:
    """
    Create sinusoidal timestep embeddings (DDPM-style).

    Args:
        timesteps: 1D or ND tensor of timesteps, e.g. shape (N,) or (B,)
                   values can be integer steps or continuous time.
        dim:       output embedding dimension.
        max_period: controls the minimum frequency of the embeddings.
        repeat_only: if True, just repeat the raw timesteps (no sin/cos),
                     useful for ablation or very low-dimensional cases.

    Returns:
        emb: tensor of shape (..., dim), where ... is timesteps.shape
    """
    # ensure float
    timesteps = timesteps.float()

    if repeat_only:
        # simply repeat scalar timesteps to desired dimension
        while timesteps.ndim < 2:
            timesteps = timesteps.unsqueeze(-1)  # (..., 1)
        return timesteps.repeat_interleave(dim, dim=-1)

    half = dim // 2
    device = timesteps.device

    # frequencies: (half,)
    freqs = torch.exp(
        -math.log(max_period)
        * torch.arange(start=0, end=half, dtype=torch.float32, device=device)
        / half
    )

    # timesteps: (...,) -> (..., 1)
    args = timesteps.unsqueeze(-1) * freqs  # (..., half)

    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)  # (..., 2*half)

    # if dim is odd, pad one zero channel
    if dim % 2 == 1:
        pad_shape = list(emb.shape[:-1]) + [1]
        emb = torch.cat([emb, emb.new_zeros(pad_shape)], dim=-1)

    return emb  # shape (..., dim)
</file>

<file path="models/base/mlp.py">
# models/base/base.py
import math
from typing import Sequence, Optional, Union

import torch
from torch import nn

from .utils import get_activation


class BaseMLP(nn.Module):
    """
    Generic MLP block.

    Features:
      - Flexible hidden sizes
      - Configurable activation
      - Optional dropout between layers
      - Optional residual connection (with optional linear projection)

    Input / output:
      - Accepts tensor of shape (..., in_dim)
      - Returns tensor of shape (..., out_dim)
      - Residual connection is applied on the last dimension

    Args:
        in_dim: input feature dimension
        out_dim: output feature dimension
        hidden_dims: list/tuple of hidden sizes (e.g. [256, 256])
        activation: activation name (e.g. 'gelu', 'relu', 'silu', 'tanh', 'leaky_relu', 'identity')
        dropout: dropout probability applied after each hidden layer
        use_residual: whether to use residual connection
        residual_proj: if True, learn a linear projection when in_dim != out_dim;
                       if False, only add residual when in_dim == out_dim
        last_activation: if True, also apply activation after the last linear layer
    """

    def __init__(
        self,
        in_dim: int,
        out_dim: int,
        hidden_dims: Optional[Union[int, Sequence[int]]] = None,
        activation: str = "gelu",
        dropout: float = 0.0,
        use_residual: bool = False,
        residual_proj: bool = True,
        last_activation: bool = False,
    ) -> None:
        super().__init__()

        # Normalize hidden_dims to a list
        if hidden_dims is None:
            hidden_dims_list: list[int] = []
        elif isinstance(hidden_dims, int):
            hidden_dims_list = [hidden_dims]
        else:
            hidden_dims_list = list(hidden_dims)

        self.in_dim = in_dim
        self.out_dim = out_dim
        self.use_residual = use_residual
        self.last_activation = last_activation and (out_dim is not None)

        # Build main MLP stack
        dims = [in_dim] + hidden_dims_list + [out_dim]
        act = get_activation(activation)
        layers = []

        for i in range(len(dims) - 1):
            in_d, out_d = dims[i], dims[i + 1]
            layers.append(nn.Linear(in_d, out_d))

            is_last_layer = (i == len(dims) - 2)
            if not is_last_layer or self.last_activation:
                if activation is not None:
                    layers.append(act.__class__())  # new instance
            if (not is_last_layer) and dropout > 0.0:
                layers.append(nn.Dropout(dropout))

        self.net = nn.Sequential(*layers)

        # Residual projection if needed
        if use_residual:
            if in_dim == out_dim:
                self.residual_proj = nn.Identity()
            elif residual_proj:
                self.residual_proj = nn.Linear(in_dim, out_dim)
            else:
                # Residual requested but dimensions mismatch and projection disabled
                # In this case, we simply skip residual connection.
                self.residual_proj = None
        else:
            self.residual_proj = None

        self.reset_parameters()

    def reset_parameters(self) -> None:
        """
        Basic parameter initialization for linear layers.
        """
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                if m.bias is not None:
                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)
                    bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
                    nn.init.uniform_(m.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: input tensor of shape (..., in_dim)

        Returns:
            Tensor of shape (..., out_dim)
        """
        # Flatten last dimension only, keep leading shape intact
        orig = x
        y = self.net(x)

        if self.use_residual and self.residual_proj is not None:
            res = self.residual_proj(orig)
            y = y + res

        return y
</file>

<file path="models/base/utils.py">
# models/base/utils.py
from typing import Optional

from torch import nn


def get_activation(name: Optional[str]) -> nn.Module:
    """
    Return an activation module by name.
    If name is None or 'identity', returns nn.Identity().
    """
    if name is None or name.lower() == "identity":
        return nn.Identity()
    name = name.lower()
    if name == "relu":
        return nn.ReLU(inplace=True)
    if name == "gelu":
        return nn.GELU()
    if name == "silu":
        return nn.SiLU()
    if name == "tanh":
        return nn.Tanh()
    if name == "leaky_relu":
        return nn.LeakyReLU(negative_slope=0.1, inplace=True)
    raise ValueError(f"Unsupported activation: {name}")
</file>

<file path="models/fno/__init__.py">
from .fno import FNO1d, FNO2d, FNO3d
</file>

<file path="models/fno/fno.py">
# models/fno/fno.py
from typing import Any, Dict, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F

from .spectral_conv import SpectralConv1d, SpectralConv2d, SpectralConv3d


class FNO1d(nn.Module):
    """
    Fourier Neural Operator for 1D fields.

    Interface:
        input:  x in shape (B, L, C_in)
        output: y in shape (B, L, C_out)

    Typical model_params:
        {
            "in_channels": 3,
            "out_channels": 1,
            "modes": 16,
            "width": 64,
            "n_layers": 4,
            "padding": 0,       # optional spatial padding on the right
            "use_grid": True,   # whether to append 1D coordinate
            "fc_dim": 128,      # hidden dim of final pointwise MLP (optional)
        }
    """

    def __init__(
        self,
        model_params: Dict[str, Any],
        **kwargs: Any,
    ) -> None:
        super().__init__()

        # Core config
        self.in_channels: int = model_params.get("in_channels", 3)
        self.out_channels: int = model_params.get("out_channels", 1)
        self.modes: int = model_params.get("modes", 16)
        self.width: int = model_params.get("width", 64)
        self.n_layers: int = model_params.get("n_layers", 4)
        self.padding: int = model_params.get("padding", 0)
        self.use_grid: bool = model_params.get("use_grid", True)

        # Final "fc" hidden dim (per-point MLP)
        self.fc_dim: int = model_params.get("fc_dim", 2 * self.width)

        # If use_grid=True, append 1 coordinate channel (x ∈ [0, 1])
        in_proj_in_channels = self.in_channels + (1 if self.use_grid else 0)

        # Lift to width channels via 1×1 conv
        self.input_proj = nn.Conv1d(
            in_channels=in_proj_in_channels,
            out_channels=self.width,
            kernel_size=1,
        )

        # Spectral + local (1×1) paths for each layer
        self.spectral_convs = nn.ModuleList(
            [
                SpectralConv1d(self.width, self.width, self.modes)
                for _ in range(self.n_layers)
            ]
        )
        self.ws = nn.ModuleList(
            [
                nn.Conv1d(self.width, self.width, kernel_size=1)
                for _ in range(self.n_layers)
            ]
        )

        self.activation = nn.GELU()

        # Final pointwise MLP (per grid point), implemented as 1×1 convs
        # fc1: width -> fc_dim
        # fc2: fc_dim -> out_channels
        self.output_layers = nn.Sequential(
            nn.Conv1d(self.width, self.fc_dim, kernel_size=1),
            nn.GELU(),
            nn.Conv1d(self.fc_dim, self.out_channels, kernel_size=1),
        )

    @staticmethod
    def _make_grid(size: int, device: torch.device) -> torch.Tensor:
        """
        Build a 1D coordinate grid in [0, 1] with shape (1, 1, L).
        """
        grid = torch.linspace(0.0, 1.0, size, device=device)
        grid = grid.view(1, 1, size)
        return grid

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor of shape (B, L, C_in), channels-last.

        Returns:
            Tensor of shape (B, L, C_out), channels-last.
        """
        if x.dim() != 3:
            raise ValueError(f"Expected 3D input (B, L, C), got {x.shape}")

        B, L, C_in = x.shape
        if C_in != self.in_channels:
            raise ValueError(
                f"Input channels {C_in} != configured in_channels {self.in_channels}"
            )

        # (B, L, C_in) -> (B, C_in, L)
        x = x.permute(0, 2, 1).contiguous()

        # Optional padding on the spatial dimension (right side)
        if self.padding > 0:
            x = F.pad(x, (0, self.padding))
            L_p = L + self.padding
        else:
            L_p = L

        # Append coordinate grid if requested
        if self.use_grid:
            grid = self._make_grid(L_p, x.device)   # (1, 1, L_p)
            grid = grid.expand(B, -1, -1)           # (B, 1, L_p)
            x = torch.cat([x, grid], dim=1)         # (B, C_in+1, L_p)

        # Lift to width channels
        x = self.input_proj(x)  # (B, width, L_p)

        # Spectral blocks
        for spec_conv, w in zip(self.spectral_convs, self.ws):
            x1 = spec_conv(x)   # spectral branch
            x2 = w(x)           # local branch (1x1 conv)
            x = self.activation(x1 + x2)

        # Final per-point MLP
        x = self.output_layers(x)  # (B, C_out, L_p)

        # Remove padding
        if self.padding > 0:
            x = x[..., :L]  # (B, C_out, L)

        # Back to channels-last
        x = x.permute(0, 2, 1).contiguous()  # (B, L, C_out)
        return x


class FNO2d(nn.Module):
    """
    2D Fourier Neural Operator (FNO) with channels-last PDE interface.

    Design:
      - Optionally augment input with normalized spatial coordinates (y, x) ∈ [0,1]^2.
      - Lift to a higher-dimensional channel space (width).
      - Apply several spectral convolution blocks:
            x_{k+1} = GELU( SpectralConv2d(x_k) + 1x1Conv(x_k) )
      - Apply final per-point MLP to map width → fc_dim → out_channels.

    Interface:
      forward(x): x ∈ R^{B × H × W × C_in} → y ∈ R^{B × H × W × C_out}

    Expected model_params:
      - in_channels (int, required)
      - out_channels (int, required)
      - modes_x (int, default=12)
      - modes_y (int, default=12)
      - width (int, default=64)
      - n_layers (int, default=4)
      - padding (int, default=0)      # pad on right / bottom
      - use_grid (bool, default=True) # append (y, x)
      - fc_dim (int, optional)        # hidden dim of final MLP, default 2*width
    """

    def __init__(
        self,
        model_params: Dict[str, Any],
        **kwargs: Any,
    ) -> None:
        super().__init__()

        # ---- Core configuration ----
        in_channels: int = model_params.get("in_channels", 1)
        out_channels: int = model_params.get("out_channels", 1)

        modes_x: int = model_params.get("modes_x", 12)
        modes_y: int = model_params.get("modes_y", 12)
        width: int = model_params.get("width", 64)
        n_layers: int = model_params.get("n_layers", 4)

        padding: int = model_params.get("padding", 0)
        use_grid: bool = model_params.get("use_grid", True)
        fc_dim: int = model_params.get("fc_dim", 2 * width)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.width = width
        self.n_layers = n_layers
        self.padding = padding
        self.use_grid = use_grid
        self.fc_dim = fc_dim

        # First projection: from (C_in (+ 2 coords)) → width
        in_proj_in_channels = in_channels + (2 if use_grid else 0)
        self.input_proj = nn.Conv2d(in_proj_in_channels, width, kernel_size=1)

        # Spectral blocks
        self.spectral_convs = nn.ModuleList(
            [SpectralConv2d(width, width, modes_x, modes_y) for _ in range(n_layers)]
        )
        # Local 1x1 conv in physical space for each block
        self.ws = nn.ModuleList(
            [nn.Conv2d(width, width, kernel_size=1) for _ in range(n_layers)]
        )

        self.activation = nn.GELU()

        # Final projection: per-point MLP width → fc_dim → out_channels
        self.output_proj = nn.Sequential(
            nn.Conv2d(width, fc_dim, kernel_size=1),
            nn.GELU(),
            nn.Conv2d(fc_dim, out_channels, kernel_size=1),
        )

    @staticmethod
    def _make_grid(H: int, W: int, device: torch.device) -> torch.Tensor:
        """
        Construct a normalized coordinate grid in [0, 1]^2.

        Returns:
            grid: Tensor of shape (1, 2, H, W)
                  grid[:, 0, :, :] = y / (H - 1)
                  grid[:, 1, :, :] = x / (W - 1)
        """
        y = torch.linspace(0.0, 1.0, H, device=device)
        x = torch.linspace(0.0, 1.0, W, device=device)
        grid_y, grid_x = torch.meshgrid(y, x, indexing="ij")
        grid = torch.stack((grid_y, grid_x), dim=0)  # (2, H, W)
        return grid.unsqueeze(0)  # (1, 2, H, W)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor of shape (B, H, W, C_in), channels-last.

        Returns:
            Tensor of shape (B, H, W, C_out), channels-last.
        """
        if x.dim() != 4:
            raise ValueError(f"Expected 4D input (B, H, W, C), got {x.shape}")

        B, H, W, C = x.shape
        if C != self.in_channels:
            raise ValueError(
                f"Expected input with {self.in_channels} channels, but got {C}"
            )

        # (B, H, W, C) → (B, C, H, W)
        x = x.permute(0, 3, 1, 2).contiguous()

        # Optional zero-padding on right / bottom
        if self.padding > 0:
            pad = self.padding
            # pad format: (pad_left, pad_right, pad_top, pad_bottom)
            x = F.pad(x, (0, pad, 0, pad))
            H_p, W_p = H + pad, W + pad
        else:
            H_p, W_p = H, W

        # Append coordinate grid channels if requested
        if self.use_grid:
            grid = self._make_grid(H_p, W_p, x.device)  # (1, 2, H_p, W_p)
            grid = grid.expand(B, -1, -1, -1)           # (B, 2, H_p, W_p)
            x = torch.cat([x, grid], dim=1)             # (B, C_in+2, H_p, W_p)

        # Lift to width channels
        x = self.input_proj(x)  # (B, width, H_p, W_p)

        # Spectral blocks
        for spec_conv, w in zip(self.spectral_convs, self.ws):
            x1 = spec_conv(x)   # spectral conv
            x2 = w(x)           # local 1x1 conv
            x = self.activation(x1 + x2)

        # Final pointwise MLP
        x = self.output_proj(x)  # (B, C_out, H_p, W_p)

        # Remove padding if applied
        if self.padding > 0:
            x = x[:, :, :H, :W]

        # Back to channels-last
        x = x.permute(0, 2, 3, 1).contiguous()  # (B, H, W, C_out)
        return x


class FNO3d(nn.Module):
    """
    Fourier Neural Operator for 3D fields.

    Interface:
        input:  x in shape (B, D, H, W, C_in)
        output: y in shape (B, D, H, W, C_out)

    D, H, W can be (z, y, x), or (time, y, x), etc., depending on the PDE.

    Typical model_params:
        {
            "in_channels": 3,
            "out_channels": 1,
            "modes1": 12,
            "modes2": 12,
            "modes3": 12,
            "width": 32,
            "n_layers": 4,
            "padding": (0, 0, 0),   # or int
            "use_grid": True,
            "fc_dim": 128,          # hidden dim of final MLP (optional)
        }
    """

    def __init__(
        self,
        model_params: Dict[str, Any],
        **kwargs: Any,
    ) -> None:
        super().__init__()

        self.in_channels: int = model_params.get("in_channels", 3)
        self.out_channels: int = model_params.get("out_channels", 1)

        self.modes1: int = model_params.get("modes1", 12)
        self.modes2: int = model_params.get("modes2", 12)
        self.modes3: int = model_params.get("modes3", 12)

        self.width: int = model_params.get("width", 32)
        self.n_layers: int = model_params.get("n_layers", 4)

        padding = model_params.get("padding", (0, 0, 0))
        if isinstance(padding, int):
            padding = (padding, padding, padding)
        self.padding: Tuple[int, int, int] = padding

        self.use_grid: bool = model_params.get("use_grid", True)
        self.fc_dim: int = model_params.get("fc_dim", 2 * self.width)

        # Append 3 coordinate channels (d, h, w) if use_grid=True
        in_proj_in_channels = self.in_channels + (3 if self.use_grid else 0)

        # Lift to `width` channels via 1×1 conv
        self.input_proj = nn.Conv3d(
            in_channels=in_proj_in_channels,
            out_channels=self.width,
            kernel_size=1,
        )

        # Spectral + local paths for each layer
        self.spectral_convs = nn.ModuleList(
            [
                SpectralConv3d(
                    self.width,
                    self.width,
                    self.modes1,
                    self.modes2,
                    self.modes3,
                )
                for _ in range(self.n_layers)
            ]
        )
        self.ws = nn.ModuleList(
            [
                nn.Conv3d(self.width, self.width, kernel_size=1)
                for _ in range(self.n_layers)
            ]
        )

        self.activation = nn.GELU()

        # Final per-point MLP: width -> fc_dim -> out_channels
        # 对应你给的参考代码里的 fc1 / fc2，只是用 Conv3d(kernel_size=1) 实现
        self.output_layers = nn.Sequential(
            nn.Conv3d(self.width, self.fc_dim, kernel_size=1),
            nn.GELU(),
            nn.Conv3d(self.fc_dim, self.out_channels, kernel_size=1),
        )

    @staticmethod
    def _make_grid(D: int, H: int, W: int, device: torch.device) -> torch.Tensor:
        """
        Build a 3D coordinate grid in [0, 1]^3 with shape (1, 3, D, H, W).
        """
        d = torch.linspace(0.0, 1.0, D, device=device)
        h = torch.linspace(0.0, 1.0, H, device=device)
        w = torch.linspace(0.0, 1.0, W, device=device)

        grid_d, grid_h, grid_w = torch.meshgrid(d, h, w, indexing="ij")
        grid = torch.stack((grid_d, grid_h, grid_w), dim=0)  # (3, D, H, W)
        return grid.unsqueeze(0)  # (1, 3, D, H, W)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor of shape (B, D, H, W, C_in), channels-last.

        Returns:
            y: Tensor of shape (B, D, H, W, C_out), channels-last.
        """
        if x.dim() != 5:
            raise ValueError(f"Expected 5D input (B, D, H, W, C), got {x.shape}")
        B, D, H, W, C_in = x.shape
        if C_in != self.in_channels:
            raise ValueError(
                f"Input channels {C_in} != configured in_channels {self.in_channels}"
            )

        # channels-last -> channels-first
        x = x.permute(0, 4, 1, 2, 3).contiguous()  # (B, C_in, D, H, W)

        pad_d, pad_h, pad_w = self.padding
        if pad_d or pad_h or pad_w:
            # pad order for 5D (N, C, D, H, W):
            # (pad_w_left, pad_w_right, pad_h_left, pad_h_right, pad_d_left, pad_d_right)
            x = F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))
            D_p, H_p, W_p = D + pad_d, H + pad_h, W + pad_w
        else:
            D_p, H_p, W_p = D, H, W

        # Append coordinate grid if requested
        if self.use_grid:
            grid = self._make_grid(D_p, H_p, W_p, x.device)  # (1, 3, D_p, H_p, W_p)
            grid = grid.expand(B, -1, -1, -1, -1)
            x = torch.cat([x, grid], dim=1)  # (B, C_in+3, D_p, H_p, W_p)

        # Lift to width channels
        x = self.input_proj(x)  # (B, width, D_p, H_p, W_p)

        # Spectral blocks
        for spec_conv, w in zip(self.spectral_convs, self.ws):
            x1 = spec_conv(x)
            x2 = w(x)
            x = self.activation(x1 + x2)

        # Final per-point MLP
        x = self.output_layers(x)  # (B, C_out, D_p, H_p, W_p)

        # Remove padding if any
        if pad_d or pad_h or pad_w:
            x = x[:, :, :D, :H, :W]

        # channels-first -> channels-last
        x = x.permute(0, 2, 3, 4, 1).contiguous()  # (B, D, H, W, C_out)
        return x
</file>

<file path="models/fno/spectral_conv.py">
# models/fno/spectral_conv.py
import math
import torch
import torch.nn as nn


class SpectralConv1d(nn.Module):
    """
    1D spectral convolution layer used in FNO-1D.

    It applies a learnable linear mapping in Fourier space on the
    lowest `modes` Fourier modes and returns the inverse FFT.
    """

    def __init__(self, in_channels: int, out_channels: int, modes: int) -> None:
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes = modes

        scale = 1.0 / math.sqrt(in_channels * out_channels)
        # Only store weights for the lowest `modes` frequencies (positive half).
        self.weight = nn.Parameter(
            scale * torch.randn(in_channels, out_channels, modes, dtype=torch.cfloat)
        )

    @staticmethod
    def compl_mul1d(input: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:
        """
        Complex multiplication of Fourier coefficients.

        Args:
            input:  (B, C_in, L_modes)
            weight: (C_in, C_out, L_modes)

        Returns:
            out: (B, C_out, L_modes)
        """
        return torch.einsum("bcl, col -> bol", input, weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (B, C_in, L)

        Returns:
            out: (B, C_out, L)
        """
        B, C, L = x.shape

        # FFT
        x_ft = torch.fft.rfft(x, n=L, dim=-1, norm="ortho")  # (B, C_in, L_ft)
        L_ft = x_ft.size(-1)

        m = min(self.modes, L_ft)

        out_ft = torch.zeros(
            B,
            self.out_channels,
            L_ft,
            device=x.device,
            dtype=torch.cfloat,
        )

        if m > 0:
            out_ft[:, :, :m] = self.compl_mul1d(
                x_ft[:, :, :m],
                self.weight[:, :, :m],
            )

        # Inverse FFT
        x = torch.fft.irfft(out_ft, n=L, dim=-1, norm="ortho")
        return x
    

class SpectralConv2d(nn.Module):
    """
    2D spectral convolution layer used in Fourier Neural Operator (FNO).

    This layer:
      1) Transforms the input to Fourier domain via rFFT.
      2) Keeps only the lowest (modes_x, modes_y) Fourier modes.
      3) Applies learned complex-valued weights in the spectral domain.
      4) Transforms back to physical space via inverse rFFT.

    Args:
        in_channels (int):  Number of input channels.
        out_channels (int): Number of output channels.
        modes_x (int):      Number of Fourier modes to keep in the vertical (H) direction.
        modes_y (int):      Number of Fourier modes to keep in the horizontal (W) direction.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        modes_x: int,
        modes_y: int,
    ) -> None:
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes_x = modes_x
        self.modes_y = modes_y

        # Scale helps keep parameter magnitudes controlled at init
        scale = 1.0 / (in_channels * out_channels)

        # Weights for positive-low and negative-low frequency bands in H dimension.
        # Shape: (in_channels, out_channels, modes_x, modes_y)
        self.weight_pos = nn.Parameter(
            scale * torch.randn(in_channels, out_channels, modes_x, modes_y, dtype=torch.cfloat)
        )
        self.weight_neg = nn.Parameter(
            scale * torch.randn(in_channels, out_channels, modes_x, modes_y, dtype=torch.cfloat)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor of shape (B, C_in, H, W).

        Returns:
            Tensor of shape (B, C_out, H, W).
        """
        B, C_in, H, W = x.shape

        # rFFT over spatial dimensions (H, W)
        # Shape: (B, C_in, H, W//2 + 1), complex tensor
        x_ft = torch.fft.rfft2(x, norm="ortho")

        # Allocate output in Fourier domain
        out_ft = torch.zeros(
            B,
            self.out_channels,
            H,
            W // 2 + 1,
            device=x.device,
            dtype=torch.cfloat,
        )

        # How many modes can we actually use given the resolution?
        m1 = min(self.modes_x, H)
        m2 = min(self.modes_y, W // 2 + 1)

        # Positive-low frequencies in H
        out_ft[:, :, :m1, :m2] = torch.einsum(
            "bchw,cohw->bohw",
            x_ft[:, :, :m1, :m2],
            self.weight_pos[:, :, :m1, :m2],
        )

        # Negative-low frequencies in H
        out_ft[:, :, -m1:, :m2] = torch.einsum(
            "bchw,cohw->bohw",
            x_ft[:, :, -m1:, :m2],
            self.weight_neg[:, :, :m1, :m2],
        )

        # Back to physical space
        x_out = torch.fft.irfft2(out_ft, s=(H, W), norm="ortho")
        return x_out


class SpectralConv3d(nn.Module):
    """
    3D spectral convolution layer used in FNO-3D.

    We apply a learnable linear mapping in Fourier space on a set
    of low-frequency modes (modes1, modes2, modes3) and respect
    the rFFT symmetries: last dimension is half-spectrum, while
    we handle positive/negative corners in the first two dims.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        modes1: int,
        modes2: int,
        modes3: int,
    ) -> None:
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes1 = modes1
        self.modes2 = modes2
        self.modes3 = modes3

        scale = 1.0 / math.sqrt(in_channels * out_channels)
        # We use four sets of weights to cover +/- along the first two
        # frequency dimensions; last dimension uses half-spectrum.
        self.weight1 = nn.Parameter(
            scale * torch.randn(in_channels, out_channels, modes1, modes2, modes3, dtype=torch.cfloat)
        )
        self.weight2 = nn.Parameter(
            scale * torch.randn(in_channels, out_channels, modes1, modes2, modes3, dtype=torch.cfloat)
        )
        self.weight3 = nn.Parameter(
            scale * torch.randn(in_channels, out_channels, modes1, modes2, modes3, dtype=torch.cfloat)
        )
        self.weight4 = nn.Parameter(
            scale * torch.randn(in_channels, out_channels, modes1, modes2, modes3, dtype=torch.cfloat)
        )

    @staticmethod
    def compl_mul3d(input: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:
        """
        Complex multiplication for 3D Fourier coefficients.

        Args:
            input:  (B, C_in, D, H, W)
            weight: (C_in, C_out, D, H, W)

        Returns:
            out:    (B, C_out, D, H, W)
        """
        return torch.einsum("bcdhw, codhw -> bodhw", input, weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (B, C_in, D, H, W)

        Returns:
            out: (B, C_out, D, H, W)
        """
        B, C, D, H, W = x.shape

        # rFFT over the last 3 dimensions
        x_ft = torch.fft.rfftn(x, s=(D, H, W), dim=(-3, -2, -1), norm="ortho")
        D_ft, H_ft, W_ft = x_ft.shape[-3:]

        m1 = min(self.modes1, D_ft)
        m2 = min(self.modes2, H_ft)
        m3 = min(self.modes3, W_ft)

        out_ft = torch.zeros(
            B,
            self.out_channels,
            D_ft,
            H_ft,
            W_ft,
            device=x.device,
            dtype=torch.cfloat,
        )

        if m1 > 0 and m2 > 0 and m3 > 0:
            # Top-left corner
            out_ft[:, :, :m1, :m2, :m3] = self.compl_mul3d(
                x_ft[:, :, :m1, :m2, :m3],
                self.weight1[:, :, :m1, :m2, :m3],
            )
            # Bottom-left (negative along D)
            out_ft[:, :, -m1:, :m2, :m3] = self.compl_mul3d(
                x_ft[:, :, -m1:, :m2, :m3],
                self.weight2[:, :, :m1, :m2, :m3],
            )
            # Top-right (negative along H)
            out_ft[:, :, :m1, -m2:, :m3] = self.compl_mul3d(
                x_ft[:, :, :m1, -m2:, :m3],
                self.weight3[:, :, :m1, :m2, :m3],
            )
            # Bottom-right (negative along D and H)
            out_ft[:, :, -m1:, -m2:, :m3] = self.compl_mul3d(
                x_ft[:, :, -m1:, -m2:, :m3],
                self.weight4[:, :, :m1, :m2, :m3],
            )

        # Inverse rFFT
        x = torch.fft.irfftn(out_ft, s=(D, H, W), dim=(-3, -2, -1), norm="ortho")
        return x
</file>

<file path="models/galerkin_transformer/__init__.py">
from .galerkin_transformer import GalerkinTransformer
</file>

<file path="models/galerkin_transformer/basic.py">
import math
from typing import Optional

import torch
import torch.nn as nn
from einops import rearrange

ACTIVATION = {
    'gelu': nn.GELU,
    'tanh': nn.Tanh,
    'sigmoid': nn.Sigmoid,
    'relu': nn.ReLU,
    'leaky_relu': nn.LeakyReLU(0.1),
    'softplus': nn.Softplus,
    'ELU': nn.ELU,
    'silu': nn.SiLU
}


class MLP(nn.Module):
    def __init__(self, n_input: int, n_hidden: int, n_output: int, n_layers: int = 1,
                 act: str = 'gelu', res: bool = True) -> None:
        super(MLP, self).__init__()

        if act in ACTIVATION.keys():
            act_fn = ACTIVATION[act]
        else:
            raise NotImplementedError
        self.n_input = n_input
        self.n_hidden = n_hidden
        self.n_output = n_output
        self.n_layers = n_layers
        self.res = res
        self.linear_pre = nn.Sequential(nn.Linear(n_input, n_hidden), act_fn())
        self.linear_post = nn.Linear(n_hidden, n_output)
        self.linears = nn.ModuleList([nn.Sequential(nn.Linear(n_hidden, n_hidden), act_fn()) for _ in range(n_layers)])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.linear_pre(x)
        for i in range(self.n_layers):
            if self.res:
                x = self.linears[i](x) + x
            else:
                x = self.linears[i](x)
        x = self.linear_post(x)
        return x
    
    
class LinearAttention(nn.Module):
    def __init__(self, dim: int, heads: int = 8, dim_head: int = 64,
                 dropout: float = 0., attn_type: str = 'l1', **kwargs: Optional[dict]) -> None:
        super(LinearAttention, self).__init__()
        self.key = nn.Linear(dim, dim)
        self.query = nn.Linear(dim, dim)
        self.value = nn.Linear(dim, dim)
        self.attn_drop = nn.Dropout(dropout)
        self.proj = nn.Linear(dim, dim)
        self.n_head = heads
        self.dim_head = dim_head
        self.attn_type = attn_type

    def forward(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> torch.Tensor:
        if y is None:
            y = x
        else:
            y = y
        B, T1, _ = x.size()
        _, T2, _ = y.size()
        q = self.query(x).view(B, T1, self.n_head, self.dim_head).transpose(1, 2)
        k = self.key(y).view(B, T2, self.n_head, self.dim_head).transpose(1, 2)
        v = self.value(y).view(B, T2, self.n_head, self.dim_head).transpose(1, 2)

        if self.attn_type == 'l1':
            q = q.softmax(dim=-1)
            k = k.softmax(dim=-1)
            k_cumsum = k.sum(dim=-2, keepdim=True)
            D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)
        elif self.attn_type == "galerkin":
            q = q.softmax(dim=-1)
            k = k.softmax(dim=-1)
            D_inv = 1. / T2
        elif self.attn_type == "l2":
            q = q / q.norm(dim=-1, keepdim=True, p=1)
            k = k / k.norm(dim=-1, keepdim=True, p=1)
            k_cumsum = k.sum(dim=-2, keepdim=True)
            D_inv = 1. / (q * k_cumsum).abs().sum(dim=-1, keepdim=True)
        else:
            raise NotImplementedError

        context = k.transpose(-2, -1) @ v
        out = self.attn_drop((q @ context) * D_inv + q)
        out = rearrange(out, 'b h n d -> b n (h d)')
        out = self.proj(out)
        return out


def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False) -> torch.Tensor:
    """
    Create sinusoidal timestep embeddings.
    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param dim: the dimension of the output.
    :param max_period: controls the minimum frequency of the embeddings.
    :return: an [N x dim] Tensor of positional embeddings.
    """

    half = dim // 2
    freqs = torch.exp(
        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half
    ).to(device=timesteps.device)
    args = timesteps[:, None].float() * freqs[None]
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    if dim % 2:
        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
    return embedding
</file>

<file path="models/galerkin_transformer/galerkin_transformer.py">
import torch
import torch.nn as nn
from timm.layers.weight_init import trunc_normal_
from .basic import MLP, LinearAttention

import os.path as osp


class GalerkinTransformerBlock(nn.Module):
    """Transformer encoder block."""

    def __init__(
            self,
            num_heads: int,
            hidden_dim: int,
            dropout: float,
            act='gelu',
            mlp_ratio=4,
            last_layer=False,
            out_dim=1,
    ):
        super().__init__()
        self.__file__ = osp.abspath(__file__)
        self.last_layer = last_layer
        self.ln_1 = nn.LayerNorm(hidden_dim)
        self.ln_1a = nn.LayerNorm(hidden_dim)
        self.Attn = LinearAttention(hidden_dim, heads=num_heads, dim_head=hidden_dim // num_heads,
                                    dropout=dropout, attn_type='galerkin')
        self.ln_2 = nn.LayerNorm(hidden_dim)
        self.mlp = MLP(hidden_dim, hidden_dim * mlp_ratio, hidden_dim, n_layers=0, res=False, act=act)
        if self.last_layer:
            self.ln_3 = nn.LayerNorm(hidden_dim)
            self.mlp2 = nn.Linear(hidden_dim, out_dim)

    def forward(self, fx):
        fx = self.Attn(self.ln_1(fx), self.ln_1a(fx)) + fx
        fx = self.mlp(self.ln_2(fx)) + fx
        if self.last_layer:
            return self.mlp2(self.ln_3(fx))
        else:
            return fx


class GalerkinTransformer(nn.Module):
    def __init__(self, 
                 model_params: dict, 
                 **kwargs
                 ) -> None:
        super(GalerkinTransformer, self).__init__()
        self.__file__ = osp.abspath(__file__)
        self.__name__ = 'GalerkinTransformer'
        
        fun_dim = model_params.get('fun_dim', 1)
        space_dim = model_params.get('space_dim', 1)
        n_hidden = model_params.get('n_hidden', 128)
        out_dim = model_params.get('out_dim', 1)
        n_layers = model_params.get('n_layers', 6)
        n_heads = model_params.get('n_heads', 8)
        mlp_ratio = model_params.get('mlp_ratio', 4)
        act = model_params.get('act', 'gelu')
        dropout = model_params.get('dropout', 0.1)
        
        self.preprocess = MLP(fun_dim + space_dim, n_hidden * 2, n_hidden, n_layers=0, res=False, act=act)

        ## models
        self.blocks = nn.ModuleList([GalerkinTransformerBlock(num_heads=n_heads, hidden_dim=n_hidden,
                                                                dropout=dropout,
                                                                act=act,
                                                                mlp_ratio=mlp_ratio,
                                                                out_dim=out_dim,
                                                                last_layer=(_ == n_layers - 1)) for _ in range(n_layers)])
        self.placeholder = nn.Parameter((1 / (n_hidden)) * torch.rand(n_hidden, dtype=torch.float))
        self.initialize_weights()

    def initialize_weights(self):
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x):
        B, H, W, C = x.size()
        x = x.view(B, H * W, C)
        fx = self.preprocess(x)
        fx = fx + self.placeholder[None, None, :]

        for block in self.blocks:
            fx = block(fx)
            
        fx = fx.view(B, H, W, -1)
        return fx
</file>

<file path="models/gnot/__init__.py">
from .gnot import GNOT
</file>

<file path="models/gnot/basic.py">
import math

import torch
import torch.nn as nn
from einops import rearrange

ACTIVATION = {
    'gelu': nn.GELU,
    'tanh': nn.Tanh,
    'sigmoid': nn.Sigmoid,
    'relu': nn.ReLU,
    'leaky_relu': nn.LeakyReLU(0.1),
    'softplus': nn.Softplus,
    'ELU': nn.ELU,
    'silu': nn.SiLU
}


class MLP(nn.Module):
    def __init__(self, n_input, n_hidden, n_output, n_layers=1, act='gelu', res=True):
        super(MLP, self).__init__()

        if act in ACTIVATION.keys():
            act = ACTIVATION[act]
        else:
            raise NotImplementedError
        self.n_input = n_input
        self.n_hidden = n_hidden
        self.n_output = n_output
        self.n_layers = n_layers
        self.res = res
        self.linear_pre = nn.Sequential(nn.Linear(n_input, n_hidden), act())
        self.linear_post = nn.Linear(n_hidden, n_output)
        self.linears = nn.ModuleList([nn.Sequential(nn.Linear(n_hidden, n_hidden), act()) for _ in range(n_layers)])

    def forward(self, x):
        x = self.linear_pre(x)
        for i in range(self.n_layers):
            if self.res:
                x = self.linears[i](x) + x
            else:
                x = self.linears[i](x)
        x = self.linear_post(x)
        return x
    
    
class LinearAttention(nn.Module):
    """
    modified from https://github.com/HaoZhongkai/GNOT/blob/master/models/mmgpt.py
    """

    def __init__(self, dim, heads=8, dim_head=64, dropout=0., attn_type='l1', **kwargs):
        super(LinearAttention, self).__init__()
        self.key = nn.Linear(dim, dim)
        self.query = nn.Linear(dim, dim)
        self.value = nn.Linear(dim, dim)
        # regularization
        self.attn_drop = nn.Dropout(dropout)
        # output projection
        self.proj = nn.Linear(dim, dim)
        self.n_head = heads
        self.dim_head = dim_head
        self.attn_type = attn_type

    def forward(self, x, y=None):
        y = x if y is None else y
        B, T1, C = x.size()
        _, T2, _ = y.size()
        q = self.query(x).view(B, T1, self.n_head, self.dim_head).transpose(1, 2)  # (B, nh, T, hs)
        k = self.key(y).view(B, T2, self.n_head, self.dim_head).transpose(1, 2)  # (B, nh, T, hs)
        v = self.value(y).view(B, T2, self.n_head, self.dim_head).transpose(1, 2)  # (B, nh, T, hs)

        if self.attn_type == 'l1':
            q = q.softmax(dim=-1)
            k = k.softmax(dim=-1)
            k_cumsum = k.sum(dim=-2, keepdim=True)
            D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)  # normalized
        elif self.attn_type == "galerkin":
            q = q.softmax(dim=-1)
            k = k.softmax(dim=-1)
            D_inv = 1. / T2
        elif self.attn_type == "l2":  # still use l1 normalization
            q = q / q.norm(dim=-1, keepdim=True, p=1)
            k = k / k.norm(dim=-1, keepdim=True, p=1)
            k_cumsum = k.sum(dim=-2, keepdim=True)
            D_inv = 1. / (q * k_cumsum).abs().sum(dim=-1, keepdim=True)  # normalized
        else:
            raise NotImplementedError

        context = k.transpose(-2, -1) @ v
        y = self.attn_drop((q @ context) * D_inv + q)

        # output projection
        y = rearrange(y, 'b h n d -> b n (h d)')
        y = self.proj(y)
        return y


def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):
    """
    Create sinusoidal timestep embeddings.
    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param dim: the dimension of the output.
    :param max_period: controls the minimum frequency of the embeddings.
    :return: an [N x dim] Tensor of positional embeddings.
    """

    half = dim // 2
    freqs = torch.exp(
        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half
    ).to(device=timesteps.device)
    args = timesteps[:, None].float() * freqs[None]
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    if dim % 2:
        embedding = torch.cat([embedding, torch.zeros_like(embedding[:,:,:1])], dim=-1)
    return embedding
</file>

<file path="models/gnot/gnot.py">
import torch
import torch.nn as nn
from timm.layers.weight_init import trunc_normal_
from torch.nn import functional as F
from .basic import MLP, LinearAttention, ACTIVATION
from ..base import timestep_embedding, unified_pos_embedding


class GNOTBlock(nn.Module):
    """Transformer encoder block in MOE style."""

    def __init__(self, num_heads: int,
                 hidden_dim: int,
                 dropout: float,
                 act='gelu',
                 mlp_ratio=4,
                 space_dim=2,
                 n_experts=3):
        super(GNOTBlock, self).__init__()
        self.ln1 = nn.LayerNorm(hidden_dim)
        self.ln2 = nn.LayerNorm(hidden_dim)
        self.ln3 = nn.LayerNorm(hidden_dim)
        self.ln4 = nn.LayerNorm(hidden_dim)
        self.ln5 = nn.LayerNorm(hidden_dim)

        self.selfattn = LinearAttention(hidden_dim, heads=num_heads, dim_head=hidden_dim // num_heads, dropout=dropout)
        self.crossattn = LinearAttention(hidden_dim, heads=num_heads, dim_head=hidden_dim // num_heads, dropout=dropout)
        self.resid_drop1 = nn.Dropout(dropout)
        self.resid_drop2 = nn.Dropout(dropout)

        ## MLP in MOE
        self.n_experts = n_experts
        if act in ACTIVATION.keys():
            self.act = ACTIVATION[act]
        self.moe_mlp1 = nn.ModuleList([nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * mlp_ratio),
            self.act(),
            nn.Linear(hidden_dim * mlp_ratio, hidden_dim),
        ) for _ in range(self.n_experts)])

        self.moe_mlp2 = nn.ModuleList([nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * mlp_ratio),
            self.act(),
            nn.Linear(hidden_dim * mlp_ratio, hidden_dim),
        ) for _ in range(self.n_experts)])

        self.gatenet = nn.Sequential(
            nn.Linear(space_dim, hidden_dim * mlp_ratio),
            self.act(),
            nn.Linear(hidden_dim * mlp_ratio, hidden_dim * mlp_ratio),
            self.act(),
            nn.Linear(hidden_dim * mlp_ratio, self.n_experts)
        )

    def forward(self, x, y, pos):
        ## point-wise gate for moe
        gate_score = F.softmax(self.gatenet(pos), dim=-1).unsqueeze(2)
        ## cross attention between geo and physics observation
        x = x + self.resid_drop1(self.crossattn(self.ln1(x), self.ln2(y)))
        ## moe mlp
        x_moe1 = torch.stack([self.moe_mlp1[i](x) for i in range(self.n_experts)], dim=-1)
        x_moe1 = (gate_score * x_moe1).sum(dim=-1, keepdim=False)
        x = x + self.ln3(x_moe1)
        ## self attention among geo
        x = x + self.resid_drop2(self.selfattn(self.ln4(x)))
        ## moe mlp
        x_moe2 = torch.stack([self.moe_mlp2[i](x) for i in range(self.n_experts)], dim=-1)
        x_moe2 = (gate_score * x_moe2).sum(dim=-1, keepdim=False)
        x = x + self.ln5(x_moe2)
        return x


class GNOT(nn.Module):
    ## GNOT: Transformer in MOE style
    def __init__(self, model_params, n_experts=3):
        super(GNOT, self).__init__()
        self.__name__ = 'GNOT'
        for key, value in model_params.items():
            setattr(self, key, value)
        ## embedding
        if self.unified_pos and self.geotype != 'unstructured':  # only for structured mesh
            self.pos = unified_pos_embedding(self.shapelist, self.ref)
            self.preprocess_x = MLP(self.ref ** len(self.shapelist), self.n_hidden * 2,
                                    self.n_hidden, n_layers=0, res=False, act=self.act)
            self.preprocess_z = MLP(self.fun_dim + self.ref ** len(self.shapelist), self.n_hidden * 2,
                                    self.n_hidden, n_layers=0, res=False, act=self.act)
        else:
            self.preprocess_x = MLP(self.space_dim, self.n_hidden * 2, self.n_hidden,
                                    n_layers=0, res=False, act=self.act)
            self.preprocess_z = MLP(self.fun_dim + self.space_dim, self.n_hidden * 2, self.n_hidden,
                                    n_layers=0, res=False, act=self.act)
        if self.time_input:
            self.time_fc = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden), nn.SiLU(),
                                         nn.Linear(self.n_hidden, self.n_hidden))

        ## models
        self.blocks = nn.ModuleList([GNOTBlock(num_heads=self.n_heads,
                                                hidden_dim=self.n_hidden,
                                                dropout=self.dropout,
                                                act=self.act,
                                                mlp_ratio=self.mlp_ratio,
                                                space_dim=self.space_dim,
                                                n_experts=n_experts)
                                     for _ in range(self.n_layers)])
        self.placeholder = nn.Parameter((1 / (self.n_hidden)) * torch.rand(self.n_hidden, dtype=torch.float))
        # projectors
        self.fc1 = nn.Linear(self.n_hidden, self.n_hidden * 2)
        self.fc2 = nn.Linear(self.n_hidden * 2, self.out_dim)
        self.initialize_weights()

    def initialize_weights(self):
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x, fx=None, T=None, geo=None):
        x = x.view(x.shape[0], -1, x.shape[-1])
        pos = x[..., -self.space_dim:]
        if self.unified_pos:
            x = self.pos.repeat(x.shape[0], 1, 1)
        if fx is not None:
            fx = torch.cat((x, fx), -1)
            fx = self.preprocess_z(fx)
        else:
            fx = self.preprocess_z(x)
        fx = fx + self.placeholder[None, None, :]
        x = self.preprocess_x(x[..., -self.space_dim:])
        if T is not None:
            Time_emb = timestep_embedding(T, self.n_hidden).repeat(1, x.shape[1], 1)
            Time_emb = self.time_fc(Time_emb)
            fx = fx + Time_emb

        for block in self.blocks:
            fx = block(x, fx, pos)
        fx = self.fc1(fx)
        fx = F.gelu(fx)
        fx = self.fc2(fx)
        return fx
</file>

<file path="models/lsm/__init__.py">
from .lsm import LSM
</file>

<file path="models/lsm/basic.py">
import torch.nn as nn

ACTIVATION = {
    'gelu': nn.GELU,
    'tanh': nn.Tanh,
    'sigmoid': nn.Sigmoid,
    'relu': nn.ReLU,
    'leaky_relu': nn.LeakyReLU(0.1),
    'softplus': nn.Softplus,
    'ELU': nn.ELU,
    'silu': nn.SiLU
}


class MLP(nn.Module):
    def __init__(self, n_input, n_hidden, n_output, n_layers=1, act='gelu', res=True):
        super(MLP, self).__init__()

        if act in ACTIVATION.keys():
            act = ACTIVATION[act]
        else:
            raise NotImplementedError
        self.n_input = n_input
        self.n_hidden = n_hidden
        self.n_output = n_output
        self.n_layers = n_layers
        self.res = res
        self.linear_pre = nn.Sequential(nn.Linear(n_input, n_hidden), act())
        self.linear_post = nn.Linear(n_hidden, n_output)
        self.linears = nn.ModuleList([nn.Sequential(nn.Linear(n_hidden, n_hidden), act()) for _ in range(n_layers)])

    def forward(self, x):
        x = self.linear_pre(x)
        for i in range(self.n_layers):
            if self.res:
                x = self.linears[i](x) + x
            else:
                x = self.linears[i](x)
        x = self.linear_post(x)
        return x
</file>

<file path="models/lsm/geo_projection.py">
import torch.nn as nn
import torch
import numpy as np


################################################################
# geo projection
################################################################
class SpectralConv2d_IrregularGeo(nn.Module):
    def __init__(self, in_channels, out_channels, modes1, modes2, s1=32, s2=32):
        super(SpectralConv2d_IrregularGeo, self).__init__()

        """
        from geoFNO    
        """

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes1 = modes1  # Number of Fourier modes to multiply, at most floor(N/2) + 1
        self.modes2 = modes2
        self.s1 = s1
        self.s2 = s2

        self.scale = (1 / (in_channels * out_channels))
        self.weights1 = nn.Parameter(
            self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))
        self.weights2 = nn.Parameter(
            self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))

    # Complex multiplication
    def compl_mul2d(self, input, weights):
        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)
        return torch.einsum("bixy,ioxy->boxy", input, weights)

    def forward(self, u, x_in=None, x_out=None, iphi=None, code=None):
        batchsize = u.shape[0]

        # Compute Fourier coeffcients up to factor of e^(- something constant)
        if x_in == None:
            u_ft = torch.fft.rfft2(u)
            s1 = u.size(-2)
            s2 = u.size(-1)
        else:
            u_ft = self.fft2d(u, x_in, iphi, code)
            s1 = self.s1
            s2 = self.s2

        # Multiply relevant Fourier modes
        # print(u.shape, u_ft.shape)
        factor1 = self.compl_mul2d(u_ft[:, :, :self.modes1, :self.modes2], self.weights1)
        factor2 = self.compl_mul2d(u_ft[:, :, -self.modes1:, :self.modes2], self.weights2)

        # Return to physical space
        if x_out == None:
            out_ft = torch.zeros(batchsize, self.out_channels, s1, s2 // 2 + 1, dtype=torch.cfloat, device=u.device)
            out_ft[:, :, :self.modes1, :self.modes2] = factor1
            out_ft[:, :, -self.modes1:, :self.modes2] = factor2
            u = torch.fft.irfft2(out_ft, s=(s1, s2))
        else:
            out_ft = torch.cat([factor1, factor2], dim=-2)
            u = self.ifft2d(out_ft, x_out, iphi, code)

        return u

    def fft2d(self, u, x_in, iphi=None, code=None):
        # u (batch, channels, n)
        # x_in (batch, n, 2) locations in [0,1]*[0,1]
        # iphi: function: x_in -> x_c

        batchsize = x_in.shape[0]
        N = x_in.shape[1]
        device = x_in.device
        m1 = 2 * self.modes1
        m2 = 2 * self.modes2 - 1

        # wavenumber (m1, m2)
        k_x1 = torch.cat((torch.arange(start=0, end=self.modes1, step=1), \
                          torch.arange(start=-(self.modes1), end=0, step=1)), 0).reshape(m1, 1).repeat(1, m2).to(device)
        k_x2 = torch.cat((torch.arange(start=0, end=self.modes2, step=1), \
                          torch.arange(start=-(self.modes2 - 1), end=0, step=1)), 0).reshape(1, m2).repeat(m1, 1).to(
            device)

        if iphi == None:
            x = x_in
        else:
            x = iphi(x_in, code)

        # K = <y, k_x>,  (batch, N, m1, m2)
        K1 = torch.outer(x[..., 0].view(-1), k_x1.view(-1)).reshape(batchsize, N, m1, m2)
        K2 = torch.outer(x[..., 1].view(-1), k_x2.view(-1)).reshape(batchsize, N, m1, m2)
        K = K1 + K2

        # basis (batch, N, m1, m2)
        basis = torch.exp(-1j * 2 * np.pi * K).to(device)

        # Y (batch, channels, N)
        u = u + 0j
        Y = torch.einsum("bcn,bnxy->bcxy", u, basis)
        return Y

    def ifft2d(self, u_ft, x_out, iphi=None, code=None):
        # u_ft (batch, channels, kmax, kmax)
        # x_out (batch, N, 2) locations in [0,1]*[0,1]
        # iphi: function: x_out -> x_c

        batchsize = x_out.shape[0]
        N = x_out.shape[1]
        device = x_out.device
        m1 = 2 * self.modes1
        m2 = 2 * self.modes2 - 1

        # wavenumber (m1, m2)
        k_x1 = torch.cat((torch.arange(start=0, end=self.modes1, step=1), \
                          torch.arange(start=-(self.modes1), end=0, step=1)), 0).reshape(m1, 1).repeat(1, m2).to(device)
        k_x2 = torch.cat((torch.arange(start=0, end=self.modes2, step=1), \
                          torch.arange(start=-(self.modes2 - 1), end=0, step=1)), 0).reshape(1, m2).repeat(m1, 1).to(
            device)

        if iphi == None:
            x = x_out
        else:
            x = iphi(x_out, code)

        # K = <y, k_x>,  (batch, N, m1, m2)
        K1 = torch.outer(x[:, :, 0].view(-1), k_x1.view(-1)).reshape(batchsize, N, m1, m2)
        K2 = torch.outer(x[:, :, 1].view(-1), k_x2.view(-1)).reshape(batchsize, N, m1, m2)
        K = K1 + K2

        # basis (batch, N, m1, m2)
        basis = torch.exp(1j * 2 * np.pi * K).to(device)

        # coeff (batch, channels, m1, m2)
        u_ft2 = u_ft[..., 1:].flip(-1, -2).conj()
        u_ft = torch.cat([u_ft, u_ft2], dim=-1)

        # Y (batch, channels, N)
        Y = torch.einsum("bcxy,bnxy->bcn", u_ft, basis)
        Y = Y.real
        return Y


class IPHI(nn.Module):
    def __init__(self, width=32):
        super(IPHI, self).__init__()

        """
        inverse phi: x -> xi
        """
        self.width = width
        self.fc0 = nn.Linear(4, self.width)
        self.fc_code = nn.Linear(42, self.width)
        self.fc_no_code = nn.Linear(3 * self.width, 4 * self.width)
        self.fc1 = nn.Linear(4 * self.width, 4 * self.width)
        self.fc2 = nn.Linear(4 * self.width, 4 * self.width)
        self.fc3 = nn.Linear(4 * self.width, 4 * self.width)
        self.fc4 = nn.Linear(4 * self.width, 2)
        self.activation = torch.tanh
        self.center = torch.tensor([0.0001, 0.0001], device="cuda").reshape(1, 1, 2)

        self.B = np.pi * torch.pow(2, torch.arange(0, self.width // 4, dtype=torch.float, device="cuda")).reshape(1, 1,
                                                                                                                  1,
                                                                                                                  self.width // 4)

    def forward(self, x, code=None):
        # x (batch, N_grid, 2)
        # code (batch, N_features)

        # some feature engineering
        angle = torch.atan2(x[:, :, 1] - self.center[:, :, 1], x[:, :, 0] - self.center[:, :, 0])
        radius = torch.norm(x - self.center, dim=-1, p=2)
        xd = torch.stack([x[:, :, 0], x[:, :, 1], angle, radius], dim=-1)

        # sin features from NeRF
        b, n, d = xd.shape[0], xd.shape[1], xd.shape[2]
        x_sin = torch.sin(self.B * xd.view(b, n, d, 1)).view(b, n, d * self.width // 4)
        x_cos = torch.cos(self.B * xd.view(b, n, d, 1)).view(b, n, d * self.width // 4)
        xd = self.fc0(xd)
        xd = torch.cat([xd, x_sin, x_cos], dim=-1).reshape(b, n, 3 * self.width)

        if code != None:
            cd = self.fc_code(code)
            cd = cd.unsqueeze(1).repeat(1, xd.shape[1], 1)
            xd = torch.cat([cd, xd], dim=-1)
        else:
            xd = self.fc_no_code(xd)

        xd = self.fc1(xd)
        xd = self.activation(xd)
        xd = self.fc2(xd)
        xd = self.activation(xd)
        xd = self.fc3(xd)
        xd = self.activation(xd)
        xd = self.fc4(xd)
        return x + x * xd
</file>

<file path="models/lsm/lsm.py">
import torch
import torch.nn as nn
import torch.nn.functional as F
from .basic import MLP
from ..base import timestep_embedding, unified_pos_embedding
from .neural_spectral_block import NeuralSpectralBlock1D, NeuralSpectralBlock2D, NeuralSpectralBlock3D
from .unet_block import DoubleConv1D, Down1D, Up1D, OutConv1D, DoubleConv2D, Down2D, Up2D, OutConv2D, \
    DoubleConv3D, Down3D, Up3D, OutConv3D
from .geo_projection import SpectralConv2d_IrregularGeo, IPHI

ConvList = [None, DoubleConv1D, DoubleConv2D, DoubleConv3D]
DownList = [None, Down1D, Down2D, Down3D]
UpList = [None, Up1D, Up2D, Up3D]
OutList = [None, OutConv1D, OutConv2D, OutConv3D]
BlockList = [None, NeuralSpectralBlock1D, NeuralSpectralBlock2D, NeuralSpectralBlock3D]


class LSM(nn.Module):
    def __init__(self, model_params: dict):
        super(LSM, self).__init__()
        self.__name__ = 'LSM'
        
        for key, value in model_params.items():
            setattr(self, key, value)
        
        if model_params['task'] == 'steady':
            normtype = 'bn'
        else:
            normtype = 'in' # when conducting dynamic tasks, use instance norm for stability
        ## embedding
        if model_params['unified_pos'] and model_params['geotype'] != 'unstructured':  # only for structured mesh
            self.pos = unified_pos_embedding(model_params['shapelist'], model_params['ref'])
            self.preprocess = MLP(model_params['fun_dim'] + model_params['ref'] ** len(model_params['shapelist']), model_params['n_hidden'] * 2,
                                  model_params['n_hidden'], n_layers=0, res=False, act=model_params['act'])
        else:
            self.preprocess = MLP(model_params['fun_dim'] + model_params['space_dim'], model_params['n_hidden'] * 2, model_params['n_hidden'],
                                  n_layers=0, res=False, act=self.act)
        if self.time_input:
            self.time_fc = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden), nn.SiLU(),
                                         nn.Linear(self.n_hidden, self.n_hidden))
        # geometry projection
        if self.geotype == 'unstructured':
            self.fftproject_in = SpectralConv2d_IrregularGeo(self.n_hidden, self.n_hidden, self.modes, self.modes,
                                                             self.s1, self.s2)
            self.fftproject_out = SpectralConv2d_IrregularGeo(self.n_hidden, self.n_hidden, self.modes, self.modes,
                                                              self.s1, self.s2)
            self.iphi = IPHI()
            patch_size = [(size + (16 - size % 16) % 16) // 16 for size in [self.s1, self.s2]]
            self.padding = [(16 - size % 16) % 16 for size in [self.s1, self.s2]]
        else:
            patch_size = [(size + (16 - size % 16) % 16) // 16 for size in self.shapelist]
            self.padding = [(16 - size % 16) % 16 for size in self.shapelist]
        # multiscale modules
        self.inc = ConvList[len(patch_size)](self.n_hidden, self.n_hidden, normtype=normtype)
        self.down1 = DownList[len(patch_size)](self.n_hidden, self.n_hidden * 2, normtype=normtype)
        self.down2 = DownList[len(patch_size)](self.n_hidden * 2, self.n_hidden * 4, normtype=normtype)
        self.down3 = DownList[len(patch_size)](self.n_hidden * 4, self.n_hidden * 8, normtype=normtype)
        factor = 2 if self.bilinear else 1
        self.down4 = DownList[len(patch_size)](self.n_hidden * 8, self.n_hidden * 16 // factor, normtype=normtype)
        self.up1 = UpList[len(patch_size)](self.n_hidden * 16, self.n_hidden * 8 // factor, self.bilinear, normtype=normtype)
        self.up2 = UpList[len(patch_size)](self.n_hidden * 8, self.n_hidden * 4 // factor, self.bilinear, normtype=normtype)
        self.up3 = UpList[len(patch_size)](self.n_hidden * 4, self.n_hidden * 2 // factor, self.bilinear, normtype=normtype)
        self.up4 = UpList[len(patch_size)](self.n_hidden * 2, self.n_hidden, self.bilinear, normtype=normtype)
        self.outc = OutList[len(patch_size)](self.n_hidden, self.n_hidden)
        # Patchified Neural Spectral Blocks
        self.process1 = BlockList[len(patch_size)](self.n_hidden, self.num_basis, patch_size, self.num_token, self.n_heads)
        self.process2 = BlockList[len(patch_size)](self.n_hidden * 2, self.num_basis, patch_size, self.num_token, self.n_heads)
        self.process3 = BlockList[len(patch_size)](self.n_hidden * 4, self.num_basis, patch_size, self.num_token, self.n_heads)
        self.process4 = BlockList[len(patch_size)](self.n_hidden * 8, self.num_basis, patch_size, self.num_token, self.n_heads)
        self.process5 = BlockList[len(patch_size)](self.n_hidden * 16 // factor, self.num_basis, patch_size, self.num_token,
                                                   self.n_heads)
        # projectors
        self.fc1 = nn.Linear(self.n_hidden, self.n_hidden * 2)
        self.fc2 = nn.Linear(self.n_hidden * 2, self.out_dim)

    def structured_geo(self, x, fx, T=None):
        B, N, _ = x.shape
        if self.unified_pos:
            x = self.pos.repeat(x.shape[0], 1, 1)
        if fx is not None:
            fx = torch.cat((x, fx), -1)
            fx = self.preprocess(fx)
        else:
            fx = self.preprocess(x)

        if T is not None:
            Time_emb = timestep_embedding(T, self.n_hidden).repeat(1, x.shape[1], 1)
            Time_emb = self.time_fc(Time_emb)
            fx = fx + Time_emb
        x = fx.permute(0, 2, 1).reshape(B, self.n_hidden, *self.shapelist)
        if not all(item == 0 for item in self.padding):
            if len(self.shapelist) == 2:
                x = F.pad(x, [0, self.padding[1], 0, self.padding[0]])
            elif len(self.shapelist) == 3:
                x = F.pad(x, [0, self.padding[2], 0, self.padding[1], 0, self.padding[0]])
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(self.process5(x5), self.process4(x4))
        x = self.up2(x, self.process3(x3))
        x = self.up3(x, self.process2(x2))
        x = self.up4(x, self.process1(x1))
        x = self.outc(x)

        if not all(item == 0 for item in self.padding):
            if len(self.shapelist) == 2:
                x = x[..., :-self.padding[0], :-self.padding[1]]
            elif len(self.shapelist) == 3:
                x = x[..., :-self.padding[0], :-self.padding[1], :-self.padding[2]]
        x = x.reshape(B, self.n_hidden, -1).permute(0, 2, 1)
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        return x

    def unstructured_geo(self, x, fx=None, T=None):
        original_pos = x
        if fx is not None:
            fx = torch.cat((x, fx), -1)
            fx = self.preprocess(fx)
        else:
            fx = self.preprocess(x)

        if T is not None:
            Time_emb = timestep_embedding(T, self.n_hidden).repeat(1, x.shape[1], 1)
            Time_emb = self.time_fc(Time_emb)
            fx = fx + Time_emb

        x = self.fftproject_in(fx.permute(0, 2, 1), x_in=original_pos, iphi=self.iphi, code=None)
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(self.process5(x5), self.process4(x4))
        x = self.up2(x, self.process3(x3))
        x = self.up3(x, self.process2(x2))
        x = self.up4(x, self.process1(x1))
        x = self.outc(x)
        x = self.fftproject_out(x, x_out=original_pos, iphi=self.iphi, code=None).permute(0, 2, 1)
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        return x

    def forward(self, x, fx=None, T=None, geo=None):
        x = x.view(x.size(0), -1, x.size(-1))
        if self.geotype == 'unstructured':
            return self.unstructured_geo(x, fx, T)
        else:
            return self.structured_geo(x, fx, T)
</file>

<file path="models/lsm/neural_spectral_block.py">
import torch
import math
import torch.nn as nn


################################################################
# Patchify and Neural Spectral Block 2D
################################################################
class NeuralSpectralBlock1D(nn.Module):
    def __init__(self, width, num_basis, patch_size=[3, 3], num_token=4, n_heads=8):
        super(NeuralSpectralBlock1D, self).__init__()
        self.patch_size = patch_size
        self.width = width
        self.num_basis = num_basis

        # basis
        self.modes_list = (1.0 / float(num_basis)) * torch.tensor([i for i in range(num_basis)],
                                                                  dtype=torch.float).cuda()
        self.weights = nn.Parameter(
            (1 / (width)) * torch.rand(width, self.num_basis * 2, dtype=torch.float))
        # latent
        self.head = n_heads
        self.num_token = num_token
        self.latent = nn.Parameter(
            (1 / (width)) * torch.rand(self.head, self.num_token, width // self.head, dtype=torch.float))
        self.encoder_attn = nn.Conv1d(self.width, self.width * 2, kernel_size=1, stride=1)
        self.decoder_attn = nn.Conv1d(self.width, self.width, kernel_size=1, stride=1)
        self.softmax = nn.Softmax(dim=-1)

    def self_attn(self, q, k, v):
        # q,k,v: B H L C/H
        attn = self.softmax(torch.einsum("bhlc,bhsc->bhls", q, k))
        return torch.einsum("bhls,bhsc->bhlc", attn, v)

    def latent_encoder_attn(self, x):
        # x: B C H W
        B, C, L = x.shape
        latent_token = self.latent[None, :, :, :].repeat(B, 1, 1, 1)
        x_tmp = self.encoder_attn(x).view(B, C * 2, -1).permute(0, 2, 1).contiguous() \
            .view(B, L, self.head, C // self.head, 2).permute(4, 0, 2, 1, 3).contiguous()
        latent_token = self.self_attn(latent_token, x_tmp[0], x_tmp[1]) + latent_token
        latent_token = latent_token.permute(0, 1, 3, 2).contiguous().view(B, C, self.num_token)
        return latent_token

    def latent_decoder_attn(self, x, latent_token):
        # x: B C L
        x_init = x
        B, C, L = x.shape
        latent_token = latent_token.view(B, self.head, C // self.head, self.num_token).permute(0, 1, 3, 2).contiguous()
        x_tmp = self.decoder_attn(x).view(B, C, -1).permute(0, 2, 1).contiguous() \
            .view(B, L, self.head, C // self.head).permute(0, 2, 1, 3).contiguous()
        x = self.self_attn(x_tmp, latent_token, latent_token)
        x = x.permute(0, 1, 3, 2).contiguous().view(B, C, L) + x_init  # B H L C/H
        return x

    def get_basis(self, x):
        # x: B C N
        x_sin = torch.sin(self.modes_list[None, None, None, :] * x[:, :, :, None] * math.pi)
        x_cos = torch.cos(self.modes_list[None, None, None, :] * x[:, :, :, None] * math.pi)
        return torch.cat([x_sin, x_cos], dim=-1)

    def compl_mul2d(self, input, weights):
        return torch.einsum("bilm,im->bil", input, weights)

    def forward(self, x):
        B, C, L = x.shape
        # patchify
        x = x.view(x.shape[0], x.shape[1],
                   x.shape[2] // self.patch_size[0], self.patch_size[0]).contiguous() \
            .permute(0, 2, 1, 3).contiguous() \
            .view(x.shape[0] * (x.shape[2] // self.patch_size[0]), x.shape[1], self.patch_size[0])
        # Neural Spectral
        # (1) encoder
        latent_token = self.latent_encoder_attn(x)
        # (2) transition
        latent_token_modes = self.get_basis(latent_token)
        latent_token = self.compl_mul2d(latent_token_modes, self.weights) + latent_token
        # (3) decoder
        x = self.latent_decoder_attn(x, latent_token)
        # de-patchify
        x = x.view(B, (L // self.patch_size[0]), C, self.patch_size[0]).permute(0, 2, 1, 3).contiguous() \
            .view(B, C, L).contiguous()
        return x


################################################################
# Patchify and Neural Spectral Block 2D
################################################################
class NeuralSpectralBlock2D(nn.Module):
    def __init__(self, width, num_basis, patch_size=[3, 3], num_token=4, n_heads=8):
        super(NeuralSpectralBlock2D, self).__init__()
        self.patch_size = patch_size
        self.width = width
        self.num_basis = num_basis

        # basis
        self.modes_list = (1.0 / float(num_basis)) * torch.tensor([i for i in range(num_basis)],
                                                                  dtype=torch.float).cuda()
        self.weights = nn.Parameter(
            (1 / (width)) * torch.rand(width, self.num_basis * 2, dtype=torch.float))
        # latent
        self.head = n_heads
        self.num_token = num_token
        self.latent = nn.Parameter(
            (1 / (width)) * torch.rand(self.head, self.num_token, width // self.head, dtype=torch.float))
        self.encoder_attn = nn.Conv2d(self.width, self.width * 2, kernel_size=1, stride=1)
        self.decoder_attn = nn.Conv2d(self.width, self.width, kernel_size=1, stride=1)
        self.softmax = nn.Softmax(dim=-1)

    def self_attn(self, q, k, v):
        # q,k,v: B H L C/H
        attn = self.softmax(torch.einsum("bhlc,bhsc->bhls", q, k))
        return torch.einsum("bhls,bhsc->bhlc", attn, v)

    def latent_encoder_attn(self, x):
        # x: B C H W
        B, C, H, W = x.shape
        L = H * W
        latent_token = self.latent[None, :, :, :].repeat(B, 1, 1, 1)
        x_tmp = self.encoder_attn(x).view(B, C * 2, -1).permute(0, 2, 1).contiguous() \
            .view(B, L, self.head, C // self.head, 2).permute(4, 0, 2, 1, 3).contiguous()
        latent_token = self.self_attn(latent_token, x_tmp[0], x_tmp[1]) + latent_token
        latent_token = latent_token.permute(0, 1, 3, 2).contiguous().view(B, C, self.num_token)
        return latent_token

    def latent_decoder_attn(self, x, latent_token):
        # x: B C L
        x_init = x
        B, C, H, W = x.shape
        L = H * W
        latent_token = latent_token.view(B, self.head, C // self.head, self.num_token).permute(0, 1, 3, 2).contiguous()
        x_tmp = self.decoder_attn(x).view(B, C, -1).permute(0, 2, 1).contiguous() \
            .view(B, L, self.head, C // self.head).permute(0, 2, 1, 3).contiguous()
        x = self.self_attn(x_tmp, latent_token, latent_token)
        x = x.permute(0, 1, 3, 2).contiguous().view(B, C, H, W) + x_init  # B H L C/H
        return x

    def get_basis(self, x):
        # x: B C N
        x_sin = torch.sin(self.modes_list[None, None, None, :] * x[:, :, :, None] * math.pi)
        x_cos = torch.cos(self.modes_list[None, None, None, :] * x[:, :, :, None] * math.pi)
        return torch.cat([x_sin, x_cos], dim=-1)

    def compl_mul2d(self, input, weights):
        return torch.einsum("bilm,im->bil", input, weights)

    def forward(self, x):
        B, C, H, W = x.shape
        # patchify
        x = x.view(x.shape[0], x.shape[1],
                   x.shape[2] // self.patch_size[0], self.patch_size[0], x.shape[3] // self.patch_size[1],
                   self.patch_size[1]).contiguous() \
            .permute(0, 2, 4, 1, 3, 5).contiguous() \
            .view(x.shape[0] * (x.shape[2] // self.patch_size[0]) * (x.shape[3] // self.patch_size[1]), x.shape[1],
                  self.patch_size[0],
                  self.patch_size[1])
        # Neural Spectral
        # (1) encoder
        latent_token = self.latent_encoder_attn(x)
        # (2) transition
        latent_token_modes = self.get_basis(latent_token)
        latent_token = self.compl_mul2d(latent_token_modes, self.weights) + latent_token
        # (3) decoder
        x = self.latent_decoder_attn(x, latent_token)
        # de-patchify
        x = x.view(B, (H // self.patch_size[0]), (W // self.patch_size[1]), C, self.patch_size[0],
                   self.patch_size[1]).permute(0, 3, 1, 4, 2, 5).contiguous() \
            .view(B, C, H, W).contiguous()
        return x


################################################################
# Patchify and Neural Spectral Block 3D
################################################################
class NeuralSpectralBlock3D(nn.Module):
    def __init__(self, width, num_basis, patch_size=[8, 8, 4], num_token=4, n_heads=8):
        super(NeuralSpectralBlock3D, self).__init__()
        self.patch_size = patch_size
        self.width = width
        self.num_basis = num_basis

        # basis
        self.modes_list = (1.0 / float(num_basis)) * torch.tensor([i for i in range(num_basis)],
                                                                  dtype=torch.float).cuda()
        self.weights = nn.Parameter(
            (1 / (width)) * torch.rand(width, self.num_basis * 2, dtype=torch.float))
        # latent
        self.head = n_heads
        self.num_token = num_token
        self.latent = nn.Parameter(
            (1 / (width)) * torch.rand(self.head, self.num_token, width // self.head, dtype=torch.float))
        self.encoder_attn = nn.Conv3d(self.width, self.width * 2, kernel_size=1, stride=1)
        self.decoder_attn = nn.Conv3d(self.width, self.width, kernel_size=1, stride=1)
        self.softmax = nn.Softmax(dim=-1)

    def self_attn(self, q, k, v):
        # q,k,v: B H L C/H
        attn = self.softmax(torch.einsum("bhlc,bhsc->bhls", q, k))
        return torch.einsum("bhls,bhsc->bhlc", attn, v)

    def latent_encoder_attn(self, x):
        # x: B C H W
        B, C, H, W, T = x.shape
        L = H * W * T
        latent_token = self.latent[None, :, :, :].repeat(B, 1, 1, 1)
        x_tmp = self.encoder_attn(x).view(B, C * 2, -1).permute(0, 2, 1).contiguous() \
            .view(B, L, self.head, C // self.head, 2).permute(4, 0, 2, 1, 3).contiguous()
        latent_token = self.self_attn(latent_token, x_tmp[0], x_tmp[1]) + latent_token
        latent_token = latent_token.permute(0, 1, 3, 2).contiguous().view(B, C, self.num_token)
        return latent_token

    def latent_decoder_attn(self, x, latent_token):
        # x: B C L
        x_init = x
        B, C, H, W, T = x.shape
        L = H * W * T
        latent_token = latent_token.view(B, self.head, C // self.head, self.num_token).permute(0, 1, 3, 2).contiguous()
        x_tmp = self.decoder_attn(x).view(B, C, -1).permute(0, 2, 1).contiguous() \
            .view(B, L, self.head, C // self.head).permute(0, 2, 1, 3).contiguous()
        x = self.self_attn(x_tmp, latent_token, latent_token)
        x = x.permute(0, 1, 3, 2).contiguous().view(B, C, H, W, T) + x_init  # B H L C/H
        return x

    def get_basis(self, x):
        # x: B C N
        x_sin = torch.sin(self.modes_list[None, None, None, :] * x[:, :, :, None] * math.pi)
        x_cos = torch.cos(self.modes_list[None, None, None, :] * x[:, :, :, None] * math.pi)
        return torch.cat([x_sin, x_cos], dim=-1)

    def compl_mul2d(self, input, weights):
        return torch.einsum("bilm,im->bil", input, weights)

    def forward(self, x):
        B, C, H, W, T = x.shape
        # patchify
        x = x.view(x.shape[0], x.shape[1],
                   x.shape[2] // self.patch_size[0], self.patch_size[0], x.shape[3] // self.patch_size[1],
                   self.patch_size[1],
                   x.shape[4] // self.patch_size[2], self.patch_size[2]).contiguous() \
            .permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous() \
            .view(x.shape[0] * (x.shape[2] // self.patch_size[0]) * (x.shape[3] // self.patch_size[1]) * (
                x.shape[4] // self.patch_size[2]), x.shape[1], self.patch_size[0], self.patch_size[1],
                  self.patch_size[2])
        # Neural Spectral
        # (1) encoder
        latent_token = self.latent_encoder_attn(x)
        # (2) transition
        latent_token_modes = self.get_basis(latent_token)
        latent_token = self.compl_mul2d(latent_token_modes, self.weights) + latent_token
        # (3) decoder
        x = self.latent_decoder_attn(x, latent_token)
        # de-patchify
        x = x.view(B, (H // self.patch_size[0]), (W // self.patch_size[1]), (T // self.patch_size[2]), C,
                   self.patch_size[0], self.patch_size[1], self.patch_size[2]).permute(0, 4, 1, 5, 2, 6, 3,
                                                                                       7).contiguous() \
            .view(B, C, H, W, T).contiguous()
        return x
</file>

<file path="models/lsm/unet_block.py">
import torch
import torch.nn as nn
import torch.nn.functional as F


################################################################
# Multiscale modules 1D
################################################################

class DoubleConv1D(nn.Module):
    """(convolution => [BN] => ReLU) * 2"""

    def __init__(self, in_channels, out_channels, mid_channels=None, normtype='bn'):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        if normtype == 'bn':
            self.double_conv = nn.Sequential(
                nn.Conv1d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm1d(mid_channels),
                nn.ReLU(inplace=True),
                nn.Conv1d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm1d(out_channels),
                nn.ReLU(inplace=True)
            )
        elif normtype == 'in':
            self.double_conv = nn.Sequential(
                nn.Conv1d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                nn.InstanceNorm1d(mid_channels, affine=True),
                nn.ReLU(inplace=True),
                nn.Conv1d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                nn.InstanceNorm1d(out_channels, affine=True),
                nn.ReLU(inplace=True)
            )
        else:
            self.double_conv = nn.Sequential(
                nn.Conv1d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                nn.ReLU(inplace=True),
                nn.Conv1d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                nn.ReLU(inplace=True)
            )

    def forward(self, x):
        return self.double_conv(x)


class Down1D(nn.Module):
    """Downscaling with maxpool then double conv"""

    def __init__(self, in_channels, out_channels, normtype='bn'):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool1d(2),
            DoubleConv1D(in_channels, out_channels, normtype=normtype)
        )

    def forward(self, x):
        return self.maxpool_conv(x)


class Up1D(nn.Module):
    """Upscaling then double conv"""

    def __init__(self, in_channels, out_channels, bilinear=True, normtype='bn'):
        super().__init__()

        # if bilinear, use the normal convolutions to reduce the number of channels
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='linear', align_corners=True)
            self.conv = DoubleConv1D(in_channels, out_channels, in_channels // 2, normtype=normtype)
        else:
            self.up = nn.ConvTranspose1d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv1D(in_channels, out_channels, normtype=normtype)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv1D(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv1D, self).__init__()
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)


################################################################
# Multiscale modules 2D
################################################################
class DoubleConv2D(nn.Module):
    """(convolution => [BN] => ReLU) * 2"""

    def __init__(self, in_channels, out_channels, mid_channels=None, normtype='bn'):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        if normtype == 'bn':
            self.double_conv = nn.Sequential(
                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm2d(mid_channels),
                nn.ReLU(inplace=True),
                nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            )
        elif normtype == 'in':
            self.double_conv = nn.Sequential(
                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                nn.InstanceNorm2d(mid_channels, affine=True),
                nn.ReLU(inplace=True),
                nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                nn.InstanceNorm2d(out_channels, affine=True),
                nn.ReLU(inplace=True)
            )
        else:
            self.double_conv = nn.Sequential(
                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                nn.ReLU(inplace=True),
                nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                nn.ReLU(inplace=True)
            )

    def forward(self, x):
        return self.double_conv(x)


class Down2D(nn.Module):
    """Downscaling with maxpool then double conv"""

    def __init__(self, in_channels, out_channels, normtype='bn'):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv2D(in_channels, out_channels, normtype=normtype)
        )

    def forward(self, x):
        return self.maxpool_conv(x)


class Up2D(nn.Module):
    """Upscaling then double conv"""

    def __init__(self, in_channels, out_channels, bilinear=True, normtype='bn'):
        super().__init__()

        # if bilinear, use the normal convolutions to reduce the number of channels
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv2D(in_channels, out_channels, in_channels // 2, normtype=normtype)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv2D(in_channels, out_channels, normtype=normtype)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is CHW
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]

        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2])
        # if you have padding issues, see
        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv2D(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv2D, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)


################################################################
# Multiscale modules 3D
################################################################

class DoubleConv3D(nn.Module):
    """(convolution => [BN] => ReLU) * 2"""

    def __init__(self, in_channels, out_channels, mid_channels=None, normtype='bn'):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        if normtype == 'bn':
            self.double_conv = nn.Sequential(
                nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm3d(mid_channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm3d(out_channels),
                nn.ReLU(inplace=True)
            )
        elif normtype == 'in':
            self.double_conv = nn.Sequential(
                nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                nn.InstanceNorm3d(mid_channels, affine=True),
                nn.ReLU(inplace=True),
                nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                nn.InstanceNorm3d(out_channels, affine=True),
                nn.ReLU(inplace=True)
            )
        else:
            self.double_conv = nn.Sequential(
                nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                nn.ReLU(inplace=True),
                nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                nn.ReLU(inplace=True)
            )

    def forward(self, x):
        return self.double_conv(x)


class Down3D(nn.Module):
    """Downscaling with maxpool then double conv"""

    def __init__(self, in_channels, out_channels, normtype='bn'):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool3d(2),
            DoubleConv3D(in_channels, out_channels, normtype=normtype)
        )

    def forward(self, x):
        return self.maxpool_conv(x)


class Up3D(nn.Module):
    """Upscaling then double conv"""

    def __init__(self, in_channels, out_channels, bilinear=True, normtype='bn'):
        super().__init__()

        # if bilinear, use the normal convolutions to reduce the number of channels
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)
            self.conv = DoubleConv3D(in_channels, out_channels, in_channels // 2, normtype=normtype)
        else:
            self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv3D(in_channels, out_channels, normtype=normtype)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv3D(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv3D, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)
</file>

<file path="models/m2no/__init__.py">
from .m2no1d import M2NO1d
from .m2no2d import M2NO2d
</file>

<file path="models/m2no/grid_operator.py">
# models/m2no/grid_operator.py
from __future__ import annotations

from typing import Tuple

import numpy as np
import torch
from torch import nn, Tensor

from .utils import get_filter


class LPFOperator1d(nn.Module):
    """
    1D low-pass multiwavelet operator providing restrict / prolongate
    along a single spatial dimension.

    Channel layout:
        - total channels: C = c * fine_dim
        - fine_dim = 2 * k  (number of fine-scale modal coefficients)
        - coarse_dim = k    (number of coarse-scale modal coefficients)

    The internal filter H has shape (coarse_dim, fine_dim).
    """

    def __init__(
        self,
        c: int = 4,
        k: int = 4,
        base: str = "legendre",
        **kwargs,
    ) -> None:
        super().__init__()
        self.c = c
        self.k = k
        self.fine_dim = 2 * k
        self.coarse_dim = k
        self.sub_dim = self.coarse_dim  # kept for compatibility

        H, G = self._build_filter(base, k)
        # register as buffers so they follow .to(device)
        self.register_buffer("H", H)
        self.register_buffer("G", G)  # not used yet, but kept for high-pass if needed

    @staticmethod
    def _build_filter(base: str, k: int) -> Tuple[Tensor, Tensor]:
        """
        Build 1D low-pass / high-pass filter matrices H, G from the
        polynomial filter generator.
        """
        H0, H1, G0, G1, _, _ = get_filter(base, k)

        H_np = np.concatenate((H0, H1), axis=1)  # (k, 2k)
        G_np = np.concatenate((G0, G1), axis=1)  # (k, 2k)

        H = torch.tensor(H_np, dtype=torch.float32)
        G = torch.tensor(G_np, dtype=torch.float32)
        return H, G

    def restrict(self, x: Tensor) -> Tensor:
        """
        Restrict (downsample) along the last spatial dimension by factor 2.

        Args:
            x: tensor of shape (B, C, N), where
               C = c * fine_dim (typically c * 2k),
               N is the fine spatial resolution.

        Returns:
            Tensor of shape (B, C_out, N_out),
            where C_out = c * coarse_dim, N_out = N // 2.
        """
        B, C, N = x.shape
        # (B, C, N) -> (B, N, C)
        x_ = x.permute(0, 2, 1).contiguous()
        # (B, N/2, c, fine_dim)
        x_ = x_.view(B, N // 2, self.c, -1)
        # (B, N/2, c, coarse_dim)
        x_ = torch.matmul(x_, self.H.t())
        # (B, N/2, c * coarse_dim)
        x_ = x_.view(B, N // 2, -1)
        # (B, C_out, N/2)
        x_ = x_.permute(0, 2, 1).contiguous()
        return x_

    def prolongate(self, x: Tensor) -> Tensor:
        """
        Prolongate (upsample) along the last spatial dimension by factor 2.

        Args:
            x: tensor of shape (B, C_in, N), where
               C_in = c * coarse_dim.

        Returns:
            Tensor of shape (B, C_out, 2N),
            where C_out = c * fine_dim.
        """
        B, C, N = x.shape
        # (B, C_in, N) -> (B, N, C_in)
        x_ = x.permute(0, 2, 1).contiguous()
        # (B, N, c, coarse_dim)
        x_ = x_.view(B, N, self.c, -1)
        # (B, N, c, fine_dim)
        x_ = torch.matmul(x_, self.H)
        # (B, 2N, c * fine_dim)
        x_ = x_.view(B, N * 2, -1)
        # (B, C_out, 2N)
        x_ = x_.permute(0, 2, 1).contiguous()
        return x_


class LPFOperator2d(nn.Module):
    """
    2D low-pass multiwavelet operator providing restrict / prolongate
    on a 2D grid (H, W) with channels encoded in modal space.

    For a given (k, c):
        - hidden_channel is typically c * (k^2) or c * (4 * k^2),
          depending on how you pack subbands.
        - internally, H acts on a "fine_dim" and maps it to a "coarse_dim".

    The current implementation keeps the original channel layout used in
    the M2NO code and only refactors it for style / type checking.
    """

    def __init__(
        self,
        k: int = 4,
        c: int = 4,
        base: str = "legendre",
        **kwargs,
    ) -> None:
        super().__init__()
        self.c = c
        self.k = k

        # These three attributes are kept for backward compatibility.
        # Note: depending on how you pack the modes, hidden_channel
        # may not be exactly "channels" in the input tensor.
        self.hidden_channel = c * (k**2)
        self.fine_dim = self.hidden_channel
        self.coarse_dim = self.hidden_channel // 4
        self.sub_dim = self.coarse_dim * 3

        H, G = self._build_filter(base, k)
        self.register_buffer("H", H)
        self.register_buffer("G", G)

    @staticmethod
    def _build_filter(base: str, k: int) -> Tuple[Tensor, Tensor]:
        """
        Build 2D low-pass / high-pass filter matrices H, G using Kronecker
        products of 1D filters.
        """
        H0, H1, G0, G1, _, _ = get_filter(base, k)

        H0 = np.asarray(H0)
        H1 = np.asarray(H1)
        G0 = np.asarray(G0)
        G1 = np.asarray(G1)

        # Low-pass (scaling) part: four combinations
        H_LL = np.kron(H0, H0)
        H_LH = np.kron(H0, H1)
        H_HL = np.kron(H1, H0)
        H_HH = np.kron(H1, H1)
        H_np = np.concatenate((H_LL, H_LH, H_HL, H_HH), axis=1)

        # High-pass combinations (G*H, H*G, G*G)
        GH_LL = np.kron(G0, H0)
        GH_LH = np.kron(G0, H1)
        GH_HL = np.kron(G1, H0)
        GH_HH = np.kron(G1, H1)
        GH = np.concatenate((GH_LL, GH_LH, GH_HL, GH_HH), axis=1)

        HG_LL = np.kron(H0, G0)
        HG_LH = np.kron(H0, G1)
        HG_HL = np.kron(H1, G0)
        HG_HH = np.kron(H1, G1)
        HG = np.concatenate((HG_LL, HG_LH, HG_HL, HG_HH), axis=1)

        GG_LL = np.kron(G0, G0)
        GG_LH = np.kron(G0, G1)
        GG_HL = np.kron(G1, G0)
        GG_HH = np.kron(G1, G1)
        GG = np.concatenate((GG_LL, GG_LH, GG_HL, GG_HH), axis=1)

        H = torch.tensor(H_np, dtype=torch.float32)
        G = torch.tensor(np.concatenate((GH, HG, GG), axis=0), dtype=torch.float32)
        return H, G

    def restrict(self, x: Tensor) -> Tensor:
        """
        Restrict (downsample) by factor 2 in both H and W.

        Args:
            x: tensor of shape (B, C, H, W).

        Returns:
            Tensor of shape (B, C_out, H/2, W/2).
        """
        B, C, H, W = x.shape

        # (B, C, H, W) -> (B, H, W, C)
        x_ = x.permute(0, 2, 3, 1).contiguous()
        # (B, H/2, W/2, c, fine_dim_per_group)
        x_ = x_.view(B, H // 2, W // 2, self.c, -1)
        # apply low-pass filter along the modal dimension
        # (B, H/2, W/2, c, coarse_dim_per_group)
        x_ = torch.matmul(x_, self.H.t())
        # (B, H/2, W/2, C_out)
        x_ = x_.view(B, H // 2, W // 2, -1)
        # (B, C_out, H/2, W/2)
        x_ = x_.permute(0, 3, 1, 2).contiguous()
        return x_

    def prolongate(self, x: Tensor) -> Tensor:
        """
        Prolongate (upsample) by factor 2 in both H and W.

        Args:
            x: tensor of shape (B, C_in, H, W).

        Returns:
            Tensor of shape (B, C_out, 2H, 2W).
        """
        B, C, H, W = x.shape

        # (B, C_in, H, W) -> (B, H, W, C_in)
        x_ = x.permute(0, 2, 3, 1).contiguous()
        # (B, H, W, c, coarse_dim_per_group)
        x_ = x_.view(B, H, W, self.c, -1)
        # (B, H, W, c, fine_dim_per_group)
        x_ = torch.matmul(x_, self.H)
        # (B, 2H, 2W, C_out)
        x_ = x_.view(B, H * 2, W * 2, -1)
        # (B, C_out, 2H, 2W)
        x_ = x_.permute(0, 3, 1, 2).contiguous()
        return x_
</file>

<file path="models/m2no/m2no1d.py">
import torch
import torch.nn as nn

from .grid_operator import LPFOperator1d


class GridBlock1d(nn.Module):
    def __init__(self, in_channels, out_channels, num_ite, bias=True, padding_mode='zeros'):
        super(GridBlock1d, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_ite = num_ite
        
        self.S = nn.ModuleList([nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=bias, padding_mode=padding_mode) for _ in range(num_ite)])
        self.A = nn.ModuleList([nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=bias, padding_mode=padding_mode) for _ in range(num_ite)])
        
    # def forward(self, f, u=None):
    #     if u is None:
    #         u = torch.zeros_like(f).to(f.device)
            
    #     for i in range(self.num_ite):
    #         u = u + self.S[i](f)
    #         f = f - self.A[i](u)

    #     return f, u
    
    def forward(self, f, u=None):
        if u is None:
            u = torch.zeros_like(f).to(f.device)
            
        for i in range(self.num_ite):
            r = f - self.A[i](u)
            u = u + self.S[i](r)

        return r, u


class MultiGrid1d(nn.Module):
    def __init__(self, in_channels, out_channels, grid_levels, op, 
                 bias=True, padding_mode='zeros', resolution=1024, 
                 norm=False, **kwargs):
        super(MultiGrid1d, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.grid_levels = grid_levels
        self.num_level = len(grid_levels)
        self.norm = norm
        self.op = op
        self.resolution = resolution
        
        self.pre_S = GridBlock1d(in_channels, out_channels, 1, bias, padding_mode)
        self.post_S = GridBlock1d(in_channels, out_channels, 1, bias, padding_mode)
        self.grid_list = nn.ModuleList([GridBlock1d(in_channels, out_channels, grid_levels[i], bias, padding_mode) for i in range(self.num_level)])
        if norm:
            self.norm_list = nn.ModuleList([nn.LayerNorm([out_channels, resolution//2**i]) for i in range(self.num_level)])
    
    def forward(self, f):
        input_resolution = f.shape[2]
        
        u_list = [None] * (self.num_level + 1)
        f_list = [None] * (self.num_level + 1)
        
        r_n, u_n = self.pre_S(f)
        
        while u_n.shape[2] > self.resolution:
            r_n = self.op.restrict(r_n)
            u_n = self.op.restrict(u_n)
        
        while u_n.shape[2] < self.resolution:
            r_n = self.op.prolongate(r_n)
            u_n = self.op.prolongate(u_n)
        
        f_list[0] = r_n
        u_list[0] = u_n
        
        for i in range(self.num_level):
            r = self.op.restrict(f_list[i])
            e = self.op.restrict(u_list[i])
            r, e = self.grid_list[i](r, e)
            f_list[i+1] = r
            u_list[i+1] = e
    
        for i in range(self.num_level, 0, -1):
            if self.norm:
                u_list[i-1] = self.norm_list[i-1](u_list[i-1] + self.op.prolongate(u_list[i]))
            else:
                u_list[i-1] = u_list[i-1] + self.op.prolongate(u_list[i])
        
        u_n = u_list[0]
        
        while u_n.shape[2] > input_resolution:
            u_n = self.op.restrict(u_n)
        
        while u_n.shape[2] < input_resolution:
            u_n = self.op.prolongate(u_n)
        
        r_n, u_n = self.post_S(f, u_n)
                    
        return u_n


class M2NO1d(nn.Module):
    def __init__(self, in_channels, out_channels, c, k, num_layer, 
                 grid_levels, base='legendre', bias=True, resolution=1024, 
                 operator='wavelet', padding_mode='zeros', initializer=None, 
                 norm=True, activation='gelu', **kwargs):
        super(M2NO1d, self).__init__()
        self.num_layer = num_layer
        self.in_channels = in_channels
        self.out_channels = out_channels
        hidden_channels = c * k

        self.L_in = nn.Linear(in_channels, hidden_channels)
        self.L_hidden = nn.Linear(hidden_channels, hidden_channels)
        self.L_out = nn.Linear(hidden_channels, out_channels)
        
        self.wavelet_operator = LPFOperator1d(c=c, k=k, base=base, bias=bias, padding_mode=padding_mode)
        
        self.conv_list = nn.ModuleList([MultiGrid1d(hidden_channels, hidden_channels, 
                                                    grid_levels, self.wavelet_operator, 
                                                    norm=norm, bias=bias, padding_mode=padding_mode, 
                                                    resolution=resolution) for _ in range(num_layer)])
        
        if activation == 'gelu':
            self.activate = nn.GELU()
        elif activation == 'relu':
            self.activate = nn.ReLU()
        elif activation == 'tanh':
            self.activate = nn.Tanh()
        else:
            raise ValueError('Activation not supported')
        
        if initializer is not None:
            self.reset_parameters(initializer)
    
    def reset_parameters(self, initializer):
        initializer(self.L_in.weight)
        initializer(self.L_hidden.weight)
        initializer(self.L_out.weight)
        
    def forward(self, x):
        u = self.L_in(x)
        u = u.permute(0, 2, 1)
        for i in range(self.num_layer):
            u = self.conv_list[i](u)
            if i != self.num_layer-1:
                u = self.activate(u)
        u = u.permute(0, 2, 1)
        u = self.activate(self.L_hidden(u))
        u = self.L_out(u)
        
        return u
</file>

<file path="models/m2no/m2no2d.py">
# models/m2no/m2no_2d.py
from typing import List, Optional, Tuple

import torch
from torch import nn, Tensor

from .grid_operator import LPFOperator2d


class GridBlock2d(nn.Module):
    """
    Simple iterative grid smoother for a linear operator A(u) = f.

    Each block stores a list of 3x3 convolution smoothers S[i] and performs:
        u_{k+1} = u_k + S[i](f - A(u_k))
    starting from either:
        - u = S[0](f) if u is None, or
        - the provided initial guess u.

    Args:
        in_channels:  number of input channels
        out_channels: number of output channels
        num_iter:     number of smoothing iterations
        bias:         bias for Conv2d
        padding_mode: padding mode for Conv2d
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        num_iter: int,
        bias: bool = True,
        padding_mode: str = "zeros",
    ) -> None:
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_iter = num_iter

        self.smoothers = nn.ModuleList(
            [
                nn.Conv2d(
                    in_channels,
                    out_channels,
                    kernel_size=3,
                    padding=1,
                    bias=bias,
                    padding_mode=padding_mode,
                )
                for _ in range(num_iter)
            ]
        )

    def forward(
        self,
        A: nn.Module,
        f: Tensor,
        u: Optional[Tensor] = None,
    ) -> Tuple[Tensor, Tensor]:
        """
        Args:
            A: linear operator, typically a Conv2d (applied as A(u))
            f: right-hand side, shape (B, C, H, W)
            u: optional initial guess, same shape as f

        Returns:
            u: updated solution
            r: final residual r = f - A(u)
        """
        for i in range(self.num_iter):
            if u is None:
                # first sweep: initialize from f only
                u = self.smoothers[i](f)
            else:
                # subsequent sweeps: classic Richardson / Jacobi style step
                u = u + self.smoothers[i](f - A(u))

        if u is None:
            u = torch.zeros_like(f)

        r = f - A(u)
        return u, r


class MultiGrid2d(nn.Module):
    """
    Simple 2D multigrid V-cycle built on top of GridBlock2d and an LPF operator.

    - pre_S:  pre-smoothing at the finest scale
    - post_S: post-smoothing at the finest scale
    - grid_list[i]: smoother at level i (coarser grids)

    The LPF operator `op` provides:
        - restrict(x):   downsample to coarser grid
        - prolongate(x): upsample to finer grid
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        grid_levels: List[int],
        op: LPFOperator2d,
        bias: bool = True,
        padding_mode: str = "zeros",
        resolutions: List[int] | Tuple[int, int] = (64, 64),
        norm: bool = False,
        **kwargs,
    ) -> None:
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.grid_levels = grid_levels
        self.num_level = len(grid_levels)
        self.norm = norm

        # coarse linear operator A(u)
        self.A = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=3,
            padding=1,
            bias=bias,
            padding_mode=padding_mode,
        )

        # pre-/post-smoothers at finest grid
        self.pre_S = GridBlock2d(in_channels, out_channels, num_iter=1, bias=bias, padding_mode=padding_mode)
        self.post_S = GridBlock2d(in_channels, out_channels, num_iter=1, bias=bias, padding_mode=padding_mode)

        # level-dependent smoothers on coarser grids
        self.grid_list = nn.ModuleList(
            [
                GridBlock2d(in_channels, out_channels, num_iter=grid_levels[i], bias=bias, padding_mode=padding_mode)
                for i in range(self.num_level)
            ]
        )

        # optional LayerNorm at each grid level
        self.resolutions = list(resolutions)
        if self.norm:
            self.norm_list = nn.ModuleList(
                [
                    nn.LayerNorm(
                        [out_channels, self.resolutions[0] // (2**i), self.resolutions[1] // (2**i)]
                    )
                    for i in range(self.num_level)
                ]
            )

        self.op = op

    def forward(self, f: Tensor) -> Tuple[Tensor, Tensor]:
        """
        Args:
            f: RHS / input on finest grid, shape (B, C, H, W)

        Returns:
            u_n: final solution at finest grid
            r_n: final residual at finest grid
        """
        B, C, H_in, W_in = f.shape
        target_H, target_W = self.resolutions

        u_list: list[Optional[Tensor]] = [None] * (self.num_level + 1)
        r_list: list[Optional[Tensor]] = [None] * (self.num_level + 1)

        # pre-smoothing at finest grid
        u_n, r_n = self.pre_S(self.A, f)

        # bring (u, r) to the "reference" multigrid base resolution
        while u_n.shape[-2] > target_H:
            r_n = self.op.restrict(r_n)
            u_n = self.op.restrict(u_n)

        while u_n.shape[-2] < target_H:
            r_n = self.op.prolongate(r_n)
            u_n = self.op.prolongate(u_n)

        r_list[0] = r_n
        u_list[0] = u_n

        # go down to coarser levels
        for i in range(self.num_level):
            u_current = u_list[i]
            if u_current is None:
                raise ValueError(f"Missing state tensor at multigrid level {i}")
            u_coarse = self.op.restrict(u_current)
            u_coarse, r_coarse = self.grid_list[i](self.A, u_coarse)
            r_list[i + 1] = r_coarse
            u_list[i + 1] = u_coarse

        # go back up, adding corrections
        for i in range(self.num_level, 0, -1):
            u_coarse = u_list[i]
            if u_coarse is None:
                raise ValueError(f"Missing state tensor at multigrid level {i}")
            up = self.op.prolongate(u_coarse)
            prev_level = u_list[i - 1]
            if prev_level is None:
                raise ValueError(f"Missing state tensor at multigrid level {i - 1}")
            if self.norm:
                u_list[i - 1] = self.norm_list[i - 1](prev_level + up)
            else:
                u_list[i - 1] = prev_level + up

        u_n = u_list[0]
        if u_n is None:
            raise ValueError("Missing state tensor at multigrid level 0")

        # match the original input resolution
        while u_n.shape[-2] > H_in:
            u_n = self.op.restrict(u_n)

        while u_n.shape[-2] < H_in:
            u_n = self.op.prolongate(u_n)

        # post-smoothing at finest grid
        u_n, r_n = self.post_S(self.A, f, u_n)

        return u_n, r_n


class M2NO2d(nn.Module):
    """
    Multiwavelet-based Multigrid Neural Operator (M2NO) in 2D.

    Expected input / output:
        x: (B, H, W, C_in)
        y: (B, H, W, C_out)

    The pipeline is:
        x            -> Linear (C_in -> hidden) -> (B, hidden, H, W)
        multigrid V-cycles in feature space     -> (B, hidden, H, W)
        permute back -> MLP (hidden -> hidden)  -> Linear (hidden -> C_out)
    """

    def __init__(self, model_params: dict, **kwargs) -> None:
        super().__init__()

        # ------------------------------------------------------------------
        # Read hyperparameters from model_params
        # ------------------------------------------------------------------
        self.in_channels: int = model_params.get("in_channels", 1)
        self.out_channels: int = model_params.get("out_channels", 1)
        k: int = model_params.get("k", 2)  # polynomial order per dimension
        c: int = model_params.get("c", 4)  # number of channels per polynomial
        self.num_layers: int = model_params.get("num_layers", 2)
        grid_levels: list[int] = model_params.get("grid_levels", [1, 1])
        base: str = model_params.get("base", "legendre")
        bias: bool = model_params.get("bias", True)
        padding_mode: str = model_params.get("padding_mode", "zeros")
        norm: bool = model_params.get("norm", False)
        resolutions = model_params.get("resolutions", [64, 64])
        activation_name: str = model_params.get("activation", "gelu")

        hidden_channels: int = c * (k**2)

        # ------------------------------------------------------------------
        # Linear projection in/out of hidden feature space
        # ------------------------------------------------------------------
        self.input_proj = nn.Linear(self.in_channels, hidden_channels)
        self.hidden_proj = nn.Linear(hidden_channels, hidden_channels)
        self.output_proj = nn.Linear(hidden_channels, self.out_channels)

        # ------------------------------------------------------------------
        # Low-pass / wavelet operator providing restrict / prolongate
        # ------------------------------------------------------------------
        self.wavelet_operator = LPFOperator2d(
            k=k,
            c=c,
            base=base,
        )

        # ------------------------------------------------------------------
        # Multigrid blocks stacked in depth
        # ------------------------------------------------------------------
        self.blocks = nn.ModuleList(
            [
                MultiGrid2d(
                    in_channels=hidden_channels,
                    out_channels=hidden_channels,
                    grid_levels=grid_levels,
                    op=self.wavelet_operator,
                    bias=bias,
                    padding_mode=padding_mode,
                    resolutions=resolutions,
                    norm=norm,
                )
                for _ in range(self.num_layers)
            ]
        )

        # ------------------------------------------------------------------
        # Nonlinearity
        # ------------------------------------------------------------------
        if activation_name == "gelu":
            self.activation = nn.GELU()
        elif activation_name == "relu":
            self.activation = nn.ReLU()
        elif activation_name == "tanh":
            self.activation = nn.Tanh()
        else:
            raise ValueError(f"Unsupported activation: {activation_name}")

    def forward(self, x: Tensor) -> Tensor:
        """
        Args:
            x: input tensor, shape (B, H, W, C_in)

        Returns:
            y: output tensor, shape (B, H, W, C_out)
        """
        assert x.dim() == 4, f"Expected x of shape (B, H, W, C_in), got {x.shape}"

        B, H, W, C_in = x.shape
        if C_in != self.in_channels:
            raise ValueError(
                f"Input channels mismatch: got {C_in}, expected {self.in_channels}"
            )

        # (B, H, W, C_in) -> (B, H, W, hidden)
        u = self.input_proj(x)

        # (B, H, W, hidden) -> (B, hidden, H, W)
        u = u.permute(0, 3, 1, 2).contiguous()

        # Multigrid V-cycles
        for i, block in enumerate(self.blocks):
            u, _ = block(u)
            if i < self.num_layers - 1:
                u = self.activation(u)

        # (B, hidden, H, W) -> (B, H, W, hidden)
        u = u.permute(0, 2, 3, 1).contiguous()

        # pointwise MLP in hidden space
        u = self.activation(self.hidden_proj(u))
        y = self.output_proj(u)

        return y
</file>

<file path="models/m2no/utils.py">
# models/m2no/utils.py
# ---------------------------------------------------------------
# This file is based on the implementation in:
#   https://github.com/gaurav71531/mwt-operator
# ---------------------------------------------------------------
from __future__ import annotations

from functools import partial
from typing import Callable, Sequence, Tuple

import numpy as np
import numpy.typing as npt
from scipy.special import eval_legendre
from sympy import Poly, Symbol, chebyshevt, legendre

Array = npt.NDArray[np.float64]
BasisFn = Callable[[Array], Array]


def legendreDer(k: int, x: Array | float) -> Array:
    """
    Derivative-related helper for Legendre polynomials.

    This follows the original formula:
        sum_{i = k-1, k-3, ... >= 0} (2*i+1) * P_i(x)

    Args:
        k: polynomial order
        x: evaluation points

    Returns:
        Array of same shape as x.
    """
    x_arr = np.asarray(x, dtype=np.float64)
    out = np.zeros_like(x_arr, dtype=np.float64)

    def _legendre(idx: int, xx: Array) -> Array:
        return (2 * idx + 1) * eval_legendre(idx, xx)

    for i in range(k - 1, -1, -2):
        out = out + _legendre(i, x_arr)
    return out


def phi_(phi_c: Array, x: Array | float, lb: float = 0.0, ub: float = 1.0) -> Array:
    """
    Evaluate a polynomial on [lb, ub], zero outside.

    Args:
        phi_c: polynomial coefficients in ascending order (np.polynomial style)
        x: points to evaluate
        lb, ub: support interval

    Returns:
        Values of the polynomial on x, zero outside [lb, ub].
    """
    x_arr = np.asarray(x, dtype=np.float64)
    mask = np.logical_or(x_arr < lb, x_arr > ub)
    poly_val = np.polynomial.polynomial.Polynomial(phi_c)(x_arr)
    # ensure we return a float64 ndarray (mask may promote types)
    return poly_val.astype(np.float64) * (~mask).astype(np.float64)


def get_phi_psi(
    k: int,
    base: str,
) -> Tuple[list[BasisFn], list[BasisFn], list[BasisFn]]:
    """
    Construct scaling functions (phi) and wavelets (psi1, psi2) for a
    multiwavelet basis based on either Legendre or Chebyshev polynomials.

    Args:
        k: number of basis functions
        base: 'legendre' or 'chebyshev'

    Returns:
        (phi, psi1, psi2):
            phi  : list of k scaling basis functions
            psi1 : list of k wavelet functions on [0, 0.5]
            psi2 : list of k wavelet functions on [0.5, 1]
    """
    if base not in ("legendre", "chebyshev"):
        raise ValueError(f"Base '{base}' not supported. Use 'legendre' or 'chebyshev'.")

    x_sym = Symbol("x")

    # coefficient matrices (ascending power, Polynomial-style)
    phi_coeff: Array = np.zeros((k, k), dtype=np.float64)
    phi_2x_coeff: Array = np.zeros((k, k), dtype=np.float64)

    # ---------------------------------------------------------------------
    # Legendre-based basis
    # ---------------------------------------------------------------------
    if base == "legendre":
        # build phi and phi(2x)
        for ki in range(k):
            # legendre(ki, 2x-1) expanded in x
            coeff_ = Poly(legendre(ki, 2 * x_sym - 1), x_sym).all_coeffs()  # type: ignore
            coeff_arr = np.array([float(c) for c in coeff_], dtype=np.float64)
            phi_coeff[ki, : ki + 1] = np.flip(np.sqrt(2 * ki + 1.0) * coeff_arr)

            # legendre(ki, 4x-1) expanded in x
            coeff_2 = Poly(legendre(ki, 4 * x_sym - 1), x_sym).all_coeffs()  # type: ignore
            coeff_2_arr = np.array([float(c) for c in coeff_2], dtype=np.float64)
            phi_2x_coeff[ki, : ki + 1] = np.flip(
                np.sqrt(2.0) * np.sqrt(2 * ki + 1.0) * coeff_2_arr
            )

        psi1_coeff: Array = np.zeros((k, k), dtype=np.float64)
        psi2_coeff: Array = np.zeros((k, k), dtype=np.float64)

        for ki in range(k):
            psi1_coeff[ki, :] = phi_2x_coeff[ki, :]
            psi2_coeff[ki, :] = phi_2x_coeff[ki, :]

            # project out phi_i
            for i in range(k):
                a = phi_2x_coeff[ki, : ki + 1]
                b = phi_coeff[i, : i + 1]
                prod_ = np.convolve(a, b)
                prod_[np.abs(prod_) < 1e-8] = 0.0
                idx = np.arange(len(prod_), dtype=np.float64)
                proj_ = (prod_ * 1.0 / (idx + 1.0) * np.power(0.5, 1.0 + idx)).sum()
                psi1_coeff[ki, :] -= proj_ * phi_coeff[i, :]
                psi2_coeff[ki, :] -= proj_ * phi_coeff[i, :]

            # orthogonalize against previous psi_j
            for j in range(ki):
                a = phi_2x_coeff[ki, : ki + 1]
                b = psi1_coeff[j, :]
                prod_ = np.convolve(a, b)
                prod_[np.abs(prod_) < 1e-8] = 0.0
                idx = np.arange(len(prod_), dtype=np.float64)
                proj_ = (prod_ * 1.0 / (idx + 1.0) * np.power(0.5, 1.0 + idx)).sum()
                psi1_coeff[ki, :] -= proj_ * psi1_coeff[j, :]
                psi2_coeff[ki, :] -= proj_ * psi2_coeff[j, :]

            # normalize
            def _norm_from_coeff(c: Array, use_left_half: bool) -> float:
                prod = np.convolve(c, c)
                prod[np.abs(prod) < 1e-8] = 0.0
                idx2 = np.arange(len(prod), dtype=np.float64)
                if use_left_half:
                    # integral over [0, 0.5]
                    w = 1.0 / (idx2 + 1.0) * np.power(0.5, 1.0 + idx2)
                else:
                    # integral over [0.5, 1] (complement)
                    w = 1.0 / (idx2 + 1.0) * (1.0 - np.power(0.5, 1.0 + idx2))
                return float((prod * w).sum())

            norm1 = _norm_from_coeff(psi1_coeff[ki, :], use_left_half=True)
            norm2 = _norm_from_coeff(psi2_coeff[ki, :], use_left_half=False)
            norm_ = np.sqrt(norm1 + norm2)

            psi1_coeff[ki, :] /= norm_
            psi2_coeff[ki, :] /= norm_

            psi1_coeff[np.abs(psi1_coeff) < 1e-8] = 0.0
            psi2_coeff[np.abs(psi2_coeff) < 1e-8] = 0.0

        # convert coeffs to callable polynomials (np.poly1d uses descending powers)
        phi: list[BasisFn] = [
            np.poly1d(np.flip(phi_coeff[i, :])) for i in range(k)
        ]
        psi1: list[BasisFn] = [
            np.poly1d(np.flip(psi1_coeff[i, :])) for i in range(k)
        ]
        psi2: list[BasisFn] = [
            np.poly1d(np.flip(psi2_coeff[i, :])) for i in range(k)
        ]

    # ---------------------------------------------------------------------
    # Chebyshev-based basis
    # ---------------------------------------------------------------------
    else:  # base == 'chebyshev'
        for ki in range(k):
            if ki == 0:
                phi_coeff[ki, : ki + 1] = np.array([np.sqrt(2.0 / np.pi)], dtype=np.float64)
                phi_2x_coeff[ki, : ki + 1] = np.array(
                    [np.sqrt(2.0 / np.pi) * np.sqrt(2.0)], dtype=np.float64
                )
            else:
                coeff_ = Poly(chebyshevt(ki, 2 * x_sym - 1), x_sym).all_coeffs()  # type: ignore
                coeff_arr = np.array([float(c) for c in coeff_], dtype=np.float64)
                phi_coeff[ki, : ki + 1] = np.flip(2.0 / np.sqrt(np.pi) * coeff_arr)

                coeff_2 = Poly(chebyshevt(ki, 4 * x_sym - 1), x_sym).all_coeffs()  # type: ignore
                coeff_2_arr = np.array([float(c) for c in coeff_2], dtype=np.float64)
                phi_2x_coeff[ki, : ki + 1] = np.flip(
                    np.sqrt(2.0) * 2.0 / np.sqrt(np.pi) * coeff_2_arr
                )

        # scaling functions on [0,1]
        phi = [partial(phi_, phi_coeff[i, :]) for i in range(k)]

        k_use = 2 * k
        roots = Poly(chebyshevt(k_use, 2 * x_sym - 1), x_sym).all_roots()  # type: ignore
        x_m = np.array([float(rt.evalf(20)) for rt in roots], dtype=np.float64)
        w_m = np.pi / k_use / 2.0

        psi1_coeff: Array = np.zeros((k, k), dtype=np.float64)
        psi2_coeff: Array = np.zeros((k, k), dtype=np.float64)

        psi1: list[BasisFn] = [lambda _x: np.zeros_like(x_m) for _ in range(k)]
        psi2: list[BasisFn] = [lambda _x: np.zeros_like(x_m) for _ in range(k)]

        for ki in range(k):
            psi1_coeff[ki, :] = phi_2x_coeff[ki, :]
            psi2_coeff[ki, :] = phi_2x_coeff[ki, :]

            # project out phi_i
            for i in range(k):
                proj = (w_m * phi[i](x_m) * np.sqrt(2.0) * phi[ki](2.0 * x_m)).sum()
                psi1_coeff[ki, :] -= proj * phi_coeff[i, :]
                psi2_coeff[ki, :] -= proj * phi_coeff[i, :]

            # orthogonalize against previous psi_j
            for j in range(ki):
                proj = (w_m * psi1[j](x_m) * np.sqrt(2.0) * phi[ki](2.0 * x_m)).sum()
                psi1_coeff[ki, :] -= proj * psi1_coeff[j, :]
                psi2_coeff[ki, :] -= proj * psi2_coeff[j, :]

            # build piecewise functions on [0, 0.5] and (0.5, 1]
            psi1[ki] = partial(phi_, psi1_coeff[ki, :], lb=0.0, ub=0.5)
            psi2[ki] = partial(phi_, psi2_coeff[ki, :], lb=0.5, ub=1.0)

            norm1 = float((w_m * psi1[ki](x_m) * psi1[ki](x_m)).sum())
            norm2 = float((w_m * psi2[ki](x_m) * psi2[ki](x_m)).sum())
            norm_ = np.sqrt(norm1 + norm2)

            psi1_coeff[ki, :] /= norm_
            psi2_coeff[ki, :] /= norm_

            psi1_coeff[np.abs(psi1_coeff) < 1e-8] = 0.0
            psi2_coeff[np.abs(psi2_coeff) < 1e-8] = 0.0

            psi1[ki] = partial(phi_, psi1_coeff[ki, :], lb=0.0, ub=0.5 + 1e-16)
            psi2[ki] = partial(phi_, psi2_coeff[ki, :], lb=0.5 + 1e-16, ub=1.0)

    return phi, psi1, psi2


def get_filter(
    base: str,
    k: int,
) -> Tuple[Array, Array, Array, Array, Array, Array]:
    """
    Build multiresolution filter matrices (H0, H1, G0, G1, PHI0, PHI1).

    Args:
        base: 'legendre' or 'chebyshev'
        k: number of basis functions

    Returns:
        (H0, H1, G0, G1, PHI0, PHI1) each of shape (k, k)
    """
    if base not in ("legendre", "chebyshev"):
        raise ValueError(f"Base '{base}' not supported. Use 'legendre' or 'chebyshev'.")

    x_sym = Symbol("x")

    def psi_fn(
        psi1: Sequence[BasisFn],
        psi2: Sequence[BasisFn],
        i: int,
        inp: Array,
    ) -> Array:
        """Piecewise wavelet on [0, 0.5] and (0.5, 1]."""
        mask = (inp <= 0.5)
        return psi1[i](inp) * mask.astype(np.float64) + psi2[i](inp) * (~mask).astype(
            np.float64
        )

    H0: Array = np.zeros((k, k), dtype=np.float64)
    H1: Array = np.zeros((k, k), dtype=np.float64)
    G0: Array = np.zeros((k, k), dtype=np.float64)
    G1: Array = np.zeros((k, k), dtype=np.float64)
    PHI0: Array = np.zeros((k, k), dtype=np.float64)
    PHI1: Array = np.zeros((k, k), dtype=np.float64)

    phi, psi1, psi2 = get_phi_psi(k, base)

    # ------------------------------------------------------------------
    # Legendre
    # ------------------------------------------------------------------
    if base == "legendre":
        roots = Poly(legendre(k, 2 * x_sym - 1), x_sym).all_roots()  # type: ignore
        x_m = np.array([float(rt.evalf(20)) for rt in roots], dtype=np.float64)
        wm = 1.0 / k / legendreDer(k, 2.0 * x_m - 1.0) / eval_legendre(k - 1, 2.0 * x_m - 1.0)

        for ki in range(k):
            for kpi in range(k):
                H0[ki, kpi] = 1.0 / np.sqrt(2.0) * (
                    wm * phi[ki](x_m / 2.0) * phi[kpi](x_m)
                ).sum()
                G0[ki, kpi] = 1.0 / np.sqrt(2.0) * (
                    wm * psi_fn(psi1, psi2, ki, x_m / 2.0) * phi[kpi](x_m)
                ).sum()
                H1[ki, kpi] = 1.0 / np.sqrt(2.0) * (
                    wm * phi[ki]((x_m + 1.0) / 2.0) * phi[kpi](x_m)
                ).sum()
                G1[ki, kpi] = 1.0 / np.sqrt(2.0) * (
                    wm * psi_fn(psi1, psi2, ki, (x_m + 1.0) / 2.0) * phi[kpi](x_m)
                ).sum()

        PHI0 = np.eye(k, dtype=np.float64)
        PHI1 = np.eye(k, dtype=np.float64)

    # ------------------------------------------------------------------
    # Chebyshev
    # ------------------------------------------------------------------
    else:  # base == 'chebyshev'
        k_use = 2 * k
        roots = Poly(chebyshevt(k_use, 2 * x_sym - 1), x_sym).all_roots()  # type: ignore
        x_m = np.array([float(rt.evalf(20)) for rt in roots], dtype=np.float64)
        w_m = np.pi / k_use / 2.0

        for ki in range(k):
            for kpi in range(k):
                H0[ki, kpi] = 1.0 / np.sqrt(2.0) * (
                    w_m * phi[ki](x_m / 2.0) * phi[kpi](x_m)
                ).sum()
                G0[ki, kpi] = 1.0 / np.sqrt(2.0) * (
                    w_m * psi_fn(psi1, psi2, ki, x_m / 2.0) * phi[kpi](x_m)
                ).sum()
                H1[ki, kpi] = 1.0 / np.sqrt(2.0) * (
                    w_m * phi[ki]((x_m + 1.0) / 2.0) * phi[kpi](x_m)
                ).sum()
                G1[ki, kpi] = 1.0 / np.sqrt(2.0) * (
                    w_m * psi_fn(psi1, psi2, ki, (x_m + 1.0) / 2.0) * phi[kpi](x_m)
                ).sum()

                PHI0[ki, kpi] = float(
                    (w_m * phi[ki](2.0 * x_m) * phi[kpi](2.0 * x_m)).sum() * 2.0
                )
                PHI1[ki, kpi] = float(
                    (w_m * phi[ki](2.0 * x_m - 1.0) * phi[kpi](2.0 * x_m - 1.0)).sum()
                    * 2.0
                )

        PHI0[np.abs(PHI0) < 1e-8] = 0.0
        PHI1[np.abs(PHI1) < 1e-8] = 0.0

    # threshold tiny entries for numerical cleanliness
    for mat in (H0, H1, G0, G1):
        mat[np.abs(mat) < 1e-8] = 0.0

    return H0, H1, G0, G1, PHI0, PHI1
</file>

<file path="models/ono/__init__.py">
from .ono import ONO
</file>

<file path="models/ono/basic.py">
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from torch import einsum
from functools import partial

ACTIVATION = {
    'gelu': nn.GELU,
    'tanh': nn.Tanh,
    'sigmoid': nn.Sigmoid,
    'relu': nn.ReLU,
    'leaky_relu': nn.LeakyReLU(0.1),
    'softplus': nn.Softplus,
    'ELU': nn.ELU,
    'silu': nn.SiLU
}


class MLP(nn.Module):
    def __init__(self, n_input, n_hidden, n_output, n_layers=1, act='gelu', res=True):
        super(MLP, self).__init__()

        if act in ACTIVATION.keys():
            act = ACTIVATION[act]
        else:
            raise NotImplementedError
        self.n_input = n_input
        self.n_hidden = n_hidden
        self.n_output = n_output
        self.n_layers = n_layers
        self.res = res
        self.linear_pre = nn.Sequential(nn.Linear(n_input, n_hidden), act())
        self.linear_post = nn.Linear(n_hidden, n_output)
        self.linears = nn.ModuleList([nn.Sequential(nn.Linear(n_hidden, n_hidden), act()) for _ in range(n_layers)])

    def forward(self, x):
        x = self.linear_pre(x)
        for i in range(self.n_layers):
            if self.res:
                x = self.linears[i](x) + x
            else:
                x = self.linears[i](x)
        x = self.linear_post(x)
        return x


class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)


class Attention(nn.Module):
    def __init__(self, dim, heads=8, dim_head=64, dropout=0., **kwargs):
        super().__init__()
        inner_dim = dim_head * heads
        self.dim_head = dim_head
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.softmax = nn.Softmax(dim=-1)
        self.dropout = nn.Dropout(dropout)
        self.to_q = nn.Linear(dim_head, dim_head, bias=False)
        self.to_k = nn.Linear(dim_head, dim_head, bias=False)
        self.to_v = nn.Linear(dim_head, dim_head, bias=False)
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        # B N C
        B, N, C = x.shape
        x = x.reshape(B, N, self.heads, self.dim_head).permute(0, 2, 1, 3).contiguous()  # B H N C
        q = self.to_q(x)
        k = self.to_k(x)
        v = self.to_v(x)
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        attn = self.softmax(dots)
        attn = self.dropout(attn)
        res = torch.matmul(attn, v)  # B H G D
        res = rearrange(res, 'b h n d -> b n (h d)')
        return self.to_out(res)

class FlashAttention(nn.Module):
    def __init__(self, dim, heads=8, dim_head=64, dropout=0., **kwargs):
        super().__init__()
        inner_dim = dim_head * heads
        self.dim_head = dim_head
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.dropout = nn.Dropout(dropout)
        
        # Separate projection layers for query, key, and value
        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_k = nn.Linear(dim, inner_dim, bias=False)
        self.to_v = nn.Linear(dim, inner_dim, bias=False)
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        # x shape: [batch_size, seq_len, dim]
        batch_size, seq_len, _ = x.shape
        
        # Get query, key, value projections for all heads
        q = self.to_q(x)
        k = self.to_k(x)
        v = self.to_v(x)
        
        # Reshape for multi-head attention
        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)
        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)
        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)
        
        # Flash attention implementation
        attn_output = F.scaled_dot_product_attention(
            q, k, v,
            dropout_p=self.dropout.p if self.training else 0.0,
        )
        out = rearrange(attn_output, 'b h n d -> b n (h d)')
        return self.to_out(out)

class Vanilla_Linear_Attention(nn.Module):
    def __init__(self, dim, heads=8, dim_head=64, dropout=0., **kwargs):
        super().__init__()
        inner_dim = dim_head * heads
        self.dim_head = dim_head
        self.heads = heads
        self.softmax = nn.Softmax(dim=-1)
        self.dropout = nn.Dropout(dropout)
        self.to_q = nn.Linear(dim_head, dim_head, bias=False)
        self.to_k = nn.Linear(dim_head, dim_head, bias=False)
        self.to_v = nn.Linear(dim_head, dim_head, bias=False)
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        # B N C
        B, N, C = x.shape
        x = x.reshape(B, N, self.heads, self.dim_head).permute(0, 2, 1, 3).contiguous()  # B H N C
        q = self.to_q(x)
        k = self.to_k(x)
        v = self.to_v(x)
        dots = torch.matmul(k.transpose(-1, -2), v) / float(N)
        dots = self.dropout(dots)
        res = torch.matmul(q, dots)  # B H G D
        res = rearrange(res, 'b h n d -> b n (h d)')
        return self.to_out(res)


class LinearAttention(nn.Module):
    """
    modified from https://github.com/HaoZhongkai/GNOT/blob/master/models/mmgpt.py
    """

    def __init__(self, dim, heads=8, dim_head=64, dropout=0., attn_type='l1', **kwargs):
        super(LinearAttention, self).__init__()
        self.key = nn.Linear(dim, dim)
        self.query = nn.Linear(dim, dim)
        self.value = nn.Linear(dim, dim)
        # regularization
        self.attn_drop = nn.Dropout(dropout)
        # output projection
        self.proj = nn.Linear(dim, dim)
        self.n_head = heads
        self.dim_head = dim_head
        self.attn_type = attn_type

    def forward(self, x, y=None):
        y = x if y is None else y
        B, T1, C = x.size()
        _, T2, _ = y.size()
        q = self.query(x).view(B, T1, self.n_head, self.dim_head).transpose(1, 2)  # (B, nh, T, hs)
        k = self.key(y).view(B, T2, self.n_head, self.dim_head).transpose(1, 2)  # (B, nh, T, hs)
        v = self.value(y).view(B, T2, self.n_head, self.dim_head).transpose(1, 2)  # (B, nh, T, hs)

        if self.attn_type == 'l1':
            q = q.softmax(dim=-1)
            k = k.softmax(dim=-1)
            k_cumsum = k.sum(dim=-2, keepdim=True)
            D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)  # normalized
        elif self.attn_type == "galerkin":
            q = q.softmax(dim=-1)
            k = k.softmax(dim=-1)
            D_inv = 1. / T2
        elif self.attn_type == "l2":  # still use l1 normalization
            q = q / q.norm(dim=-1, keepdim=True, p=1)
            k = k / k.norm(dim=-1, keepdim=True, p=1)
            k_cumsum = k.sum(dim=-2, keepdim=True)
            D_inv = 1. / (q * k_cumsum).abs().sum(dim=-1, keepdim=True)  # normalized
        else:
            raise NotImplementedError

        context = k.transpose(-2, -1) @ v
        y = self.attn_drop((q @ context) * D_inv + q)

        # output projection
        y = rearrange(y, 'b h n d -> b n (h d)')
        y = self.proj(y)
        return y

def exists(val):
    return val is not None

def default(value, d):
    return d if not exists(value) else value

def max_neg_value(tensor):
    return -torch.finfo(tensor.dtype).max

def linear_attn(q, k, v, kv_mask = None):
    dim = q.shape[-1]

    if exists(kv_mask):
        mask_value = max_neg_value(q)
        mask = kv_mask[:, None, :, None] if kv_mask is not None else None
        if mask is not None:
            k = k.masked_fill_(~mask, mask_value)
            v = v.masked_fill_(~mask, 0.)
        del mask

    q = q.softmax(dim=-1)
    k = k.softmax(dim=-2)

    q = q * dim ** -0.5

    context = einsum('bhnd,bhne->bhde', k, v)
    attn = einsum('bhnd,bhde->bhne', q, context)
    return attn.reshape(*q.shape)

def split_at_index(dim, index, t):
    pre_slices = (slice(None),) * dim
    l = (*pre_slices, slice(None, index))
    r = (*pre_slices, slice(index, None))
    return t[l], t[r]

class SelfAttention(nn.Module):
    def __init__(self, dim, heads, dim_head = None,dropout = 0.):
        super().__init__()
        assert dim_head or (dim % heads) == 0, 'embedding dimension must be divisible by number of heads'
        d_heads = default(dim_head, dim // heads)

        self.heads = heads
        self.d_heads = d_heads

        self.global_attn_heads = heads
        self.global_attn_fn = linear_attn
        

        self.to_q = nn.Linear(dim, d_heads * heads, bias = False)

        kv_heads = heads

        self.kv_heads = kv_heads
        self.to_k = nn.Linear(dim, d_heads * kv_heads, bias = False)
        self.to_v = nn.Linear(dim, d_heads * kv_heads, bias = False)

        self.to_out = nn.Linear(d_heads * heads, dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        q, k, v = (self.to_q(x), self.to_k(x), self.to_v(x))

        b, t, e, h, dh = *q.shape, self.heads, self.d_heads

        merge_heads = lambda x: x.reshape(*x.shape[:2], -1, dh).transpose(1, 2)

        q, k, v = map(merge_heads, (q, k, v))

        out = []

        split_index_fn = partial(split_at_index, 1, 0)

        (lq, q), (lk, k), (lv, v) = map(split_index_fn, (q, k, v))

        _, has_global = map(lambda x: x.shape[1] > 0, (lq, q))

        if has_global:
            global_out = self.global_attn_fn(q, k, v)
            out.append(global_out)

        attn = torch.cat(out, dim=1)
        attn = attn.transpose(1, 2).reshape(b, t, -1)
        return self.dropout(self.to_out(attn))
</file>

<file path="models/ono/embedding.py">
import math
import torch
import torch.nn as nn
from einops import rearrange
import numpy as np


def unified_pos_embedding(shapelist, ref, batchsize=1, device='cuda'):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device
    if len(shapelist) == 1:
        size_x = shapelist[0]
        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)
        grid = gridx.reshape(1, size_x, 1).repeat([batchsize, 1, 1]).to(device)  # B N 1
        gridx = torch.tensor(np.linspace(0, 1, ref), dtype=torch.float)
        grid_ref = gridx.reshape(1, ref, 1).repeat([batchsize, 1, 1]).to(device)  # B N 1
        pos = torch.sqrt(torch.sum((grid[:, :, None, :] - grid_ref[:, None, :, :]) ** 2, dim=-1)). \
            reshape(batchsize, size_x, ref).contiguous()
    if len(shapelist) == 2:
        size_x, size_y = shapelist[0], shapelist[1]
        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)
        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])
        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)
        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])
        grid = torch.cat((gridx, gridy), dim=-1).to(device)  # B H W 2

        gridx = torch.tensor(np.linspace(0, 1, ref), dtype=torch.float)
        gridx = gridx.reshape(1, ref, 1, 1).repeat([batchsize, 1, ref, 1])
        gridy = torch.tensor(np.linspace(0, 1, ref), dtype=torch.float)
        gridy = gridy.reshape(1, 1, ref, 1).repeat([batchsize, ref, 1, 1])
        grid_ref = torch.cat((gridx, gridy), dim=-1).to(device)  # B H W 8 8 2

        pos = torch.sqrt(torch.sum((grid[:, :, :, None, None, :] - grid_ref[:, None, None, :, :, :]) ** 2, dim=-1)). \
            reshape(batchsize, size_x * size_y, ref * ref).contiguous()
    if len(shapelist) == 3:
        size_x, size_y, size_z = shapelist[0], shapelist[1], shapelist[2]
        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)
        gridx = gridx.reshape(1, size_x, 1, 1, 1).repeat([batchsize, 1, size_y, size_z, 1])
        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)
        gridy = gridy.reshape(1, 1, size_y, 1, 1).repeat([batchsize, size_x, 1, size_z, 1])
        gridz = torch.tensor(np.linspace(0, 1, size_z), dtype=torch.float)
        gridz = gridz.reshape(1, 1, 1, size_z, 1).repeat([batchsize, size_x, size_y, 1, 1])
        grid = torch.cat((gridx, gridy, gridz), dim=-1).to(device)  # B H W D 3

        gridx = torch.tensor(np.linspace(0, 1, ref), dtype=torch.float)
        gridx = gridx.reshape(1, ref, 1, 1, 1).repeat([batchsize, 1, ref, ref, 1])
        gridy = torch.tensor(np.linspace(0, 1, ref), dtype=torch.float)
        gridy = gridy.reshape(1, 1, ref, 1, 1).repeat([batchsize, ref, 1, ref, 1])
        gridz = torch.tensor(np.linspace(0, 1, ref), dtype=torch.float)
        gridz = gridz.reshape(1, 1, 1, ref, 1).repeat([batchsize, ref, ref, 1, 1])
        grid_ref = torch.cat((gridx, gridy, gridz), dim=-1).to(device)  # B 4 4 4 3

        pos = torch.sqrt(
            torch.sum((grid[:, :, :, :, None, None, None, :] - grid_ref[:, None, None, None, :, :, :, :]) ** 2,
                      dim=-1)). \
            reshape(batchsize, size_x * size_y * size_z, ref * ref * ref).contiguous()
    return pos


class RotaryEmbedding(nn.Module):
    def __init__(self, dim, min_freq=1 / 2, scale=1.):
        super().__init__()
        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))
        self.min_freq = min_freq
        self.scale = scale
        self.register_buffer('inv_freq', inv_freq)

    def forward(self, coordinates, device='cuda'):
        # coordinates [b, n]
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device
        t = coordinates.to(device).type_as(self.inv_freq)
        t = t * (self.scale / self.min_freq)
        freqs = torch.einsum('... i , j -> ... i j', t, self.inv_freq)  # [b, n, d//2]
        return torch.cat((freqs, freqs), dim=-1)  # [b, n, d]


def rotate_half(x):
    x = rearrange(x, '... (j d) -> ... j d', j=2)
    x1, x2 = x.unbind(dim=-2)
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(t, freqs):
    return (t * freqs.cos()) + (rotate_half(t) * freqs.sin())


def apply_2d_rotary_pos_emb(t, freqs_x, freqs_y):
    # split t into first half and second half
    # t: [b, h, n, d]
    # freq_x/y: [b, n, d]
    d = t.shape[-1]
    t_x, t_y = t[..., :d // 2], t[..., d // 2:]

    return torch.cat((apply_rotary_pos_emb(t_x, freqs_x),
                      apply_rotary_pos_emb(t_y, freqs_y)), dim=-1)


class PositionalEncoding(nn.Module):
    "Implement the PE function."

    def __init__(self, d_model, dropout, max_len=421 * 421):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[:, : x.size(1)].requires_grad_(False)
        return self.dropout(x)


def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):
    """
    Create sinusoidal timestep embeddings.
    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param dim: the dimension of the output.
    :param max_period: controls the minimum frequency of the embeddings.
    :return: an [N x dim] Tensor of positional embeddings.
    """

    half = dim // 2
    freqs = torch.exp(
        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half
    ).to(device=timesteps.device)
    args = timesteps[:, None].float() * freqs[None]
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    if dim % 2:
        embedding = torch.cat([embedding, torch.zeros_like(embedding[:,:,:1])], dim=-1)
    return embedding
</file>

<file path="models/ono/ono.py">
import torch
import torch.nn as nn
from timm.layers.weight_init import trunc_normal_
from .basic import MLP, LinearAttention, SelfAttention as LinearSelfAttention
from ..base import timestep_embedding, unified_pos_embedding
import warnings


def psd_safe_cholesky(A, upper=False, out=None, jitter=None):
    """Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.
    Args:
        :attr:`A` (Tensor):
            The tensor to compute the Cholesky decomposition of
        :attr:`upper` (bool, optional):
            See torch.cholesky
        :attr:`out` (Tensor, optional):
            See torch.cholesky
        :attr:`jitter` (float, optional):
            The jitter to add to the diagonal of A in case A is only p.s.d. If omitted, chosen
            as 1e-6 (float) or 1e-8 (double)
    """
    try:
        L = torch.linalg.cholesky(A, upper=upper, out=out)
        if torch.isnan(L).any():
            raise RuntimeError
        return L
    except RuntimeError as e:
        isnan = torch.isnan(A)
        if isnan.any():
            raise ValueError(
                f"cholesky_cpu: {isnan.sum().item()} of {A.numel()} elements of the {A.shape} tensor are NaN."
            )

        if jitter is None:
            jitter = 1e-6 if A.dtype == torch.float32 else 1e-8
        Aprime = A.clone()
        jitter_prev = 0
        for i in range(10):
            jitter_new = jitter * (10 ** i)
            Aprime.diagonal(dim1=-2, dim2=-1).add_(jitter_new - jitter_prev)
            jitter_prev = jitter_new
            try:
                L = torch.linalg.cholesky(Aprime, upper=upper, out=out)
                warnings.warn(
                    f"A not p.d., added jitter of {jitter_new} to the diagonal",
                    RuntimeWarning,
                )
                return L
            except RuntimeError:
                continue
        raise e


class ONOBlock(nn.Module):
    """ONO encoder block."""

    def __init__(
            self,
            num_heads: int,
            hidden_dim: int,
            dropout: float,
            act='gelu',
            attn_type='nystrom',
            mlp_ratio=4,
            last_layer=False,
            momentum=0.9,
            psi_dim: int = 8,
            out_dim=1
    ):
        super().__init__()
        self.momentum = momentum
        self.psi_dim = psi_dim

        self.register_buffer("feature_cov", None)
        self.register_parameter("mu", nn.Parameter(torch.zeros(psi_dim))) # type: ignore
        self.ln_1 = nn.LayerNorm(hidden_dim)
        if attn_type == 'linear':
            self.Attn = LinearAttention(hidden_dim, heads=num_heads, dim_head=hidden_dim // num_heads, dropout=dropout,
                                        attn_type='galerkin')
        elif attn_type == 'selfAttention':
            self.Attn = LinearSelfAttention(hidden_dim, heads=num_heads, dim_head=hidden_dim // num_heads, dropout=dropout)
        else:
            raise ValueError('Attn type only supports nystrom or linear')
        self.ln_2 = nn.LayerNorm(hidden_dim)
        self.mlp = MLP(hidden_dim, hidden_dim * mlp_ratio, hidden_dim, n_layers=0, res=False, act=act)
        self.proj = nn.Linear(hidden_dim, psi_dim)
        self.ln_3 = nn.LayerNorm(hidden_dim)
        self.mlp2 = nn.Linear(hidden_dim, out_dim) if last_layer else MLP(hidden_dim, hidden_dim * mlp_ratio,
                                                                          hidden_dim, n_layers=0, res=False, act=act)

    def forward(self, x, fx):
        x = self.Attn(self.ln_1(x)) + x
        x = self.mlp(self.ln_2(x)) + x
        x_ = self.proj(x)
        if self.training:
            batch_cov = torch.einsum("blc, bld->cd", x_, x_) / x_.shape[0] / x_.shape[1]
            with torch.no_grad():
                if self.feature_cov is None:
                    self.feature_cov = batch_cov
                else:
                    self.feature_cov.mul_(self.momentum).add_(batch_cov, alpha=1 - self.momentum)
        else:
            batch_cov = self.feature_cov
        L = psd_safe_cholesky(batch_cov)
        L_inv_T = L.inverse().transpose(-2, -1)
        x_ = x_ @ L_inv_T

        fx = (x_ * torch.nn.functional.softplus(self.mu)) @ (x_.transpose(-2, -1) @ fx) + fx
        fx = self.mlp2(self.ln_3(fx))

        return x, fx


class ONO(nn.Module):
    ## speed up with flash attention
    def __init__(self, model_params: dict):
        super(ONO, self).__init__()
        self.__name__ = 'ONO'
        for key, value in model_params.items():
            setattr(self, key, value)
        ## embedding
        if self.unified_pos and self.geotype != 'unstructured':  # only for structured mesh
            self.pos = unified_pos_embedding(self.shapelist, self.ref)
            self.preprocess_x = MLP(self.ref ** len(self.shapelist), self.n_hidden * 2,
                                    self.n_hidden, n_layers=0, res=False, act=self.act)
            self.preprocess_z = MLP(self.fun_dim + self.ref ** len(self.shapelist), self.n_hidden * 2,
                                    self.n_hidden, n_layers=0, res=False, act=self.act)
        else:
            self.preprocess_x = MLP(self.fun_dim + self.space_dim, self.n_hidden * 2, self.n_hidden,
                                    n_layers=0, res=False, act=self.act)
            self.preprocess_z = MLP(self.fun_dim + self.space_dim, self.n_hidden * 2, self.n_hidden,
                                    n_layers=0, res=False, act=self.act)
        if self.time_input:
            self.time_fc = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden), nn.SiLU(),
                                         nn.Linear(self.n_hidden, self.n_hidden))
        ## models
        self.blocks = nn.ModuleList([ONOBlock(num_heads=self.n_heads, hidden_dim=self.n_hidden,
                                              dropout=self.dropout,
                                              act=self.act,
                                              mlp_ratio=self.mlp_ratio,
                                              out_dim=self.out_dim,
                                              psi_dim=self.psi_dim,
                                              attn_type=self.attn_type,
                                              last_layer=(_ == self.n_layers - 1))
                                     for _ in range(self.n_layers)])
        self.placeholder = nn.Parameter((1 / (self.n_hidden)) * torch.rand(self.n_hidden, dtype=torch.float))
        self.initialize_weights()

    def initialize_weights(self):
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x, fx=None, T=None, geo=None):
        x = x.view(x.shape[0], -1, x.shape[-1])
        if self.unified_pos:
            x = self.pos.repeat(x.shape[0], 1, 1)
        if fx is not None:
            x = torch.cat((x, fx), -1)
            fx = self.preprocess_z(x)
            x = self.preprocess_x(x)
        else:
            fx = self.preprocess_z(x)
            x = self.preprocess_x(x)
        fx = fx + self.placeholder[None, None, :]

        if T is not None:
            Time_emb = timestep_embedding(T, self.n_hidden).repeat(1, x.shape[1], 1)
            Time_emb = self.time_fc(Time_emb)
            fx = fx + Time_emb

        for block in self.blocks:
            x, fx = block(x, fx)
        return fx
</file>

<file path="models/swin_transformer/__init__.py">
from .swin_transformer_v2 import SwinTransformerV2
from .swin_mlp import SwinMLP
</file>

<file path="models/swin_transformer/swin_mlp.py">
# --------------------------------------------------------
# Swin Transformer
# Copyright (c) 2021 Microsoft
# Licensed under The MIT License [see LICENSE for details]
# Written by Ze Liu
# --------------------------------------------------------

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
from timm.layers.drop import DropPath
from timm.layers.helpers import to_2tuple
from timm.layers.weight_init import trunc_normal_


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class SwinMLPBlock(nn.Module):
    r""" Swin MLP Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        drop (float, optional): Dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.padding = [self.window_size - self.shift_size, self.shift_size,
                        self.window_size - self.shift_size, self.shift_size]  # P_l,P_r,P_t,P_b

        self.norm1 = norm_layer(dim)
        # use group convolution to implement multi-head MLP
        self.spatial_mlp = nn.Conv1d(self.num_heads * self.window_size ** 2,
                                     self.num_heads * self.window_size ** 2,
                                     kernel_size=1,
                                     groups=self.num_heads)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # shift
        if self.shift_size > 0:
            P_l, P_r, P_t, P_b = self.padding
            shifted_x = F.pad(x, [0, 0, P_l, P_r, P_t, P_b], "constant", 0)
        else:
            shifted_x = x
        _, _H, _W, _ = shifted_x.shape

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # Window/Shifted-Window Spatial MLP
        x_windows_heads = x_windows.view(-1, self.window_size * self.window_size, self.num_heads, C // self.num_heads)
        x_windows_heads = x_windows_heads.transpose(1, 2)  # nW*B, nH, window_size*window_size, C//nH
        x_windows_heads = x_windows_heads.reshape(-1, self.num_heads * self.window_size * self.window_size,
                                                  C // self.num_heads)
        spatial_mlp_windows = self.spatial_mlp(x_windows_heads)  # nW*B, nH*window_size*window_size, C//nH
        spatial_mlp_windows = spatial_mlp_windows.view(-1, self.num_heads, self.window_size * self.window_size,
                                                       C // self.num_heads).transpose(1, 2)
        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size * self.window_size, C)

        # merge windows
        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(spatial_mlp_windows, self.window_size, _H, _W)  # B H' W' C

        # reverse shift
        if self.shift_size > 0:
            P_l, P_r, P_t, P_b = self.padding
            x = shifted_x[:, P_t:-P_b, P_l:-P_r, :].contiguous()
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W

        # Window/Shifted-Window Spatial MLP
        if self.shift_size > 0:
            nW = (H / self.window_size + 1) * (W / self.window_size + 1)
        else:
            nW = H * W / self.window_size / self.window_size
        flops += nW * self.dim * (self.window_size * self.window_size) * (self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops


class PatchMerging(nn.Module):
    r""" Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        x: B, H*W, C
        """
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x

    def extra_repr(self) -> str:
        return f"input_resolution={self.input_resolution}, dim={self.dim}"

    def flops(self):
        H, W = self.input_resolution
        flops = H * W * self.dim
        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
        return flops


class BasicLayer(nn.Module):
    """ A basic Swin MLP layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        drop (float, optional): Dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., drop=0., drop_path: float | list[float] = 0.,
                 norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        self.blocks = nn.ModuleList([
            SwinMLPBlock(dim=dim, input_resolution=input_resolution,
                         num_heads=num_heads, window_size=window_size,
                         shift_size=0 if (i % 2 == 0) else window_size // 2,
                         mlp_ratio=mlp_ratio,
                         drop=drop,
                         drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                         norm_layer=norm_layer)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops


class PatchEmbed(nn.Module):
    r""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops


class SwinMLP(nn.Module):
    r""" Swin MLP

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin MLP layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        drop_rate (float): Dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """

    def __init__(
        self,
        model_params: dict, 
        **kwargs
        ):
        super().__init__()

        img_size = model_params.get('img_size', 224)
        patch_size = model_params.get('patch_size', 4)
        in_chans = model_params.get('in_chans', 3)
        num_classes = model_params.get('num_classes', 1000)
        embed_dim = model_params.get('embed_dim', 96)
        depths = model_params.get('depths', [2, 2, 6, 2])
        num_heads = model_params.get('num_heads', [3, 6, 12, 24])
        window_size = model_params.get('window_size', 7)
        mlp_ratio = model_params.get('mlp_ratio', 4.)
        drop_rate = model_params.get('drop_rate', 0.)
        drop_path_rate = model_params.get('drop_path_rate', 0.1)
        ape = model_params.get('ape', False)
        patch_norm = model_params.get('patch_norm', True)
        use_checkpoint = model_params.get('use_checkpoint', False)
        norm_layer = nn.LayerNorm
        
        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               drop=drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, (nn.Linear, nn.Conv1d)):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore()
    def no_weight_decay(self) -> set[str]:
        return {'absolute_pos_embed'}

    @torch.jit.ignore()
    def no_weight_decay_keywords(self) -> set[str]:
        return {'relative_position_bias_table'}

    def forward_features(self, x):
        x = self.patch_embed(x)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

    def flops(self):
        flops = 0
        flops += self.patch_embed.flops()
        for i, layer in enumerate(self.layers):
            flops += layer.flops()
        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)
        flops += self.num_features * self.num_classes
        return flops
</file>

<file path="models/swin_transformer/swin_transformer_v2.py">
# --------------------------------------------------------
# Swin Transformer V2
# Copyright (c) 2022 Microsoft
# Licensed under The MIT License [see LICENSE for details]
# Written by Ze Liu
# --------------------------------------------------------

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
from timm.layers.drop import DropPath
from timm.layers.helpers import to_2tuple
from timm.layers.weight_init import trunc_normal_
import numpy as np


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class WindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.,
                 pretrained_window_size=[0, 0]):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.pretrained_window_size = pretrained_window_size
        self.num_heads = num_heads

        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)

        # mlp to generate continuous relative position bias
        self.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),
                                     nn.ReLU(inplace=True),
                                     nn.Linear(512, num_heads, bias=False))

        # get relative_coords_table
        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)
        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)
        relative_coords_table = torch.stack(
            torch.meshgrid([relative_coords_h,
                            relative_coords_w])).permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2
        if pretrained_window_size[0] > 0:
            relative_coords_table[:, :, :, 0] /= (pretrained_window_size[0] - 1)
            relative_coords_table[:, :, :, 1] /= (pretrained_window_size[1] - 1)
        else:
            relative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)
            relative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)
        relative_coords_table *= 8  # normalize to -8, 8
        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(
            torch.abs(relative_coords_table) + 1.0) / np.log2(8)

        self.register_buffer("relative_coords_table", relative_coords_table)

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        if qkv_bias:
            self.q_bias = nn.Parameter(torch.zeros(dim))
            self.v_bias = nn.Parameter(torch.zeros(dim))
        else:
            self.q_bias = None
            self.v_bias = None
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        B_, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            if self.v_bias is None:
                raise ValueError("Unexpected v_bias is None when q_bias is not None")
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        # cosine attention
        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))
        max_logit = torch.log(
            torch.tensor(1.0 / 0.01, device=self.logit_scale.device)
        )
        logit_scale = torch.clamp(self.logit_scale, max=max_logit).exp()
        attn = attn * logit_scale

        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)
        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def extra_repr(self) -> str:
        return f'dim={self.dim}, window_size={self.window_size}, ' \
               f'pretrained_window_size={self.pretrained_window_size}, num_heads={self.num_heads}'

    def flops(self, N):
        # calculate flops for 1 window with token length of N
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops


class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        pretrained_window_size (int): Window size in pre-training.
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, pretrained_window_size=0):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,
            pretrained_window_size=to_2tuple(pretrained_window_size))

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)
        x = shortcut + self.drop_path(self.norm1(x))

        # FFN
        x = x + self.drop_path(self.norm2(self.mlp(x)))

        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops


class PatchMerging(nn.Module):
    r""" Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(2 * dim)

    def forward(self, x):
        """
        x: B, H*W, C
        """
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.reduction(x)
        x = self.norm(x)

        return x

    def extra_repr(self) -> str:
        return f"input_resolution={self.input_resolution}, dim={self.dim}"

    def flops(self):
        H, W = self.input_resolution
        flops = (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
        flops += H * W * self.dim // 2
        return flops


class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
        pretrained_window_size (int): Local window size in pre-training.
    """

    def __init__(
        self, 
        dim, 
        input_resolution, 
        depth, 
        num_heads, 
        window_size,
        mlp_ratio=4., 
        qkv_bias=True, 
        drop: float = 0., 
        attn_drop=0.,
        drop_path: float | list[float] = 0.,
        norm_layer=nn.LayerNorm, 
        downsample=None, 
        use_checkpoint=False,
        pretrained_window_size=0):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer,
                                 pretrained_window_size=pretrained_window_size)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops

    def _init_respostnorm(self):
        for blk in self.blocks:
            nn.init.constant_(blk.norm1.bias, 0)
            nn.init.constant_(blk.norm1.weight, 0)
            nn.init.constant_(blk.norm2.bias, 0)
            nn.init.constant_(blk.norm2.weight, 0)


class PatchEmbed(nn.Module):
    r""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops


class SwinTransformerV2(nn.Module):
    r""" Swin Transformer V2 (PDE version)

    Minimal adaptation for PDE:
      - Input : previous frame u_t,  shape (B, H, W, C_in)
      - Output: next frame u_{t+1}, shape (B, H, W, C_out)

    Hierarchical Swin backbone + token-wise linear projection +
    optional bilinear upsampling back to original resolution.
    """

    def __init__(
        self,
        model_params: dict,
        **kwargs,
    ) -> None:
        super().__init__()

        # ---- PDE-friendly configs ----
        # 允许 in_channels / in_chans 两种写法
        in_chans: int = model_params.get("in_channels", model_params.get("in_chans", 1))
        out_chans: int = model_params.get("out_channels", in_chans)

        img_size: int = model_params.get("img_size", 64)
        patch_size: int = model_params.get("patch_size", 1)  # PDE 场景建议 1
        embed_dim: int = model_params.get("embed_dim", 96)
        depths: list[int] = model_params.get("depths", [2, 2, 6, 2])
        num_heads: list[int] = model_params.get("num_heads", [3, 6, 12, 24])
        window_size: int = model_params.get("window_size", 7)
        mlp_ratio: float = model_params.get("mlp_ratio", 4.0)
        qkv_bias: bool = model_params.get("qkv_bias", True)
        drop_rate: float = model_params.get("drop_rate", 0.0)
        attn_drop_rate: float = model_params.get("attn_drop_rate", 0.0)
        drop_path_rate: float = model_params.get("drop_path_rate", 0.1)
        ape: bool = model_params.get("ape", False)
        patch_norm: bool = model_params.get("patch_norm", True)
        use_checkpoint: bool = model_params.get("use_checkpoint", False)
        pretrained_window_sizes: list[int] = model_params.get(
            "pretrained_window_sizes", [0, 0, 0, 0]
        )
        norm_layer: type[nn.Module] = nn.LayerNorm

        self.in_channels = in_chans
        self.out_channels = out_chans
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.mlp_ratio = mlp_ratio
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))

        # ---- Patch embedding ----
        self.patch_embed = PatchEmbed(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=in_chans,
            embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None,
        )
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution  # [H_p, W_p]
        self.patches_resolution = patches_resolution

        # ---- Absolute position embedding (optional) ----
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(
                torch.zeros(1, num_patches, embed_dim)
            )
            trunc_normal_(self.absolute_pos_embed, std=0.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # ---- Stochastic depth schedule ----
        dpr = [
            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))
        ]  # total len = sum(depths)

        # ---- Build hierarchical Swin stages ----
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(
                dim=int(embed_dim * 2 ** i_layer),
                input_resolution=(
                    patches_resolution[0] // (2 ** i_layer),
                    patches_resolution[1] // (2 ** i_layer),
                ),
                depth=depths[i_layer],
                num_heads=num_heads[i_layer],
                window_size=window_size,
                mlp_ratio=self.mlp_ratio,
                qkv_bias=qkv_bias,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],
                norm_layer=norm_layer,
                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                use_checkpoint=use_checkpoint,
                pretrained_window_size=pretrained_window_sizes[i_layer],
            )
            self.layers.append(layer)

        # 最终的 patch 分辨率（例如 8x8）
        self.final_patches_resolution = (
            self.patches_resolution[0] // (2 ** (self.num_layers - 1)),
            self.patches_resolution[1] // (2 ** (self.num_layers - 1)),
        )

        # ---- Final norm & projection to PDE channel dimension ----
        self.norm = norm_layer(self.num_features)
        self.out_proj = nn.Linear(self.num_features, self.out_channels)

        # 初始化权重
        self.apply(self._init_weights)
        for bly in self.layers:
            bly._init_respostnorm()

    def _init_weights(self, m: nn.Module) -> None:
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore()
    def no_weight_decay(self) -> set[str]:
        return {"absolute_pos_embed"}

    @torch.jit.ignore()
    def no_weight_decay_keywords(self) -> set[str]:
        # SwinV2 论文中建议 logit_scale & cpb_mlp 不做 weight decay
        return {"cpb_mlp", "logit_scale", "relative_position_bias_table"}

    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, C_in, H, W)
        return: (B, L_last, C_feat)
        """
        x = self.patch_embed(x)  # (B, L, C_embed)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)          # (B, L', C_feat)

        x = self.norm(x)          # (B, L_last, C_feat)
        return x

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        PDE forward:
          input  x: (B, H, W, C_in)   - u_t
          output y: (B, H, W, C_out) - u_{t+1}
        """
        assert x.dim() == 4, "Expected input shape (B, H, W, C_in)"
        B, H, W, C_in = x.shape
        assert (
            C_in == self.in_channels
        ), f"Input channels {C_in} != configured in_channels {self.in_channels}"

        # (B, H, W, C_in) -> (B, C_in, H, W)
        x = x.permute(0, 3, 1, 2).contiguous()

        # Hierarchical Swin backbone
        x = self.forward_features(x)   # (B, L, C_feat)
        x = self.out_proj(x)           # (B, L, C_out)

        # 恢复到最终 patch 分辨率
        H_f, W_f = self.final_patches_resolution
        L = x.shape[1]
        assert (
            L == H_f * W_f
        ), f"Token length {L} != H_f*W_f {H_f*W_f}, check img_size/patch_size/depths"

        x = x.view(B, H_f, W_f, self.out_channels)  # (B, H_f, W_f, C_out)

        # 如果最终 patch 分辨率比原始小（有 patch / downsample），做一次双线性插值
        if (H_f, W_f) != (H, W):
            x = F.interpolate(
                x.permute(0, 3, 1, 2),   # (B, C_out, H_f, W_f)
                size=(H, W),
                mode="bilinear",
                align_corners=False,
            ).permute(0, 2, 3, 1)       # (B, H, W, C_out)

        return x

    def flops(self) -> float:
        """Rough FLOPs estimation."""
        flops = 0.0
        flops += self.patch_embed.flops()
        for layer in self.layers:
            flops += layer.flops()

        H_p, W_p = self.patches_resolution
        num_tokens = H_p * W_p // (2 ** (self.num_layers - 1))

        # final norm + out_proj
        flops += num_tokens * self.num_features          # LayerNorm
        flops += num_tokens * self.num_features * self.out_channels  # Linear

        return flops
</file>

<file path="models/transformer/__init__.py">
# models/transformer/__init__.py
from .transformer import Transformer

__all__ = ["Transformer"]
</file>

<file path="models/transformer/transformer.py">
# models/transformer/transformer.py
import math
from typing import Optional, Dict, Any

from torch import nn, Tensor

from ..base import (
    MultiHeadSelfAttention,
    BaseMLP,
    RoPE1DAdapter,
    RoPE2DAdapter,
    RoPE3DAdapter,
    unified_pos_embedding
)


class TransformerBlock(nn.Module):
    """
    Standard pre-norm Transformer block:

        x -> LN -> MHSA -> Drop -> + (residual)
        x -> LN -> MLP  -> Drop -> + (residual)
    """

    def __init__(
        self,
        d_model: int,
        num_heads: int,
        attn_dropout: float = 0.0,
        proj_dropout: float = 0.0,
        mlp_ratio: float = 4.0,
        mlp_dropout: float = 0.0,
        activation: str = "gelu",
        use_flash: bool = False,
        rope_adapter: Optional[nn.Module] = None,
    ) -> None:
        super().__init__()

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.self_attn = MultiHeadSelfAttention(
            d_model=d_model,
            num_heads=num_heads,
            attn_dropout=attn_dropout,
            proj_dropout=proj_dropout,
            use_flash=use_flash,
            rope_adapter=rope_adapter,
        )

        # FFN implemented via BaseMLP (no internal residual; residual is in the block)
        self.mlp = BaseMLP(
            in_dim=d_model,
            out_dim=d_model,
            hidden_dims=int(d_model * mlp_ratio),
            activation=activation,
            dropout=mlp_dropout,
            use_residual=False,
            residual_proj=False,
            last_activation=False,
        )

        self.dropout1 = nn.Dropout(proj_dropout)
        self.dropout2 = nn.Dropout(proj_dropout)

    def forward(
        self,
        x: Tensor,
        attn_bias: Optional[Tensor] = None,
        rope_kwargs: Optional[Dict[str, Any]] = None,
    ) -> Tensor:
        """
        Args:
            x:          (B, N, d_model)
            attn_bias:  (N, N) or (B, N, N), added to attention logits.
            rope_kwargs: optional kwargs passed to rope_adapter inside attention.
        """
        # Self-attention sub-layer
        x_res = x
        x_norm = self.norm1(x)
        x_attn = self.self_attn(
            x_norm,
            attn_mask=attn_bias,
            key_padding_mask=None,
            rope_kwargs=rope_kwargs,
        )
        x = x_res + self.dropout1(x_attn)

        # MLP sub-layer
        x_res = x
        x_norm = self.norm2(x)
        x_mlp = self.mlp(x_norm)
        x = x_res + self.dropout2(x_mlp)

        return x


class Transformer(nn.Module):
    """
    Generic Transformer for grid-based PDE fields.

    Input:
        x: (B, *spatial, C_in)
           - spatial can be 1D (L), 2D (H, W), 3D (T, H, W), etc.
    Output:
        y: (B, *spatial, C_out)

    RoPE usage:
        - rope_ndim = 1: 1D RoPE (coords: (B, *spatial, 1))
        - rope_ndim = 2: 2D RoPE (coords: (B, *spatial, 2))
        - rope_ndim = 3: 3D RoPE (coords: (B, *spatial, 3))
    """

    def __init__(
        self,
        model_params: dict,
        **kwargs,
    ) -> None:
        super().__init__()

        # Basic model hyper-parameters
        self.in_channels = model_params.get("in_channels", 1)
        self.out_channels = model_params.get("out_channels", 1)
        self.d_model = model_params.get("d_model", 64)
        self.num_heads = model_params.get("num_heads", 4)
        self.num_layers = model_params.get("num_layers", 4)

        # RoPE control
        self.use_rope = model_params.get("use_rope", False)
        self.rope_ndim = model_params.get("rope_ndim", 2)  # 1, 2, or 3
        
        # Pos embedding option (unified PE)
        self.use_uni_pos = model_params.get("use_uni_pos", False)

        attn_dropout = model_params.get("attn_dropout", 0.0)
        proj_dropout = model_params.get("proj_dropout", 0.0)
        mlp_ratio = model_params.get("mlp_ratio", 4.0)
        mlp_dropout = model_params.get("mlp_dropout", 0.0)
        activation = model_params.get("activation", "gelu")
        use_flash = model_params.get("use_flash", False)

        # Project input features to model dimension
        # in_channels already includes any extra channels (e.g. concatenated coords)
        self.input_proj = nn.Linear(self.in_channels, self.d_model)
        self.output_proj = nn.Linear(self.d_model, self.out_channels)

        # RoPE adapter selection
        if self.use_rope:
            if self.rope_ndim is None:
                raise ValueError("rope_ndim must be set to 1/2/3 when use_rope=True.")
            assert self.d_model % self.num_heads == 0, "d_model must be divisible by num_heads."
            head_dim = self.d_model // self.num_heads

            if self.rope_ndim == 1:
                self.rope_adapter: Optional[nn.Module] = RoPE1DAdapter(head_dim=head_dim)
            elif self.rope_ndim == 2:
                self.rope_adapter = RoPE2DAdapter(head_dim=head_dim)
            elif self.rope_ndim == 3:
                self.rope_adapter = RoPE3DAdapter(head_dim=head_dim)
            else:
                raise ValueError(f"Unsupported rope_ndim={self.rope_ndim}. Use 1, 2, or 3.")
        else:
            self.rope_adapter = None

        # Transformer layers
        self.blocks = nn.ModuleList([
            TransformerBlock(
                d_model=self.d_model,
                num_heads=self.num_heads,
                attn_dropout=attn_dropout,
                proj_dropout=proj_dropout,
                mlp_ratio=mlp_ratio,
                mlp_dropout=mlp_dropout,
                activation=activation,
                use_flash=use_flash,
                rope_adapter=self.rope_adapter,
            )
            for _ in range(self.num_layers)
        ])

        self.norm_out = nn.LayerNorm(self.d_model)

    def forward(
        self,
        x: Tensor,
        attn_bias: Optional[Tensor] = None,
    ) -> Tensor:
        """
        Args:
            x:
                Tensor of shape (B, *spatial, C_in), e.g.
                    1D: (B, L, C_in)
                    2D: (B, H, W, C_in)
                    3D: (B, T, H, W, C_in)
                in_channels in config must match C_in.

            coords:
                Optional coordinates used for RoPE.
                Shape: (B, *spatial, rope_ndim), with rope_ndim = 1 / 2 / 3.
                Values are usually normalized to [0, 1].

                If you also want coords as raw input features, concatenate them
                to x before calling forward and increase in_channels accordingly.

            attn_bias:
                Optional attention bias tensor:
                    - shape (N, N) or (B, N, N)
                    - N = product of spatial dimensions
                This is added to attention logits (e.g., from unified_pos_embedding).

        Returns:
            y: Tensor of shape (B, *spatial, C_out)
        """
        assert x.dim() >= 3, "x must be of shape (B, *spatial, C_in)."

        B = x.shape[0]
        spatial_shape = x.shape[1:-1]   # tuple of 1D/2D/3D...
        C_in = x.shape[-1]
        assert C_in == self.in_channels, \
            f"Expected in_channels={self.in_channels}, but got {C_in}."
        
        # Flatten spatial dimensions to token sequence
        N = int(math.prod(spatial_shape))  # number of tokens
        x = x.view(B, N, C_in)             # (B, N, C_in)
        coords = x[..., :self.rope_ndim] if C_in > 1 else None  # (B, N, rope_ndim) or None
        x = self.input_proj(x)             # (B, N, d_model)

        # Build RoPE kwargs if needed
        if self.use_rope:
            if coords is None:
                raise ValueError("coords must be provided when use_rope=True.")

            if coords.dim() != x.dim():  # (B, *spatial, rope_ndim)
                raise ValueError(
                    "coords must have shape (B, *spatial, rope_ndim). "
                    f"Got coords.dim()={coords.dim()}, expected {x.dim()}."
                )

            d_coord = coords.shape[-1]
            if d_coord < self.rope_ndim:
                raise ValueError(
                    f"coords last dim={d_coord} < rope_ndim={self.rope_ndim}."
                )

            coords_flat = coords.view(B, N, d_coord)  # (B, N, d_coord)

            if self.rope_ndim == 1:
                coords_1d = coords_flat[..., 0]  # (B, N)
                rope_kwargs = {"coords": coords_1d}
            elif self.rope_ndim == 2:
                coords_x = coords_flat[..., 0]   # (B, N)
                coords_y = coords_flat[..., 1]   # (B, N)
                rope_kwargs = {"coords_x": coords_x, "coords_y": coords_y}
            elif self.rope_ndim == 3:
                coords_x = coords_flat[..., 0]
                coords_y = coords_flat[..., 1]
                coords_z = coords_flat[..., 2]
                rope_kwargs = {
                    "coords_x": coords_x,
                    "coords_y": coords_y,
                    "coords_z": coords_z,
                }
            else:
                rope_kwargs = None
        else:
            rope_kwargs = None
        
        # Add unified positional embedding if enabled
        if self.use_uni_pos:
            attn_bias = unified_pos_embedding(
                shape_list=spatial_shape,
                ref = min(spatial_shape),
                batch_size=B,
                device=x.device,
            )  # (B, N, N)

        # Transformer encoder
        for blk in self.blocks:
            x = blk(x, attn_bias=attn_bias, rope_kwargs=rope_kwargs)

        x = self.norm_out(x)
        x = self.output_proj(x)            # (B, N, C_out)

        # Reshape back to grid
        y = x.view(B, *spatial_shape, self.out_channels)
        return y
</file>

<file path="models/transolver/__init__.py">
from .transolver import Transolver
</file>

<file path="models/transolver/basic.py">
import torch.nn as nn

ACTIVATION = {
    'gelu': nn.GELU,
    'tanh': nn.Tanh,
    'sigmoid': nn.Sigmoid,
    'relu': nn.ReLU,
    'leaky_relu': nn.LeakyReLU(0.1),
    'softplus': nn.Softplus,
    'ELU': nn.ELU,
    'silu': nn.SiLU
}


class MLP(nn.Module):
    def __init__(self, n_input, n_hidden, n_output, n_layers=1, act='gelu', res=True):
        super(MLP, self).__init__()

        if act in ACTIVATION.keys():
            act = ACTIVATION[act]
        else:
            raise NotImplementedError
        self.n_input = n_input
        self.n_hidden = n_hidden
        self.n_output = n_output
        self.n_layers = n_layers
        self.res = res
        self.linear_pre = nn.Sequential(nn.Linear(n_input, n_hidden), act())
        self.linear_post = nn.Linear(n_hidden, n_output)
        self.linears = nn.ModuleList([nn.Sequential(nn.Linear(n_hidden, n_hidden), act()) for _ in range(n_layers)])

    def forward(self, x):
        x = self.linear_pre(x)
        for i in range(self.n_layers):
            if self.res:
                x = self.linears[i](x) + x
            else:
                x = self.linears[i](x)
        x = self.linear_post(x)
        return x
</file>

<file path="models/transolver/physics_attention.py">
"""Physics-aware attention modules for various mesh types.

This module provides compact, documented implementations of physics-aware
attention for irregular and structured 1/2/3D meshes. Types and short
docstrings were added for improved Pylance support and consistency with
other models.
"""

from typing import Optional, Sequence

import torch
import torch.nn as nn
from einops import rearrange


class PhysicsAttentionIrregularMesh(nn.Module):
    """Attention for irregular (unstructured) meshes.

    Args:
        dim: input channel dimension
        heads: number of attention heads
        dim_head: dimension per head
        dropout: dropout probability
        slice_num: number of slice tokens
        shapelist: optional shape info (unused for irregular meshes)
    """

    def __init__(
        self,
        dim: int,
        heads: int = 8,
        dim_head: int = 64,
        dropout: float = 0.0,
        slice_num: int = 64,
        shapelist: Optional[Sequence[int]] = None,
    ) -> None:
        super().__init__()
        inner_dim = dim_head * heads
        self.dim_head = dim_head
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.softmax = nn.Softmax(dim=-1)
        self.dropout = nn.Dropout(dropout)
        self.temperature = nn.Parameter(torch.ones([1, heads, 1, 1]) * 0.5)

        self.in_project_x = nn.Linear(dim, inner_dim)
        self.in_project_fx = nn.Linear(dim, inner_dim)
        self.in_project_slice = nn.Linear(dim_head, slice_num)
        for l in [self.in_project_slice]:
            torch.nn.init.orthogonal_(l.weight)
        self.to_q = nn.Linear(dim_head, dim_head, bias=False)
        self.to_k = nn.Linear(dim_head, dim_head, bias=False)
        self.to_v = nn.Linear(dim_head, dim_head, bias=False)
        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass.

        Args:
            x: tensor of shape (B, N, C)

        Returns:
            Tensor of shape (B, N, C)
        """
        B, N, C = x.shape

        fx_mid = (
            self.in_project_fx(x)
            .reshape(B, N, self.heads, self.dim_head)
            .permute(0, 2, 1, 3)
            .contiguous()
        )
        x_mid = (
            self.in_project_x(x)
            .reshape(B, N, self.heads, self.dim_head)
            .permute(0, 2, 1, 3)
            .contiguous()
        )
        slice_weights = self.softmax(self.in_project_slice(x_mid) / self.temperature)
        slice_norm = slice_weights.sum(2)
        slice_token = torch.einsum("bhnc,bhng->bhgc", fx_mid, slice_weights)
        slice_token = slice_token / ((slice_norm + 1e-5)[:, :, :, None].repeat(1, 1, 1, self.dim_head))

        q_slice_token = self.to_q(slice_token)
        k_slice_token = self.to_k(slice_token)
        v_slice_token = self.to_v(slice_token)
        dots = torch.matmul(q_slice_token, k_slice_token.transpose(-1, -2)) * self.scale
        attn = self.softmax(dots)
        attn = self.dropout(attn)
        out_slice_token = torch.matmul(attn, v_slice_token)

        out_x = torch.einsum("bhgc,bhng->bhnc", out_slice_token, slice_weights)
        out_x = rearrange(out_x, "b h n d -> b n (h d)")
        return self.to_out(out_x)


class PhysicsAttentionStructuredMesh1D(nn.Module):
    """Attention for structured 1D meshes.

    Args:
        dim: input channel dimension
        heads: number of attention heads
        dim_head: dimension per head
        dropout: dropout probability
        slice_num: number of slice tokens
        shapelist: shape list, required for structured meshes
        kernel: convolution kernel size used for local projection
    """

    def __init__(
        self,
        dim: int,
        heads: int = 8,
        dim_head: int = 64,
        dropout: float = 0.0,
        slice_num: int = 64,
        shapelist: Optional[Sequence[int]] = None,
        kernel: int = 3,
    ) -> None:
        super().__init__()
        inner_dim = dim_head * heads
        self.dim_head = dim_head
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.softmax = nn.Softmax(dim=-1)
        self.dropout = nn.Dropout(dropout)
        self.temperature = nn.Parameter(torch.ones([1, heads, 1, 1]) * 0.5)
        assert shapelist is not None, "shapelist must be provided for structured meshes"
        self.length = shapelist[0]

        self.in_project_x = nn.Conv1d(dim, inner_dim, kernel, 1, kernel // 2)
        self.in_project_fx = nn.Conv1d(dim, inner_dim, kernel, 1, kernel // 2)
        self.in_project_slice = nn.Linear(dim_head, slice_num)
        for l in [self.in_project_slice]:
            torch.nn.init.orthogonal_(l.weight)
        self.to_q = nn.Linear(dim_head, dim_head, bias=False)
        self.to_k = nn.Linear(dim_head, dim_head, bias=False)
        self.to_v = nn.Linear(dim_head, dim_head, bias=False)
        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward for structured 1D input; x shape (B, N, C)."""
        B, N, C = x.shape
        x = x.reshape(B, self.length, C).contiguous().permute(0, 2, 1).contiguous()

        fx_mid = (
            self.in_project_fx(x)
            .permute(0, 2, 1)
            .contiguous()
            .reshape(B, N, self.heads, self.dim_head)
            .permute(0, 2, 1, 3)
            .contiguous()
        )
        x_mid = (
            self.in_project_x(x)
            .permute(0, 2, 1)
            .contiguous()
            .reshape(B, N, self.heads, self.dim_head)
            .permute(0, 2, 1, 3)
            .contiguous()
        )
        slice_weights = self.softmax(self.in_project_slice(x_mid) / torch.clamp(self.temperature, min=0.1, max=5))
        slice_norm = slice_weights.sum(2)
        slice_token = torch.einsum("bhnc,bhng->bhgc", fx_mid, slice_weights)
        slice_token = slice_token / ((slice_norm + 1e-5)[:, :, :, None].repeat(1, 1, 1, self.dim_head))

        q_slice_token = self.to_q(slice_token)
        k_slice_token = self.to_k(slice_token)
        v_slice_token = self.to_v(slice_token)
        dots = torch.matmul(q_slice_token, k_slice_token.transpose(-1, -2)) * self.scale
        attn = self.softmax(dots)
        attn = self.dropout(attn)
        out_slice_token = torch.matmul(attn, v_slice_token)

        out_x = torch.einsum("bhgc,bhng->bhnc", out_slice_token, slice_weights)
        out_x = rearrange(out_x, "b h n d -> b n (h d)")
        return self.to_out(out_x)


class PhysicsAttentionStructuredMesh2D(nn.Module):
    """Attention for structured 2D meshes."""

    def __init__(
        self,
        dim: int,
        heads: int = 8,
        dim_head: int = 64,
        dropout: float = 0.0,
        slice_num: int = 64,
        shapelist: Optional[Sequence[int]] = None,
        kernel: int = 3,
    ) -> None:
        super().__init__()
        inner_dim = dim_head * heads
        self.dim_head = dim_head
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.softmax = nn.Softmax(dim=-1)
        self.dropout = nn.Dropout(dropout)
        self.temperature = nn.Parameter(torch.ones([1, heads, 1, 1]) * 0.5)
        assert shapelist is not None, "shapelist must be provided for structured meshes"
        self.H = shapelist[0]
        self.W = shapelist[1]

        self.in_project_x = nn.Conv2d(dim, inner_dim, kernel, 1, kernel // 2)
        self.in_project_fx = nn.Conv2d(dim, inner_dim, kernel, 1, kernel // 2)
        self.in_project_slice = nn.Linear(dim_head, slice_num)
        for l in [self.in_project_slice]:
            torch.nn.init.orthogonal_(l.weight)
        self.to_q = nn.Linear(dim_head, dim_head, bias=False)
        self.to_k = nn.Linear(dim_head, dim_head, bias=False)
        self.to_v = nn.Linear(dim_head, dim_head, bias=False)
        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward for structured 2D input; x shape (B, N, C)."""
        B, N, C = x.shape
        x = x.reshape(B, self.H, self.W, C).contiguous().permute(0, 3, 1, 2).contiguous()

        fx_mid = (
            self.in_project_fx(x)
            .permute(0, 2, 3, 1)
            .contiguous()
            .reshape(B, N, self.heads, self.dim_head)
            .permute(0, 2, 1, 3)
            .contiguous()
        )
        x_mid = (
            self.in_project_x(x)
            .permute(0, 2, 3, 1)
            .contiguous()
            .reshape(B, N, self.heads, self.dim_head)
            .permute(0, 2, 1, 3)
            .contiguous()
        )
        slice_weights = self.softmax(self.in_project_slice(x_mid) / torch.clamp(self.temperature, min=0.1, max=5))
        slice_norm = slice_weights.sum(2)
        slice_token = torch.einsum("bhnc,bhng->bhgc", fx_mid, slice_weights)
        slice_token = slice_token / ((slice_norm + 1e-5)[:, :, :, None].repeat(1, 1, 1, self.dim_head))

        q_slice_token = self.to_q(slice_token)
        k_slice_token = self.to_k(slice_token)
        v_slice_token = self.to_v(slice_token)
        dots = torch.matmul(q_slice_token, k_slice_token.transpose(-1, -2)) * self.scale
        attn = self.softmax(dots)
        attn = self.dropout(attn)
        out_slice_token = torch.matmul(attn, v_slice_token)

        out_x = torch.einsum("bhgc,bhng->bhnc", out_slice_token, slice_weights)
        out_x = rearrange(out_x, "b h n d -> b n (h d)")
        return self.to_out(out_x)


class PhysicsAttentionStructuredMesh3D(nn.Module):
    """Attention for structured 3D meshes."""

    def __init__(
        self,
        dim: int,
        heads: int = 8,
        dim_head: int = 64,
        dropout: float = 0.0,
        slice_num: int = 32,
        shapelist: Optional[Sequence[int]] = None,
        kernel: int = 3,
    ) -> None:
        super().__init__()
        inner_dim = dim_head * heads
        self.dim_head = dim_head
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.softmax = nn.Softmax(dim=-1)
        self.dropout = nn.Dropout(dropout)
        self.temperature = nn.Parameter(torch.ones([1, heads, 1, 1]) * 0.5)
        assert shapelist is not None, "shapelist must be provided for structured meshes"
        self.H = shapelist[0]
        self.W = shapelist[1]
        self.D = shapelist[2]

        self.in_project_x = nn.Conv3d(dim, inner_dim, kernel, 1, kernel // 2)
        self.in_project_fx = nn.Conv3d(dim, inner_dim, kernel, 1, kernel // 2)
        self.in_project_slice = nn.Linear(dim_head, slice_num)
        for l in [self.in_project_slice]:
            torch.nn.init.orthogonal_(l.weight)
        self.to_q = nn.Linear(dim_head, dim_head, bias=False)
        self.to_k = nn.Linear(dim_head, dim_head, bias=False)
        self.to_v = nn.Linear(dim_head, dim_head, bias=False)
        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward for structured 3D input; x shape (B, N, C)."""
        B, N, C = x.shape
        x = x.reshape(B, self.H, self.W, self.D, C).contiguous().permute(0, 4, 1, 2, 3).contiguous()

        fx_mid = (
            self.in_project_fx(x)
            .permute(0, 2, 3, 4, 1)
            .contiguous()
            .reshape(B, N, self.heads, self.dim_head)
            .permute(0, 2, 1, 3)
            .contiguous()
        )
        x_mid = (
            self.in_project_x(x)
            .permute(0, 2, 3, 4, 1)
            .contiguous()
            .reshape(B, N, self.heads, self.dim_head)
            .permute(0, 2, 1, 3)
            .contiguous()
        )
        slice_weights = self.softmax(self.in_project_slice(x_mid) / torch.clamp(self.temperature, min=0.1, max=5))
        slice_norm = slice_weights.sum(2)
        slice_token = torch.einsum("bhnc,bhng->bhgc", fx_mid, slice_weights)
        slice_token = slice_token / ((slice_norm + 1e-5)[:, :, :, None].repeat(1, 1, 1, self.dim_head))

        q_slice_token = self.to_q(slice_token)
        k_slice_token = self.to_k(slice_token)
        v_slice_token = self.to_v(slice_token)
        dots = torch.matmul(q_slice_token, k_slice_token.transpose(-1, -2)) * self.scale
        attn = self.softmax(dots)
        attn = self.dropout(attn)
        out_slice_token = torch.matmul(attn, v_slice_token)

        out_x = torch.einsum("bhgc,bhng->bhnc", out_slice_token, slice_weights)
        out_x = rearrange(out_x, "b h n d -> b n (h d)")
        return self.to_out(out_x)
</file>

<file path="models/transolver/transolver.py">
import torch
import torch.nn as nn
from timm.layers.weight_init import trunc_normal_
from models.base import timestep_embedding, unified_pos_embedding
from models.transolver.basic import MLP
from models.transolver.physics_attention import PhysicsAttentionIrregularMesh
from models.transolver.physics_attention import PhysicsAttentionStructuredMesh1D
from models.transolver.physics_attention import PhysicsAttentionStructuredMesh2D
from models.transolver.physics_attention import PhysicsAttentionStructuredMesh3D

PHYSICS_ATTENTION = {
    'unstructured': PhysicsAttentionIrregularMesh,
    'structured_1D': PhysicsAttentionStructuredMesh1D,
    'structured_2D': PhysicsAttentionStructuredMesh2D,
    'structured_3D': PhysicsAttentionStructuredMesh3D
}


class TransolverBlock(nn.Module):
    """Transolver encoder block."""

    def __init__(
            self,
            num_heads: int,
            hidden_dim: int,
            dropout: float,
            act='gelu',
            mlp_ratio=4,
            last_layer=False,
            out_dim=1,
            slice_num=32,
            geotype='unstructured',
            shapelist=None
    ):
        super().__init__()
        self.last_layer = last_layer
        self.ln_1 = nn.LayerNorm(hidden_dim)

        self.Attn = PHYSICS_ATTENTION[geotype](hidden_dim, heads=num_heads, dim_head=hidden_dim // num_heads,
                                               dropout=dropout, slice_num=slice_num, shapelist=shapelist)
        self.ln_2 = nn.LayerNorm(hidden_dim)
        self.mlp = MLP(hidden_dim, hidden_dim * mlp_ratio, hidden_dim, n_layers=0, res=False, act=act)
        if self.last_layer:
            self.ln_3 = nn.LayerNorm(hidden_dim)
            self.mlp2 = nn.Linear(hidden_dim, out_dim)

    def forward(self, fx):
        fx = self.Attn(self.ln_1(fx)) + fx
        fx = self.mlp(self.ln_2(fx)) + fx
        if self.last_layer:
            return self.mlp2(self.ln_3(fx))
        else:
            return fx


class Transolver(nn.Module):
    def __init__(self, model_params):
        super(Transolver, self).__init__()
        self.__name__ = 'Transolver'
        for key, value in model_params.items():
            setattr(self, key, value)
        ## embedding
        if self.unified_pos and self.geotype != 'unstructured':  # only for structured mesh
            self.pos = unified_pos_embedding(self.shapelist, self.ref)
            self.preprocess = MLP(self.fun_dim + self.ref ** len(self.shapelist), self.n_hidden * 2,
                                  self.n_hidden, n_layers=0, res=False, act=self.act)
        else:
            self.preprocess = MLP(self.fun_dim + self.space_dim, self.n_hidden * 2, self.n_hidden,
                                  n_layers=0, res=False, act=self.act)
        if self.time_input:
            self.time_fc = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden), nn.SiLU(),
                                         nn.Linear(self.n_hidden, self.n_hidden))

        ## models
        self.blocks = nn.ModuleList([TransolverBlock(num_heads=self.n_heads, hidden_dim=self.n_hidden,
                                                      dropout=self.dropout,
                                                      act=self.act,
                                                      mlp_ratio=self.mlp_ratio,
                                                      out_dim=self.out_dim,
                                                      slice_num=self.slice_num,
                                                      last_layer=(_ == self.n_layers - 1),
                                                      geotype=self.geotype,
                                                      shapelist=self.shapelist)
                                     for _ in range(self.n_layers)])
        self.placeholder = nn.Parameter((1 / (self.n_hidden)) * torch.rand(self.n_hidden, dtype=torch.float))
        self.initialize_weights()

    def initialize_weights(self):
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def structured_geo(self, x, fx, T=None):
        if self.unified_pos:
            x = self.pos.repeat(x.shape[0], 1, 1)
        if fx is not None:
            fx = torch.cat((x, fx), -1)
            fx = self.preprocess(fx)
        else:
            fx = self.preprocess(x)
        fx = fx + self.placeholder[None, None, :]

        if T is not None:
            Time_emb = timestep_embedding(T, self.n_hidden).repeat(1, x.shape[1], 1)
            Time_emb = self.time_fc(Time_emb)
            fx = fx + Time_emb

        for block in self.blocks:
            fx = block(fx)
        return fx

    def unstructured_geo(self, x, fx=None, T=None):
        if fx is not None:
            fx = torch.cat((x, fx), -1)
            fx = self.preprocess(fx)
        else:
            fx = self.preprocess(x)
        fx = fx + self.placeholder[None, None, :]

        if T is not None:
            Time_emb = timestep_embedding(T, self.n_hidden).repeat(1, x.shape[1], 1)
            Time_emb = self.time_fc(Time_emb)
            fx = fx + Time_emb

        for block in self.blocks:
            fx = block(fx)
        return fx

    def forward(self, x, fx=None, T=None, geo=None):
        x = x.view(x.shape[0], -1, x.shape[-1])
        if self.geotype == 'unstructured':
            return self.unstructured_geo(x, fx, T)
        else:
            return self.structured_geo(x, fx, T)
</file>

<file path="models/unet/__init__.py">
from .unet1d import UNet1d
from .unet2d import UNet2d
from .unet3d import UNet3d
</file>

<file path="models/unet/unet1d.py">
# models/unet/unet1d.py
from typing import Any
from collections import OrderedDict

import torch
import torch.nn as nn


class UNet1d(nn.Module):
    """
    1D U-Net backbone for PDE / sequence forecasting.

    Assumptions:
      - Input:  x of shape (B, N, C_in)
      - Output: y of shape (B, N, C_out)
      - N must be divisible by 16 (4 levels of 2× downsampling).

    Architecture:
      enc1 -> enc2 -> enc3 -> enc4 -> bottleneck -> dec4 -> dec3 -> dec2 -> dec1.
    """

    def __init__(
        self,
        model_params: dict,
        **kwargs: Any,
    ) -> None:
        super().__init__()

        # ---- core channel configuration ----
        init_features: int = model_params.get("init_features", 32)
        in_channels: int = model_params.get("in_channels", 3)
        out_channels: int = model_params.get("out_channels", 1)

        # ---- normalization & activation configuration ----
        # norm: "batch" | "group" | "none"
        # activation: "relu" | "gelu" | "tanh"
        self.norm_type: str = model_params.get("norm", "batch")
        self.act_type: str = model_params.get("activation", "relu")

        features = init_features

        # Encoder path
        self.encoder1 = self._block(in_channels, features, name="enc1")
        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)

        self.encoder2 = self._block(features, features * 2, name="enc2")
        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)

        self.encoder3 = self._block(features * 2, features * 4, name="enc3")
        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)

        self.encoder4 = self._block(features * 4, features * 8, name="enc4")
        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)

        # Bottleneck
        self.bottleneck = self._block(features * 8, features * 16, name="bottleneck")

        # Decoder path
        self.upconv4 = nn.ConvTranspose1d(
            features * 16, features * 8, kernel_size=2, stride=2
        )
        self.decoder4 = self._block(features * 8 * 2, features * 8, name="dec4")

        self.upconv3 = nn.ConvTranspose1d(
            features * 8, features * 4, kernel_size=2, stride=2
        )
        self.decoder3 = self._block(features * 4 * 2, features * 4, name="dec3")

        self.upconv2 = nn.ConvTranspose1d(
            features * 4, features * 2, kernel_size=2, stride=2
        )
        self.decoder2 = self._block(features * 2 * 2, features * 2, name="dec2")

        self.upconv1 = nn.ConvTranspose1d(
            features * 2, features, kernel_size=2, stride=2
        )
        self.decoder1 = self._block(features * 2, features, name="dec1")

        # Final 1×1 conv to target channels
        self.conv = nn.Conv1d(
            in_channels=features, out_channels=out_channels, kernel_size=1
        )

        # Initialize weights
        self.apply(self._init_weights)

    # ------------------------------------------------------------------
    # Helper factories for normalization and activation
    # ------------------------------------------------------------------
    def _make_norm(self, num_features: int) -> nn.Module:
        """Create a 1D normalization layer according to self.norm_type."""
        norm = self.norm_type.lower()
        if norm == "batch":
            return nn.BatchNorm1d(num_features)
        if norm == "group":
            # Choose a reasonable number of groups that divides num_features
            num_groups = min(8, num_features)
            while num_groups > 1 and (num_features % num_groups != 0):
                num_groups -= 1
            if num_groups <= 1:
                return nn.Identity()
            return nn.GroupNorm(num_groups=num_groups, num_channels=num_features)
        if norm in ("none", "identity"):
            return nn.Identity()
        raise ValueError(f"Unsupported norm type: {self.norm_type}")

    def _make_act(self) -> nn.Module:
        """Create an activation layer according to self.act_type."""
        act = self.act_type.lower()
        if act == "relu":
            return nn.ReLU(inplace=True)
        if act == "gelu":
            return nn.GELU()
        if act == "tanh":
            return nn.Tanh()
        raise ValueError(f"Unsupported activation type: {self.act_type}")

    # ------------------------------------------------------------------
    # Building block: two Conv1d layers with norm + activation
    # ------------------------------------------------------------------
    def _block(
        self,
        in_channels: int,
        features: int,
        name: str,
    ) -> nn.Sequential:
        """
        1D U-Net block:
            Conv1d -> Norm -> Act -> Conv1d -> Norm -> Act
        """
        return nn.Sequential(
            OrderedDict(
                [
                    (
                        name + "_conv1",
                        nn.Conv1d(
                            in_channels=in_channels,
                            out_channels=features,
                            kernel_size=3,
                            padding=1,
                            bias=False,
                        ),
                    ),
                    (name + "_norm1", self._make_norm(features)),
                    (name + "_act1", self._make_act()),
                    (
                        name + "_conv2",
                        nn.Conv1d(
                            in_channels=features,
                            out_channels=features,
                            kernel_size=3,
                            padding=1,
                            bias=False,
                        ),
                    ),
                    (name + "_norm2", self._make_norm(features)),
                    (name + "_act2", self._make_act()),
                ]
            )
        )

    # ------------------------------------------------------------------
    # Forward
    # ------------------------------------------------------------------
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Input tensor of shape (B, N, C_in),
               where N is the 1D grid size and C_in are channels/features.

        Returns:
            Tensor of shape (B, N, C_out).
        """
        assert x.dim() == 3, f"Expected (B, N, C), got shape {x.shape}"
        B, N, C_in = x.shape

        # 4 levels of 2x pooling → N / 16 must be integer
        assert N % 16 == 0, (
            f"N must be divisible by 16 for this UNet configuration, got N={N}."
        )

        # (B, N, C_in) -> (B, C_in, N)
        x = x.permute(0, 2, 1).contiguous()

        # Encoder
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(self.pool1(enc1))
        enc3 = self.encoder3(self.pool2(enc2))
        enc4 = self.encoder4(self.pool3(enc3))

        # Bottleneck
        bottleneck = self.bottleneck(self.pool4(enc4))

        # Decoder with skip connections
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        dec4 = self.decoder4(dec4)

        dec3 = self.upconv3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        dec3 = self.decoder3(dec3)

        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.decoder2(dec2)

        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.decoder1(dec1)

        out = self.conv(dec1)  # (B, C_out, N)

        # (B, C_out, N) -> (B, N, C_out)
        out = out.permute(0, 2, 1).contiguous()
        return out

    # ------------------------------------------------------------------
    # Initialization
    # ------------------------------------------------------------------
    def _init_weights(self, m: nn.Module) -> None:
        """Kaiming initialization for Conv1d and reasonable defaults for Norm."""
        if isinstance(m, nn.Conv1d):
            nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            if m.bias is not None:
                nn.init.constant_(m.bias, 0.0)
        elif isinstance(m, (nn.BatchNorm1d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias, 0.0)
</file>

<file path="models/unet/unet2d.py">
# models/unet/unet2d.py
from typing import Any
from collections import OrderedDict

import torch
import torch.nn as nn


class UNet2d(nn.Module):
    """
    2D U-Net backbone for PDE forecasting.

    This implementation assumes:
      - Input:  x of shape (B, H, W, C_in)
      - Output: y of shape (B, H, W, C_out)
      - Spatial sizes H and W must be divisible by 16 (4 levels of pooling).

    The architecture follows the classic 4-level U-Net:
      enc1 -> enc2 -> enc3 -> enc4 -> bottleneck -> dec4 -> dec3 -> dec2 -> dec1.
    """

    def __init__(
        self,
        model_params: dict,
        **kwargs: Any,
    ) -> None:
        super().__init__()

        # ---- core channel configuration ----
        init_features: int = model_params.get("init_features", 32)
        in_channels: int = model_params.get("in_channels", 3)
        out_channels: int = model_params.get("out_channels", 1)

        # ---- normalization & activation configuration ----
        # norm: "batch" | "group" | "none"
        # activation: "relu" | "gelu" | "tanh"
        self.norm_type: str = model_params.get("norm", "batch")
        self.act_type: str = model_params.get("activation", "relu")

        features = init_features

        # Encoder path
        self.encoder1 = self._block(in_channels, features, name="enc1")
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.encoder2 = self._block(features, features * 2, name="enc2")
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.encoder3 = self._block(features * 2, features * 4, name="enc3")
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.encoder4 = self._block(features * 4, features * 8, name="enc4")
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Bottleneck
        self.bottleneck = self._block(features * 8, features * 16, name="bottleneck")

        # Decoder path
        self.upconv4 = nn.ConvTranspose2d(
            features * 16, features * 8, kernel_size=2, stride=2
        )
        self.decoder4 = self._block(features * 8 * 2, features * 8, name="dec4")

        self.upconv3 = nn.ConvTranspose2d(
            features * 8, features * 4, kernel_size=2, stride=2
        )
        self.decoder3 = self._block(features * 4 * 2, features * 4, name="dec3")

        self.upconv2 = nn.ConvTranspose2d(
            features * 4, features * 2, kernel_size=2, stride=2
        )
        self.decoder2 = self._block(features * 2 * 2, features * 2, name="dec2")

        self.upconv1 = nn.ConvTranspose2d(
            features * 2, features, kernel_size=2, stride=2
        )
        self.decoder1 = self._block(features * 2, features, name="dec1")

        # Final 1x1 conv to target channels
        self.conv = nn.Conv2d(
            in_channels=features, out_channels=out_channels, kernel_size=1
        )

        # Initialize weights
        self.apply(self._init_weights)

    # ------------------------------------------------------------------
    # Helper factories for normalization and activation
    # ------------------------------------------------------------------
    def _make_norm(self, num_features: int) -> nn.Module:
        """Create a normalization layer according to self.norm_type."""
        if self.norm_type.lower() == "batch":
            return nn.BatchNorm2d(num_features)
        if self.norm_type.lower() == "group":
            # Choose a reasonable number of groups that divides num_features
            num_groups = min(8, num_features)
            while num_groups > 1 and (num_features % num_groups != 0):
                num_groups -= 1
            if num_groups <= 1:
                # Fallback to Identity if we cannot find a valid group count
                return nn.Identity()
            return nn.GroupNorm(num_groups=num_groups, num_channels=num_features)
        if self.norm_type.lower() in ("none", "identity"):
            return nn.Identity()
        raise ValueError(f"Unsupported norm type: {self.norm_type}")

    def _make_act(self) -> nn.Module:
        """Create an activation layer according to self.act_type."""
        act = self.act_type.lower()
        if act == "relu":
            return nn.ReLU(inplace=True)
        if act == "gelu":
            return nn.GELU()
        if act == "tanh":
            return nn.Tanh()
        raise ValueError(f"Unsupported activation type: {self.act_type}")

    # ------------------------------------------------------------------
    # Building block: two conv layers with norm + activation
    # ------------------------------------------------------------------
    def _block(
        self,
        in_channels: int,
        features: int,
        name: str,
    ) -> nn.Sequential:
        """
        Build a simple conv block:
            Conv2d -> Norm -> Act -> Conv2d -> Norm -> Act
        """
        return nn.Sequential(
            OrderedDict(
                [
                    (
                        name + "_conv1",
                        nn.Conv2d(
                            in_channels=in_channels,
                            out_channels=features,
                            kernel_size=3,
                            padding=1,
                            bias=False,
                        ),
                    ),
                    (name + "_norm1", self._make_norm(features)),
                    (name + "_act1", self._make_act()),
                    (
                        name + "_conv2",
                        nn.Conv2d(
                            in_channels=features,
                            out_channels=features,
                            kernel_size=3,
                            padding=1,
                            bias=False,
                        ),
                    ),
                    (name + "_norm2", self._make_norm(features)),
                    (name + "_act2", self._make_act()),
                ]
            )
        )

    # ------------------------------------------------------------------
    # Forward
    # ------------------------------------------------------------------
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Input tensor of shape (B, H, W, C_in).

        Returns:
            Output tensor of shape (B, H, W, C_out).
        """
        assert x.dim() == 4, f"Expected (B, H, W, C), got shape {x.shape}"
        B, H, W, C = x.shape

        # 4 levels of 2x2 pooling → H/16, W/16 must be integer
        assert H % 16 == 0 and W % 16 == 0, (
            f"H and W must be divisible by 16 for this UNet configuration, got H={H}, W={W}."
        )

        # (B, H, W, C) -> (B, C, H, W)
        x = x.permute(0, 3, 1, 2).contiguous()

        # Encoder
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(self.pool1(enc1))
        enc3 = self.encoder3(self.pool2(enc2))
        enc4 = self.encoder4(self.pool3(enc3))

        # Bottleneck
        bottleneck = self.bottleneck(self.pool4(enc4))

        # Decoder with skip connections
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        dec4 = self.decoder4(dec4)

        dec3 = self.upconv3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        dec3 = self.decoder3(dec3)

        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.decoder2(dec2)

        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.decoder1(dec1)

        out = self.conv(dec1)  # (B, C_out, H, W)

        # (B, C_out, H, W) -> (B, H, W, C_out)
        out = out.permute(0, 2, 3, 1).contiguous()
        return out

    # ------------------------------------------------------------------
    # Initialization
    # ------------------------------------------------------------------
    def _init_weights(self, m: nn.Module) -> None:
        """Kaiming initialization for conv and reasonable defaults for norm."""
        if isinstance(m, nn.Conv2d):
            # fan_out mode is standard for Conv2d + ReLU-like activations
            nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            if m.bias is not None:
                nn.init.constant_(m.bias, 0.0)
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias, 0.0)
</file>

<file path="models/unet/unet3d.py">
# models/unet/unet3d.py
from typing import Any
from collections import OrderedDict

import torch
import torch.nn as nn


class UNet3d(nn.Module):
    """
    3D U-Net backbone for PDE / volumetric forecasting.

    Assumptions:
      - Input:  x of shape (B, D, H, W, C_in)
      - Output: y of shape (B, D, H, W, C_out)
      - D, H, W must be divisible by 16 (4 levels of 2× downsampling).

    Architecture:
      enc1 -> enc2 -> enc3 -> enc4 -> bottleneck -> dec4 -> dec3 -> dec2 -> dec1.
    """

    def __init__(
        self,
        model_params: dict,
        **kwargs: Any,
    ) -> None:
        super().__init__()

        # ---- core channel configuration ----
        init_features: int = model_params.get("init_features", 32)
        in_channels: int = model_params.get("in_channels", 3)
        out_channels: int = model_params.get("out_channels", 1)

        # ---- normalization & activation configuration ----
        # norm: "batch" | "group" | "none"
        # activation: "relu" | "gelu" | "tanh"
        self.norm_type: str = model_params.get("norm", "batch")
        self.act_type: str = model_params.get("activation", "relu")

        features = init_features

        # Encoder path
        self.encoder1 = self._block(in_channels, features, name="enc1")
        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.encoder2 = self._block(features, features * 2, name="enc2")
        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.encoder3 = self._block(features * 2, features * 4, name="enc3")
        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.encoder4 = self._block(features * 4, features * 8, name="enc4")
        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2)

        # Bottleneck
        self.bottleneck = self._block(features * 8, features * 16, name="bottleneck")

        # Decoder path
        self.upconv4 = nn.ConvTranspose3d(
            features * 16, features * 8, kernel_size=2, stride=2
        )
        self.decoder4 = self._block(features * 8 * 2, features * 8, name="dec4")

        self.upconv3 = nn.ConvTranspose3d(
            features * 8, features * 4, kernel_size=2, stride=2
        )
        self.decoder3 = self._block(features * 4 * 2, features * 4, name="dec3")

        self.upconv2 = nn.ConvTranspose3d(
            features * 4, features * 2, kernel_size=2, stride=2
        )
        self.decoder2 = self._block(features * 2 * 2, features * 2, name="dec2")

        self.upconv1 = nn.ConvTranspose3d(
            features * 2, features, kernel_size=2, stride=2
        )
        self.decoder1 = self._block(features * 2, features, name="dec1")

        # Final 1×1×1 conv to target channels
        self.conv = nn.Conv3d(
            in_channels=features, out_channels=out_channels, kernel_size=1
        )

        # Initialize weights
        self.apply(self._init_weights)

    # ------------------------------------------------------------------
    # Helper factories for normalization and activation
    # ------------------------------------------------------------------
    def _make_norm(self, num_features: int) -> nn.Module:
        """Create a 3D normalization layer according to self.norm_type."""
        norm = self.norm_type.lower()
        if norm == "batch":
            return nn.BatchNorm3d(num_features)
        if norm == "group":
            # Choose a reasonable number of groups that divides num_features
            num_groups = min(8, num_features)
            while num_groups > 1 and (num_features % num_groups != 0):
                num_groups -= 1
            if num_groups <= 1:
                return nn.Identity()
            return nn.GroupNorm(num_groups=num_groups, num_channels=num_features)
        if norm in ("none", "identity"):
            return nn.Identity()
        raise ValueError(f"Unsupported norm type: {self.norm_type}")

    def _make_act(self) -> nn.Module:
        """Create an activation layer according to self.act_type."""
        act = self.act_type.lower()
        if act == "relu":
            return nn.ReLU(inplace=True)
        if act == "gelu":
            return nn.GELU()
        if act == "tanh":
            return nn.Tanh()
        raise ValueError(f"Unsupported activation type: {self.act_type}")

    # ------------------------------------------------------------------
    # Building block: two Conv3d layers with norm + activation
    # ------------------------------------------------------------------
    def _block(
        self,
        in_channels: int,
        features: int,
        name: str,
    ) -> nn.Sequential:
        """
        3D U-Net block:
            Conv3d -> Norm -> Act -> Conv3d -> Norm -> Act
        """
        return nn.Sequential(
            OrderedDict(
                [
                    (
                        name + "_conv1",
                        nn.Conv3d(
                            in_channels=in_channels,
                            out_channels=features,
                            kernel_size=3,
                            padding=1,
                            bias=False,
                        ),
                    ),
                    (name + "_norm1", self._make_norm(features)),
                    (name + "_act1", self._make_act()),
                    (
                        name + "_conv2",
                        nn.Conv3d(
                            in_channels=features,
                            out_channels=features,
                            kernel_size=3,
                            padding=1,
                            bias=False,
                        ),
                    ),
                    (name + "_norm2", self._make_norm(features)),
                    (name + "_act2", self._make_act()),
                ]
            )
        )

    # ------------------------------------------------------------------
    # Forward
    # ------------------------------------------------------------------
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Input tensor of shape (B, D, H, W, C_in).

        Returns:
            Tensor of shape (B, D, H, W, C_out).
        """
        assert x.dim() == 5, f"Expected (B, D, H, W, C), got shape {x.shape}"
        B, D, H, W, C_in = x.shape

        # 4 levels of 2x pooling → D/H/W / 16 must be integer
        assert D % 16 == 0 and H % 16 == 0 and W % 16 == 0, (
            f"D, H, W must be divisible by 16 for this UNet configuration, "
            f"got D={D}, H={H}, W={W}."
        )

        # (B, D, H, W, C_in) -> (B, C_in, D, H, W)
        x = x.permute(0, 4, 1, 2, 3).contiguous()

        # Encoder
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(self.pool1(enc1))
        enc3 = self.encoder3(self.pool2(enc2))
        enc4 = self.encoder4(self.pool3(enc3))

        # Bottleneck
        bottleneck = self.bottleneck(self.pool4(enc4))

        # Decoder with skip connections
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        dec4 = self.decoder4(dec4)

        dec3 = self.upconv3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        dec3 = self.decoder3(dec3)

        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.decoder2(dec2)

        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.decoder1(dec1)

        out = self.conv(dec1)  # (B, C_out, D, H, W)

        # (B, C_out, D, H, W) -> (B, D, H, W, C_out)
        out = out.permute(0, 2, 3, 4, 1).contiguous()
        return out

    # ------------------------------------------------------------------
    # Initialization
    # ------------------------------------------------------------------
    def _init_weights(self, m: nn.Module) -> None:
        """Kaiming initialization for Conv3d and reasonable defaults for Norm."""
        if isinstance(m, nn.Conv3d):
            nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            if m.bias is not None:
                nn.init.constant_(m.bias, 0.0)
        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias, 0.0)
</file>

<file path="template/config/base.yaml">
# =============================
# Generic experiment config
# =============================

model:
  # Key used to select the trainer from _trainer_dict
  name: 'YourTrainerName'

data:
  name: 'YourDatasetName' # Path to your data file or root directory
  data_path: '/path/to/your/data'
  shape: [64, 64]
  train_batchsize: 64
  eval_batchsize: 64
  train_ratio: 0.98
  valid_ratio: 0.01
  test_ratio: 0.01
  subset: false
  subset_ratio: 0.1
  normalize: true
  normalizer_type: 'PGN'      # or 'Standard', 'MinMax', etc.

train:
  seed: 42
  epochs: 500
  patience: -1  # Early stopping patience; -1 means "disable early stopping"
  eval_freq: 10
  saving_best: true      # save best checkpoint on validation metrics
  load_ckpt: false       # whether to resume from a checkpoint
  saving_ckpt: true      # whether to save intermediate checkpoints
  ckpt_freq: 10          # save checkpoint every N epochs
  ckpt_max: 3            # keep at most N recent checkpoints
  # Distributed training flags (DDP)
  distribute: false
  distribute_mode: 'DDP'
  device_ids: [0, 1, 2, 3]

optimize:
  optimizer: 'AdamW'
  lr: 4.0e-5
  weight_decay: 1.0e-4
  # You can add extra optimizer kwargs if needed, e.g.:
  # betas: [0.9, 0.999]
  # eps: 1.0e-8

schedule:
  scheduler: 'StepLR'
  step_size: 100
  gamma: 0.8

log:
  verbose: true           # print progress to stdout
  log: true               # enable logging to file
  log_dir: './logs'       # root directory for logs / checkpoints
  wandb: false
  wandb_project: ''
  wandb_run_name: ''      # if empty, your logger can auto-generate a name
</file>

<file path="template/config/ns2d.yaml">
# =============================
# Generic experiment config
# =============================

model:
  # Key used to select the trainer from _trainer_dict
  name: 'MLP'
  in_dim: 3
  out_dim: 1
  hidden_dims: [64, 64, 64]
  activation: 'gelu'
  dropout: 0.0
  use_residual: True

data:
  name: 'ns2d' # Path to your data file or root directory
  data_path: '/ai/gno/DATA/ns/ns_V1e-5_N1200_T20.mat'
  shape: [64, 64]
  train_batchsize: 64
  eval_batchsize: 64
  train_ratio: 0.8
  valid_ratio: 0.1
  test_ratio: 0.1
  subset: false
  subset_ratio: 0.1
  normalize: true
  normalizer_type: 'PGN'      # or 'Standard', 'MinMax', etc.

train:
  seed: 42
  epochs: 500
  patience: 10  # Early stopping patience; -1 means "disable early stopping"
  eval_freq: 10
  saving_best: true      # save best checkpoint on validation metrics
  load_ckpt: false       # whether to resume from a checkpoint
  saving_ckpt: true      # whether to save intermediate checkpoints
  ckpt_freq: 10          # save checkpoint every N epochs
  ckpt_max: 3            # keep at most N recent checkpoints
  # Distributed training flags (DDP)
  distribute: false
  distribute_mode: 'DDP'
  device_ids: [0, 1, 2, 3]

optimize:
  optimizer: 'AdamW'
  lr: 1.0e-3
  weight_decay: 1.0e-4
  # You can add extra optimizer kwargs if needed, e.g.:
  # betas: [0.9, 0.999]
  # eps: 1.0e-8

schedule:
  scheduler: 'StepLR'
  step_size: 100
  gamma: 0.8

log:
  verbose: true           # print progress to stdout
  log: true               # enable logging to file
  log_dir: './logs'       # root directory for logs / checkpoints
  wandb: false
  wandb_project: ''
  wandb_run_name: ''      # if empty, your logger can auto-generate a name
</file>

<file path="utils/ddp.py">
# utils/ddp.py
import os
from typing import Tuple

import torch
import torch.distributed as dist


def init_distributed(backend: str = "nccl") -> Tuple[bool, int]:
    """
    Initialize distributed training if launched with torchrun.

    This function is safe to call even if the process group is already
    initialized; in that case it will simply infer the local rank.

    Args:
        backend: Backend to use for distributed training (default: "nccl").

    Returns:
        distributed (bool): True if running in DDP (WORLD_SIZE > 1).
        local_rank (int): Local GPU index for this process (0 if not distributed).
    """
    # If the process group is already initialized, just infer local_rank
    if dist.is_available() and dist.is_initialized():
        local_rank_env = os.environ.get("LOCAL_RANK", "0")
        local_rank = int(local_rank_env)
        if torch.cuda.is_available():
            torch.cuda.set_device(local_rank)
        return True, local_rank

    world_size = int(os.environ.get("WORLD_SIZE", "1"))
    if world_size > 1:
        # torchrun sets LOCAL_RANK, RANK, WORLD_SIZE, etc.
        dist.init_process_group(backend=backend, init_method="env://")

        local_rank = int(os.environ.get("LOCAL_RANK", "0"))
        if torch.cuda.is_available():
            torch.cuda.set_device(local_rank)

        return True, local_rank
    else:
        # Single-process / non-distributed mode
        return False, 0


def debug_barrier(tag: str) -> None:
    """
    A small helper for debugging synchronization issues.

    When the process group is initialized, all ranks will:
    - print "reached <tag>" with their rank
    - synchronize at a barrier
    - print "passed <tag>" after all ranks reach the barrier
    """
    if not (dist.is_available() and dist.is_initialized()):
        return

    rank = dist.get_rank()
    print(f"[Rank {rank}] reached {tag}", flush=True)
    dist.barrier()
    print(f"[Rank {rank}] passed {tag}", flush=True)
</file>

<file path="utils/loss.py">
# utils/loss.py
from __future__ import annotations

from time import time
from typing import Callable, Dict, Any, Optional

import torch
import torch.distributed as dist


LOSS_REGISTRY: Dict[str, Callable[..., torch.Tensor]] = {}


def register_loss(name: str) -> Callable[[Callable[..., torch.Tensor]], Callable[..., torch.Tensor]]:
    """
    Decorator to register a loss function.

    Usage:
        @register_loss("l2")
        def l2_loss(pred, target, **batch):
            return torch.mean((pred - target) ** 2)
    """
    def decorator(fn: Callable[..., torch.Tensor]) -> Callable[..., torch.Tensor]:
        if name in LOSS_REGISTRY:
            raise ValueError(f"Loss '{name}' is already registered.")
        LOSS_REGISTRY[name] = fn
        return fn

    return decorator


class CompositeLoss:
    """
    Composite loss that combines multiple named losses.

    Args:
        spec: dict mapping loss names to weights, e.g. {"l1": 1.0, "l2": 0.1}
        with_record: if True, maintain an internal LossRecord.
    """

    def __init__(self, spec: Dict[str, float], with_record: bool = False) -> None:
        self.spec = {k: float(v) for k, v in spec.items()}
        # 'total_loss' + all individual loss names
        self.loss_list = ["total_loss"] + list(self.spec.keys())
        self.record: Optional[LossRecord] = LossRecord(self.loss_list) if with_record else None

    def __call__(
        self,
        pred: torch.Tensor,
        target: torch.Tensor,
        *,
        batch_size: Optional[int] = None,
        **batch: Any,
    ) -> torch.Tensor:
        """
        Compute the weighted sum of all registered losses.

        Args:
            pred: model prediction.
            target: ground truth.
            batch_size: number of samples in this batch (for averaging logs).
            **batch: extra info passed to each loss function (e.g. masks).
        """
        logs: Dict[str, float] = {}
        total = torch.zeros((), dtype=torch.float32, device=pred.device)

        for name, w in self.spec.items():
            if w == 0.0:
                continue
            if name not in LOSS_REGISTRY:
                raise KeyError(f"Loss '{name}' is not registered in LOSS_REGISTRY.")
            fn = LOSS_REGISTRY[name]
            val = fn(pred, target, **batch)  # scalar tensor
            if val.dim() != 0:
                # Ensure each loss returns a scalar
                val = val.mean()
            total = total + w * val
            logs[name] = float(val.detach().item())

        logs["total_loss"] = float(total.detach().item())

        # Update internal record if enabled
        if self.record is not None and batch_size is not None:
            self.record.update(logs, n=batch_size)

        return total  # used for backward

    def reset_record(self) -> None:
        """Reset the internal LossRecord, if any."""
        if self.record is not None:
            self.record = LossRecord(self.loss_list)

    def reduce_record(self, device: Optional[torch.device] = None) -> None:
        """
        All-reduce the internal LossRecord across DDP processes.
        """
        if self.record is not None:
            self.record.dist_reduce(device=device)

    def get_record_dict(self) -> Dict[str, float]:
        """
        Return current averaged losses as a plain dict.
        """
        if self.record is None:
            return {}
        return self.record.to_dict()


class LpLoss(object):
    """
    Lp loss with absolute or relative mode.

    By default __call__ returns relative Lp loss.
    """

    def __init__(self, d: int = 2, p: int = 2, size_average: bool = True, reduction: bool = True) -> None:
        super().__init__()
        assert d > 0 and p > 0

        self.d = d
        self.p = p
        self.reduction = reduction
        self.size_average = size_average

    def abs(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """
        Absolute Lp loss under uniform mesh assumption.
        """
        num_examples = x.size(0)
        # Assume uniform mesh along the first spatial dimension
        h = 1.0 / (x.size(1) - 1.0)

        diff = x.view(num_examples, -1) - y.view(num_examples, -1)
        all_norms = (h ** (self.d / self.p)) * torch.norm(diff, self.p, dim=1)

        if self.reduction:
            return all_norms.mean() if self.size_average else all_norms.sum()
        return all_norms

    def rel(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """
        Relative Lp loss: ||x - y||_p / ||y||_p.
        """
        num_examples = x.size(0)

        x_flat = x.reshape(num_examples, -1)
        y_flat = y.reshape(num_examples, -1)

        diff_norms = torch.norm(x_flat - y_flat, self.p, dim=1)
        y_norms = torch.norm(y_flat, self.p, dim=1)

        y_norms = torch.where(y_norms == 0, torch.ones_like(y_norms), y_norms)
        rel = diff_norms / y_norms

        if self.reduction:
            return rel.mean() if self.size_average else rel.sum()
        return rel

    def __call__(self, x: torch.Tensor, y: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        return self.rel(x, y)


class AverageRecord(object):
    """Keep running sum, count and average for scalar values."""

    def __init__(self) -> None:
        self.avg = 0.0
        self.sum = 0.0
        self.count = 0.0

    def update(self, val: float, n: int = 1) -> None:
        self.sum += float(val) * n
        self.count += n
        if self.count > 0:
            self.avg = self.sum / self.count


class LossRecord:
    """
    Track running averages of multiple named losses.
    """

    def __init__(self, loss_list) -> None:
        self.start_time = time()
        self.loss_list = list(loss_list)
        self.loss_dict = {loss: AverageRecord() for loss in self.loss_list}

    def update(self, update_dict: Dict[str, float], n: int = 1) -> None:
        for key, value in update_dict.items():
            if key not in self.loss_dict:
                # Auto-add unseen keys to keep things flexible
                self.loss_dict[key] = AverageRecord()
                if key not in self.loss_list:
                    self.loss_list.append(key)
            self.loss_dict[key].update(value, n)

    def format_metrics(self) -> str:
        parts = []
        for loss in self.loss_list:
            avg = self.loss_dict[loss].avg
            parts.append(f"{loss}: {avg:.8f}")
        elapsed = time() - self.start_time
        parts.append(f"Time: {elapsed:.2f}s")
        return " | ".join(parts)

    def to_dict(self) -> Dict[str, float]:
        return {loss: self.loss_dict[loss].avg for loss in self.loss_list}

    def dist_reduce(self, device: Optional[torch.device] = None) -> None:
        """
        All-reduce sums and counts for each loss across DDP processes.
        """
        if not (dist.is_available() and dist.is_initialized()):
            return

        if device is None:
            if torch.cuda.is_available():
                device = torch.device("cuda", torch.cuda.current_device())
            else:
                device = torch.device("cpu")

        for loss in self.loss_list:
            rec = self.loss_dict[loss]
            data = torch.tensor([rec.sum, rec.count], dtype=torch.float32, device=device)
            dist.all_reduce(data, op=dist.ReduceOp.SUM)

            global_sum, global_cnt = data.tolist()
            if global_cnt > 0:
                rec.sum = global_sum
                rec.count = global_cnt
                rec.avg = global_sum / global_cnt
            else:
                rec.sum = 0.0
                rec.count = 0.0
                rec.avg = 0.0

    def __str__(self) -> str:
        return self.format_metrics()

    def __repr__(self) -> str:
        if not self.loss_list:
            return "LossRecord(empty)"
        first = self.loss_list[0]
        return f"{first}: {self.loss_dict[first].avg:.8f}"
</file>

<file path="utils/normalizer.py">
# utils/normalizer.py
import torch
from torch import nn
from typing import Optional, Any


class UnitGaussianNormalizer(nn.Module):
    """
    Per-dimension Gaussian normalizer.

    x is typically shaped like:
        - (n_train, n)
        - (n_train, T, n)
        - (n_train, n, T)
    and we compute mean/std along dim=0.
    """

    def __init__(self, x: torch.Tensor, eps: float = 1e-5) -> None:
        super().__init__()
        # Compute statistics along the training batch dimension
        mean = torch.mean(x, dim=0)
        std = torch.std(x, dim=0)

        self.register_buffer("mean", mean)
        self.register_buffer("std", std)
        self.eps = eps

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """
        Normalize x using stored mean/std.
        """
        return (x - self.mean) / (self.std + self.eps)

    def decode(self, x: torch.Tensor, sample_idx: Optional[Any] = None) -> torch.Tensor:
        """
        Inverse normalization.

        Args:
            x: tensor to be denormalized.
            sample_idx: optional indices to select a subset of mean/std.
                        This is kept for compatibility with existing code.
        """
        # Select statistics
        if sample_idx is None:
            std = self.std
            mean = self.mean
        else:
            # sample_idx might be a list/tuple of index tensors or a tensor itself.
            idx0 = sample_idx[0] if isinstance(sample_idx, (list, tuple)) else sample_idx

            if self.mean.dim() == idx0.dim():
                # Case: mean/std indexed directly by batch-like indices
                std = self.std[sample_idx]
                mean = self.mean[sample_idx]
            else:
                # Case: extra leading dimension, e.g. (T, n) vs (batch,)
                std = self.std[:, sample_idx]
                mean = self.mean[:, sample_idx]

        std = (std + self.eps).to(x.device)
        mean = mean.to(x.device)

        shape = x.shape
        B = shape[0]
        C = shape[-1]

        # Flatten all but batch and channel to apply broadcasting
        x_flat = x.view(B, -1, C)

        x_flat = x_flat * std + mean
        x = x_flat.view(*shape)
        return x


class GaussianNormalizer(nn.Module):
    """
    Global Gaussian normalizer using a single scalar mean/std.
    """

    def __init__(self, x: torch.Tensor, eps: float = 1e-5) -> None:
        super().__init__()
        mean = torch.mean(x)
        std = torch.std(x)

        self.register_buffer("mean", mean)
        self.register_buffer("std", std)
        self.eps = eps

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """
        Normalize x using global mean/std.
        """
        return (x - self.mean) / (self.std + self.eps)

    def decode(self, x: torch.Tensor, sample_idx: Optional[Any] = None) -> torch.Tensor:
        """
        Inverse normalization.

        sample_idx is kept for API compatibility and ignored.
        """
        return x * (self.std + self.eps) + self.mean
</file>

<file path="utils/rollout.py">
# utils/rollout.py
from __future__ import annotations
from typing import Any, Optional, Callable
import torch


@torch.no_grad()
def autoregressive_rollout(
    step_fn: Callable[[torch.Tensor], torch.Tensor],
    u0: torch.Tensor,
    u: torch.Tensor,
    steps: int,
    **kwargs: Any,
) -> torch.Tensor:
    """
    Generic autoregressive rollout on (B, N, C) tensors.

    Args:
        step_fn: one-step function u_next = step_fn(u_cur).
        u0: (B, N, C) initial field.
        steps: number of rollout steps.

    Returns:
        seq: (B, steps, N, C)
    """
    b, n, c = u0.shape
    cur = u0
    preds = []
    for i in range(steps):
        kwargs['y'] = u[:, i, :] if u is not None else None
        nxt = step_fn(cur, **kwargs)          # (B, N, C)
        preds.append(nxt.unsqueeze(1))
        cur = nxt
    return torch.cat(preds, dim=1)  # (B, steps, N, C)
</file>

<file path="utils/vis.py">
# utils/vis.py
import numpy as np
import torch

from typing import Optional, Union

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable


def ns2d_vis(raw_x: Union[torch.Tensor, np.ndarray],
             raw_y: Union[torch.Tensor, np.ndarray],
             pred_y: Union[torch.Tensor, np.ndarray], 
             vmin: Optional[float] = None, vmax: Optional[float] = None,
             emin: Optional[float] = None, emax: Optional[float] = None, 
             dpi: Optional[int] = 100, save_path: Optional[str] = None,) -> None:
    """
    Visualize 2D Navier-Stokes prediction results.

    Args:
        raw_x: input tensor of shape (H, W)
        raw_y: ground truth tensor of shape (H, W)
        pred_y: predicted tensor of shape (H, W)
        save_path: if provided, save the figure to this path
        vmin: minimum value for color scale (if None, use min of raw_y and pred_y)
        vmax: maximum value for color scale (if None, use max of raw_y and pred_y)
        emax: maximum absolute error for error color scale (if None, use max abs error)
        dpi: resolution of the saved figure
    """
    if isinstance(raw_x, torch.Tensor):
        raw_x = raw_x.cpu().numpy()
    if isinstance(raw_y, torch.Tensor):
        raw_y = raw_y.cpu().numpy()
    if isinstance(pred_y, torch.Tensor):
        pred_y = pred_y.cpu().numpy()
    
    vmin = vmin if vmin is not None else raw_y.min().item()
    vmax = vmax if vmax is not None else raw_y.max().item()
    
    error_y = np.abs(pred_y - raw_y)
    emax = emax if emax is not None else error_y.max().item()
    emin = emin if emin is not None else error_y.min().item()
    
    fig, axs = plt.subplots(1, 4, figsize=(18, 4), constrained_layout=True, dpi=dpi)
    
    im1 = axs[0].imshow(raw_x[:, :], cmap='viridis')
    axs[0].set_title('Input (x)')
    axs[0].axis('off')
    
    axs[1].imshow(raw_y[:, :], cmap='viridis', vmin=vmin, vmax=vmax)
    axs[1].set_title('Ground Truth (y)')
    axs[1].axis('off')
    
    im2 = axs[2].imshow(pred_y[:, :], cmap='viridis', vmin=vmin, vmax=vmax)
    axs[2].set_title('Prediction (y_pred)')
    axs[2].axis('off')
    
    im3 = axs[3].imshow(error_y[:, :], cmap='inferno', vmin=emin, vmax=emax)
    axs[3].set_title('Absolute Error |y - y_pred|')
    axs[3].axis('off')
    
    for ax, im in [(axs[0], im1), (axs[2], im2), (axs[3], im3)]:
        divider = make_axes_locatable(ax)
        cax = divider.append_axes("right", size="4%", pad=0.04)
        fig.colorbar(im, cax=cax)
    
    if save_path is not None:
        plt.savefig(save_path, dpi=dpi)
        
    plt.show()
</file>

<file path="train_ddp.sh">
#!/usr/bin/env bash
set -euo pipefail

CONFIG_PATH=""

while [[ $# -gt 0 ]]; do
  case "$1" in
    --config)
      CONFIG_PATH="${2:-}"
      shift 2
      ;;
    --config=*)
      CONFIG_PATH="${1#*=}"
      shift 1
      ;;
    *)
      echo "Unknown arg: $1" >&2
      echo "Usage: $0 --config /path/to/config.yaml" >&2
      exit 2
      ;;
  esac
done

if [[ -z "$CONFIG_PATH" ]]; then
  echo "Missing --config" >&2
  echo "Usage: $0 --config /path/to/config.yaml" >&2
  exit 2
fi

CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 main.py --config "$CONFIG_PATH"
</file>

<file path="models/base/__init__.py">
# models/base/__init__.py
from .embedding import unified_pos_embedding, rotary_pos_embedding, rotary_2d_pos_embedding, rotary_3d_pos_embedding, timestep_embedding, RotaryEmbedding1D
from .utils import get_activation

from .mlp import BaseMLP
from .attention import  (
    MultiHeadSelfAttention,
    MultiHeadCrossAttention,
    AxialAttention2D,
    WindowAttention2D,
    ChannelAttention,
    RoPE1DAdapter,
    RoPE2DAdapter,
    RoPE3DAdapter
)

__all__ = [
    "get_activation",
    "unified_pos_embedding",
    "rotary_pos_embedding",
    "rotary_2d_pos_embedding",
    "rotary_3d_pos_embedding",
    "timestep_embedding",
    "RotaryEmbedding1D",
    "BaseMLP",
    "MultiHeadSelfAttention",
    "MultiHeadCrossAttention",
    "AxialAttention2D",
    "WindowAttention2D",
    "ChannelAttention",
    "RoPE1DAdapter",
    "RoPE2DAdapter",
    "RoPE3DAdapter",
]
</file>

<file path="models/mlp/__init__.py">
from .mlp import MLP

__all__ = ["MLP"]
</file>

<file path="models/mlp/mlp.py">
# models/mlp.py
from __future__ import annotations

from typing import Any, Dict, List, Optional

import torch
import torch.nn as nn


def _get_activation(name: str) -> nn.Module:
    name = name.lower()
    if name == "gelu":
        return nn.GELU()
    if name == "relu":
        return nn.ReLU(inplace=True)
    if name == "tanh":
        return nn.Tanh()
    raise ValueError(f"Unsupported activation: {name}")


class MLP(nn.Module):
    """
    Point-wise MLP for PDE fields on arbitrary geometries.

    Expected input:
        x:      (B, N, C_in)        field features (for NS2D: C_in = 1)
        coords: optional geometric coordinates:
                - (N, C_coord)
                - (1, N, C_coord)
                - (B, N, C_coord)

    The model first concatenates features and coords (if enabled), then applies
    a shared MLP to each point independently:

        [field, coords] -> hidden_dims -> out_channels

    All geometry handling is external: dataset provides `geom` and `coords`,
    trainer/forecaster passes them into `forward`.
    """

    def __init__(self, model_params: Dict[str, Any], **kwargs: Any) -> None:
        super().__init__()

        # Core dims
        self.in_channels: int = int(model_params.get("in_channels", 1))
        self.out_channels: int = int(model_params.get("out_channels", 1))

        # Geometry / coordinates
        self.use_coords: bool = bool(model_params.get("use_coords", True))
        self.coord_dim: int = int(model_params.get("coord_dim", 0))  # e.g. 2 for 2D grid, 3 for 3D, etc.

        # MLP structure
        hidden_dims: List[int] = list(model_params.get("hidden_dims", [128, 128, 128]))
        activation_name: str = str(model_params.get("activation", "gelu"))
        self.dropout_p: float = float(model_params.get("dropout", 0.0))
        self.use_residual: bool = bool(model_params.get("use_residual", True))

        act = _get_activation(activation_name)

        in_dim = self.in_channels + (self.coord_dim if self.use_coords else 0)

        dims: List[int] = [in_dim] + hidden_dims + [self.out_channels]
        layers: List[nn.Module] = []

        for i in range(len(dims) - 2):
            layers.append(nn.Linear(dims[i], dims[i + 1]))
            layers.append(act)
            if self.dropout_p > 0.0:
                layers.append(nn.Dropout(self.dropout_p))

        # Last layer without activation
        layers.append(nn.Linear(dims[-2], dims[-1]))

        self.mlp = nn.Sequential(*layers)

    @staticmethod
    def _broadcast_coords(
        coords: torch.Tensor,
        batch_size: int,
    ) -> torch.Tensor:
        """
        Normalize coords shape to (B, N, C_coord).

        Supported input shapes:
            (N, C_coord)
            (1, N, C_coord)
            (B, N, C_coord)

        Returns:
            coords_b: (B, N, C_coord)
        """
        if coords.ndim == 2:
            # (N, C_coord) -> (1, N, C_coord) -> (B, N, C_coord)
            coords = coords.unsqueeze(0)
        if coords.ndim != 3:
            raise ValueError(f"coords must be 2D or 3D, got shape {tuple(coords.shape)}")

        if coords.shape[0] == 1 and batch_size > 1:
            coords = coords.expand(batch_size, -1, -1)
        elif coords.shape[0] != batch_size:
            raise ValueError(
                f"Incompatible coords batch size: coords.shape[0]={coords.shape[0]}, "
                f"expected batch_size={batch_size}."
            )
        return coords

    def forward(
        self,
        x: torch.Tensor,
        coords: Optional[torch.Tensor] = None,
        geom: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> torch.Tensor:
        """
        Args:
            x:      (B, N, C_in)
            coords: optional, (N, C_coord) or (B, N, C_coord)
            geom:   optional geometry dict (not used directly by this MLP,
                    but kept for a unified model interface).

        Returns:
            y: (B, N, out_channels)
        """
        if x.ndim != 3:
            raise ValueError(f"Expected x of shape (B, N, C), got {tuple(x.shape)}")

        b, n, c_in = x.shape
        if c_in != self.in_channels:
            raise ValueError(
                f"Input channels mismatch: got {c_in}, expected {self.in_channels}"
            )

        features = x

        if self.use_coords:
            if coords is None:
                raise ValueError("use_coords=True but coords is None.")
            coords_b = self._broadcast_coords(coords, batch_size=b)  # (B, N, C_coord)
            if coords_b.shape[-1] != self.coord_dim:
                raise ValueError(
                    f"coord_dim mismatch: got {coords_b.shape[-1]}, expected {self.coord_dim}"
                )
            features = torch.cat([features, coords_b], dim=-1)  # (B, N, C_in + C_coord)

        b, n, c = features.shape
        features_flat = features.view(b * n, c)  # (B*N, C_total)

        out_flat = self.mlp(features_flat)       # (B*N, out_channels)
        out = out_flat.view(b, n, self.out_channels)

        # Optional residual connection on the field channel, if shapes match
        if self.use_residual and self.out_channels == self.in_channels:
            out = out + x

        return out
</file>

<file path="datasets/__init__.py">
# datasets/__init__.py
from .ns2d import NS2DDataset


DATASET_REGISTRY = {
    'ns2d': NS2DDataset,
}
</file>

<file path="forecastors/base.py">
# forecastors/base.py
from __future__ import annotations

import os
import inspect
from typing import Any, Dict, Optional, List, Tuple

import yaml
import torch
import torch.utils.data as data

from models import MODEL_REGISTRY
from datasets import DATASET_REGISTRY
from utils import Evaluator, set_seed


def _decode_field(
    norm: Optional[Any],
    u: torch.Tensor,
    shape: Optional[List[int]] = None,
) -> torch.Tensor:
    """
    Decode a field tensor using a normalizer that was fit on (B, P, C).

    shape:
        Spatial shape used when the normalizer was fit, e.g.
        [L], [H, W], [D, H, W]. If None, fall back to norm.decode(u).
    """
    if norm is None or not hasattr(norm, "decode"):
        return u

    if shape is None:
        return norm.decode(u)

    spatial_ndim = len(shape)

    # one-step: (B, *shape, C)
    if u.ndim == spatial_ndim + 2:
        B = u.shape[0]
        C = u.shape[-1]
        P = 1
        for s in shape:
            P *= s
        flat = u.reshape(B, P, C)
        dec = norm.decode(flat)
        return dec.reshape(B, *shape, C)

    # rollout: (B, S, *shape, C)
    if u.ndim == spatial_ndim + 3:
        B = u.shape[0]
        S = u.shape[1]
        C = u.shape[-1]
        P = 1
        for s in shape:
            P *= s
        flat = u.reshape(B * S, P, C)
        dec = norm.decode(flat)
        return dec.reshape(B, S, *shape, C)

    # Fallback: let the normalizer handle it directly
    return norm.decode(u)


class BaseForecaster(object):
    """
    Lightweight inference / evaluation helper for a trained run.

    - Loads config.yaml and best_model.pth from a run directory.
    - Builds model and evaluator.
    - Provides one-step and rollout forecast utilities.

    Geometry-aware:
      - If the dataset exposes `coords` (e.g. (N, d)) and/or `geom` dict,
        they are stored and, when supported by the model, automatically
        passed as `coords=...` / `geom=...` to model.forward().
    """

    def __init__(self, path: str, device: Optional[str] = None) -> None:
        self.saving_path = path

        # ----------------- config -----------------
        args_path = os.path.join(self.saving_path, "config.yaml")
        if not os.path.isfile(args_path):
            raise FileNotFoundError(f"config.yaml not found in {self.saving_path}")
        with open(args_path, "r") as f:
            self.args: Dict[str, Any] = yaml.safe_load(f)

        self.model_args = self.args["model"]
        self.train_args = self.args["train"]
        self.data_args = self.args["data"]

        # spatial shape, e.g. [L], [H, W], [D, H, W]
        self.shape: List[int] = list(self.data_args["shape"])
        self.data_name = self.data_args["name"]

        # ----------------- device & seed -----------------
        seed = self.train_args.get("seed", 42)
        set_seed(seed)

        if device is not None:
            self.device = torch.device(device)
        else:
            default_dev = self.train_args.get(
                "device", "cuda" if torch.cuda.is_available() else "cpu"
            )
            self.device = torch.device(default_dev)

        # ----------------- geometry placeholders -----------------
        self.geom: Optional[Dict[str, Any]] = None
        self.coords: Optional[torch.Tensor] = None

        # Model forward signature flags
        self._model_accepts_coords: bool = False
        self._model_accepts_geom: bool = False
        self._model_accepts_y: bool = False

        # ----------------- model -----------------
        self.model_name = self.model_args["name"]
        self.model = self.build_model()
        self.load_model()
        self.model.to(self.device)

        # Inspect forward signature once
        self._inspect_model_signature()

        # ----------------- evaluator -----------------
        self.build_evaluator()

        # Normalizers / loaders are built on demand
        self.x_normalizer: Optional[Any] = None
        self.y_normalizer: Optional[Any] = None
        self.train_loader: Optional[data.DataLoader] = None
        self.valid_loader: Optional[data.DataLoader] = None
        self.test_loader: Optional[data.DataLoader] = None

    # ------------------------------------------------------------------
    # Model / signature
    # ------------------------------------------------------------------
    def build_model(self, **kwargs: Any) -> torch.nn.Module:
        if self.model_name not in MODEL_REGISTRY:
            raise NotImplementedError(f"Model {self.model_name} not implemented")
        print(f"Building model: {self.model_name}")
        model_cls = MODEL_REGISTRY[self.model_name]
        return model_cls(self.model_args)

    def load_model(self, **kwargs: Any) -> None:
        model_path = os.path.join(self.saving_path, "best_model.pth")
        if os.path.isfile(model_path):
            print(f"=> loading checkpoint '{model_path}'")
            checkpoint = torch.load(model_path, map_location="cpu")
            if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
                state_dict = checkpoint["model_state_dict"]
            else:
                state_dict = checkpoint
            self.model.load_state_dict(state_dict, strict=False)
        else:
            print(f"=> no checkpoint found at '{model_path}'")

    def _inspect_model_signature(self) -> None:
        """
        Record whether model.forward accepts coords / geom / y.
        """
        try:
            sig = inspect.signature(self.model.forward)
        except (TypeError, ValueError):
            self._model_accepts_coords = False
            self._model_accepts_geom = False
            self._model_accepts_y = False
            return

        params = sig.parameters
        self._model_accepts_coords = "coords" in params
        self._model_accepts_geom = "geom" in params
        self._model_accepts_y = "y" in params

        print(
            f"Model forward supports: "
            f"coords={self._model_accepts_coords}, "
            f"geom={self._model_accepts_geom}, "
            f"y={self._model_accepts_y}"
        )

    def _forward_model(
        self,
        x: torch.Tensor,
        y: Optional[torch.Tensor] = None,
        **extra_kwargs: Any,
    ) -> torch.Tensor:
        """
        Unified forward:
          - auto-injects coords / geom / y if model supports them.
        """
        kwargs: Dict[str, Any] = dict(extra_kwargs)

        if self._model_accepts_coords and self.coords is not None:
            B = x.shape[0]
            coords = self.coords.to(self.device)
            if coords.dim() == 2:
                coords_batched = coords.unsqueeze(0).expand(B, -1, -1)
            else:
                coords_batched = coords
            kwargs["coords"] = coords_batched

        if self._model_accepts_geom and self.geom is not None:
            kwargs["geom"] = self.geom

        if self._model_accepts_y and (y is not None):
            kwargs["y"] = y

        return self.model(x, **kwargs)

    # ------------------------------------------------------------------
    # Evaluator / dataset
    # ------------------------------------------------------------------
    def build_evaluator(self) -> None:
        self.evaluator = Evaluator(self.shape)

    def build_data(
        self,
        **kwargs: Any,
    ) -> Tuple[data.DataLoader, data.DataLoader, data.DataLoader, Any, Any]:
        """
        Build dataset and dataloaders.

        Also captures dataset.coords / dataset.geom if available.
        """
        if self.data_name not in DATASET_REGISTRY:
            raise NotImplementedError(f"Dataset {self.data_name} not implemented")

        dataset_cls = DATASET_REGISTRY[self.data_name]
        dataset = dataset_cls(self.data_args, **kwargs)

        # geometry / coordinates
        self.geom = getattr(dataset, "geom", None)
        self.coords = getattr(dataset, "coords", None)
        if self.coords is not None:
            self.coords = self.coords.to(self.device)
        if self.geom is not None:
            print(f"Dataset geometry: {self.geom}")

        train_loader, valid_loader, test_loader, _ = dataset.make_loaders(
            ddp=False,
            rank=0,
            world_size=1,
            drop_last=True,
        )

        # store for convenience
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        self.test_loader = test_loader
        self.x_normalizer = dataset.x_normalizer
        self.y_normalizer = dataset.y_normalizer

        return train_loader, valid_loader, test_loader, dataset.x_normalizer, dataset.y_normalizer

    # ------------------------------------------------------------------
    # Inference (one-step / rollout)
    # ------------------------------------------------------------------
    @torch.no_grad()
    def rollout_inference(
        self,
        x: torch.Tensor,
        y: torch.Tensor,
        **kwargs: Any,
    ) -> torch.Tensor:
        """
        Generic autoregressive rollout.

        Assumes the last channel of x corresponds to the evolving field,
        and any leading channels are static features (e.g. coordinates).
        For each step:
            - concatenate static features (if any) with current field
            - call model
            - update current field
        Shapes:
            x: (B, ..., C_in)
            y: (B, S, ..., C_out)
        """
        if y.ndim < 3:
            raise ValueError(f"rollout_inference expects at least 3D y, got shape={tuple(y.shape)}")

        steps = int(y.shape[1])
        in_dim = x.shape[-1]
        field_dim = y.shape[-1]

        if in_dim < field_dim:
            raise ValueError(
                f"Input last dim {in_dim} < target field dim {field_dim}. "
                "Cannot separate static and dynamic channels."
            )

        static_dim = in_dim - field_dim
        if static_dim > 0:
            static = x[..., :static_dim].contiguous()
            cur = x[..., static_dim:].contiguous()
        else:
            static = None
            cur = x

        preds: List[torch.Tensor] = []
        self.model.eval()

        for s in range(steps):
            if static is not None:
                xin = torch.cat([static, cur], dim=-1)
            else:
                xin = cur

            nxt = self._forward_model(xin, y=None, **kwargs)

            # match the per-step target shape
            target_shape = y[:, s].shape
            nxt = nxt.reshape(target_shape)

            preds.append(nxt)
            cur = nxt

        return torch.stack(preds, dim=1)  # (B, S, ..., field_dim)

    @torch.no_grad()
    def inference(
        self,
        x: torch.Tensor,
        y: torch.Tensor,
        **kwargs: Any,
    ) -> torch.Tensor:
        """
        Dispatch based on target dimensionality:

        - one-step: y shape (B, *shape, C)      -> model(x) reshaped to y
        - rollout:  y shape (B, S, *shape, C)   -> autoregressive rollout
        """
        # one-step
        if y.ndim == len(self.shape) + 2:
            out = self._forward_model(x, y=None, **kwargs)
            return out.reshape(y.shape)

        # rollout
        if y.ndim == len(self.shape) + 3:
            return self.rollout_inference(x, y, **kwargs)

        raise ValueError(f"Unsupported y.ndim={y.ndim} with shape={tuple(y.shape)}")

    # ------------------------------------------------------------------
    # Forecast / metrics
    # ------------------------------------------------------------------
    @torch.no_grad()
    def forecast(
        self,
        loader: data.DataLoader,
        x_normalizer: Optional[Any] = None,
        y_normalizer: Optional[Any] = None,
        return_all: bool = False,
        return_rollout_info: bool = False,
        **kwargs: Any,
    ) -> Any:
        """
        Run forecast over a dataloader and compute metrics.

        If return_rollout_info=True for rollout targets, also return:
            {
              "rollout_steps": int,
              "rmse_per_step": List[float],
              "rmse_rollout_mean": float
            }
        """
        loss_record = self.evaluator.init_record()

        all_x: List[torch.Tensor] = []
        all_y: List[torch.Tensor] = []
        all_y_pred: List[torch.Tensor] = []

        self.model.eval()
        for x, y in loader:
            x = x.to(self.device, non_blocking=True)
            y = y.to(self.device, non_blocking=True)

            y_pred = self.inference(x, y, **kwargs)

            # decode y / y_pred to physical scale
            y_pred_phys = _decode_field(y_normalizer, y_pred, self.shape)
            y_phys = _decode_field(y_normalizer, y, self.shape)

            # decode the input field channel (last channel of x)
            if x_normalizer is not None and hasattr(x_normalizer, "decode"):
                x_u = x[..., -1:].contiguous()
                x_u_phys = _decode_field(x_normalizer, x_u, self.shape)
            else:
                x_u_phys = x[..., -1:].contiguous()

            all_x.append(x_u_phys.detach().cpu())
            all_y.append(y_phys.detach().cpu())
            all_y_pred.append(y_pred_phys.detach().cpu())

        x_all = torch.cat(all_x, dim=0)
        y_all = torch.cat(all_y, dim=0)
        y_pred_all = torch.cat(all_y_pred, dim=0)

        rollout_info: Optional[Dict[str, Any]] = None
        spatial_ndim = len(self.shape)

        # ----------------- metrics -----------------
        # one-step: (B, *shape, C)
        if y_all.ndim == spatial_ndim + 2:
            self.evaluator(y_pred_all, y_all, record=loss_record)

        # rollout: (B, S, *shape, C)
        elif y_all.ndim == spatial_ndim + 3:
            B = y_all.shape[0]
            S = y_all.shape[1]
            C = y_all.shape[-1]

            # reshape to (B*S, *shape, C) for Evaluator
            new_shape = (B * S, *self.shape, C)
            y_flat = y_all.reshape(new_shape)
            y_pred_flat = y_pred_all.reshape(new_shape)

            self.evaluator(y_pred_flat, y_flat, record=loss_record)

            # per-step RMSE on physical scale
            diff = (y_pred_all - y_all) ** 2
            reduce_dims = tuple(d for d in range(diff.ndim) if d != 1)  # keep step dim
            mse_per_step = diff.mean(dim=reduce_dims)  # (S,)
            rmse_per_step = torch.sqrt(mse_per_step.clamp_min(0.0))

            rollout_info = {
                "rollout_steps": int(S),
                "rmse_per_step": [float(v) for v in rmse_per_step.tolist()],
                "rmse_rollout_mean": float(rmse_per_step.mean().item()),
            }

        else:
            raise ValueError(
                f"Unexpected target shape {tuple(y_all.shape)} "
                f"for spatial_ndim={spatial_ndim}"
            )

        print(loss_record)

        # ----------------- return -----------------
        if return_all and return_rollout_info:
            return loss_record, x_all, y_all, y_pred_all, rollout_info

        if return_all:
            return loss_record, x_all, y_all, y_pred_all

        if return_rollout_info:
            return loss_record, rollout_info

        return loss_record
</file>

<file path="README.md">
# NerualFramework

## Usage
### Model
Write your model in directory `models/` and register your model in `models/__init__.py` as follows:
```python
from .your_model import YourModel

ALL_MODELS = {
    ...
    'your_model': YourModel,
}
```
We provide a `BaseModel` class in `models/base.py` for you to inherit.

### Dataset
Write your dataset in directory `datasets/` and register your dataset in `datasets/__init__.py` as follows:
```python
from .your_dataset import YourDataset
```
We provide a `BaseDataset` class in `datasets/base.py` for you to inherit. 

### Trainer
Write your trainer and procedure method in directory `trainers/` and register your trainer in `trainers/__init__.py` as follows:
```python
from .your_trainer import YourTrainer, your_procedure
```
We provide a `BaseTrainer` class and a `base_procedure` method in `trainers/base.py` for you to inherit. 

### Config
Write your config in directory `configs/`.
```shell
python main.py --config configs/your_config.yaml
```
</file>

<file path="utils/helper.py">
# utils/helper.py
import yaml
import torch
import shutil
import logging
import random
import numpy as np

from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Tuple


def set_seed(seed: int) -> None:
    """
    Set random seed for Python, NumPy and PyTorch to improve reproducibility.
    """
    random.seed(seed)
    np.random.seed(seed)

    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

    # Optional, but usually helpful for reproducibility
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def set_device(cuda: bool, device: int) -> torch.device:
    """
    Select computation device.

    Args:
        cuda: Whether to use CUDA if available.
        device: GPU index when cuda is True.

    Returns:
        torch.device: The selected device.
    """
    if cuda and torch.cuda.is_available():
        torch.cuda.set_device(device=device)
        return torch.device("cuda", device)
    return torch.device("cpu")


def load_config(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Load a YAML config file and merge it into the args dict.

    CLI args stay in 'args', and top-level keys from the YAML
    are copied into args, possibly overriding existing keys
    with the same name.
    """
    config_path = args["config"]
    with open(config_path, "r") as stream:
        config = yaml.safe_load(stream) or {}

    # Shallow merge: top-level keys from YAML overwrite/extend args
    for k, v in config.items():
        args[k] = v
    return args


def save_config(args: Dict[str, Any], saving_path: str) -> None:
    """
    Save the full args dict (including merged config) as config.yaml.
    """
    saving_dir = Path(saving_path)
    saving_dir.mkdir(parents=True, exist_ok=True)
    config_file = saving_dir / "config.yaml"

    with config_file.open("w") as f:
        yaml.safe_dump(args, f, sort_keys=False)


def get_dir_path(args: Dict[str, Any], create_dir: bool = True) -> Tuple[str, str]:
    """
    Build a directory path for the current run.

    Directory structure:
        log_dir / dataset / mm_dd / (model + "_HH_MM_SS")

    Returns:
        dir_path (str): full directory path to create/use.
        dir_name (str): short name used for logging / run id.
    """
    model = args["model"]["name"]
    dataset = args["data"]["name"]
    base_path = args["log"]["log_dir"]

    now = datetime.now()
    date_str = now.strftime("%m_%d")
    time_str = now.strftime("_%H_%M_%S")

    dir_name = f"{date_str}_{model}{time_str}"
    dir_path = Path(base_path) / dataset / date_str / f"{model}{time_str}"

    if create_dir:
        dir_path.mkdir(parents=True, exist_ok=True)

    return str(dir_path), dir_name


def set_up_logger(args: Dict[str, Any]) -> Tuple[str, str]:
    """
    Initialize logging to both a file (train.log) and the console.

    Returns:
        log_dir (str): directory where logs are saved.
        dir_name (str): short name for this run.
    """
    log_dir, dir_name = get_dir_path(args)
    log_dir_path = Path(log_dir)
    log_dir_path.mkdir(parents=True, exist_ok=True)
    log_file = log_dir_path / "train.log"

    # Get root logger and reset handlers to avoid duplicate logs
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    formatter = logging.Formatter(
        "%(asctime)s %(levelname)-8s %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # File handler
    fh = logging.FileHandler(log_file)
    fh.setLevel(logging.INFO)
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    # Console handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    logger.info("Saving logs in: %s", log_dir)

    return log_dir, dir_name


def save_code(
    module: Any,
    saving_path: str,
    with_dir: bool = False,
    with_path: bool = False,
) -> None:
    """
    Save source code of a given module (file or directory) into saving_path/code.

    Args:
        module: Python module object OR string path. When with_path=False,
                module is expected to be a module with a __file__ attribute.
        saving_path: Base directory where 'code' folder will be created.
        with_dir: If True, copy the entire directory containing module.__file__;
                  if False, copy only module.__file__.
        with_path: If True, 'module' is treated as a direct path (string or Path).
    """
    code_root = Path(saving_path) / "code"
    code_root.mkdir(parents=True, exist_ok=True)

    # Resolve source path
    if with_path:
        src = Path(str(module))
    else:
        if not hasattr(module, "__file__"):
            raise ValueError("module must have __file__ attribute when with_path=False.")
        src = Path(module.__file__)
        if with_dir:
            src = src.parent

    dst = code_root / src.name

    if src.is_dir():
        # If destination already exists, remove it to avoid copytree errors
        if dst.exists():
            shutil.rmtree(dst)
        shutil.copytree(src, dst)
    else:
        shutil.copy2(src, dst)
</file>

<file path=".gitignore">
__pycache__/
*.pyc
notebook/
configs/
logs/
</file>

<file path="config.py">
# config.py
import argparse

parser = argparse.ArgumentParser(description='PROJECT NAME')
parser.add_argument("--mode", type=str, choices=["train", "test"], default="train")
parser.add_argument("--config", type=str, default="./configs/base.yaml")
</file>

<file path="datasets/base.py">
# datasets/base.py

from __future__ import annotations

import os
import os.path as osp
from abc import ABC
from typing import Any, Dict, Tuple, Optional

import torch
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler


class TensorPairDataset(Dataset):
    """
    Simple tensor pair dataset: (x, y).

    Args:
        x: input tensor of shape (N, ...)
        y: target tensor of shape (N, ...)
        mode: 'train' / 'valid' / 'test' (for logging or debugging only)
    """

    def __init__(self, x: torch.Tensor, y: torch.Tensor, mode: str = "train") -> None:
        self.mode = mode
        self.x = x
        self.y = y

    def __len__(self) -> int:
        return len(self.x)

    def __getitem__(self, idx: int):
        return self.x[idx], self.y[idx]


class BaseDataset(ABC):
    """
    Generic base dataset for supervised learning.

    Responsibilities:
      - Parse common fields from data_args (data_path, ratios, subset, etc.).
      - load_or_process():
          * if cache exists -> load from .pt file
          * else:
              - load_raw_data()
              - split train/valid/test
              - process each split via process_split()
              - save cache
      - wrap (x, y) tensors into TensorPairDataset as train/valid/test.
      - expose:
          * self.train_dataset
          * self.valid_dataset
          * self.test_dataset
          * self.x_normalizer / self.y_normalizer (if any, defined by subclasses)
          * self.geom / self.coords (optional, defined by subclasses)

    Subclasses SHOULD implement:
      - load_raw_data(self, **kwargs) -> Any
      - process_split(self, data_split, mode: str, normalizer=None)
         -> (x, y, normalizer)

    Subclasses MAY override:
      - get_cache_path(self) -> str
      - split_data(self, raw) -> (train_block, valid_block, test_block)
      - make_dataset(self, x, y, mode) -> Dataset
      - save_to_cache / load_from_cache if custom format is needed.
    """

    def __init__(self, data_args: Dict[str, Any], **kwargs: Any) -> None:
        self.data_args = data_args
        self.data_path: str = data_args.get("data_path", "")

        # Split ratios
        self.train_ratio: float = data_args.get("train_ratio", 0.8)
        self.valid_ratio: float = data_args.get("valid_ratio", 0.1)
        self.test_ratio: float = data_args.get("test_ratio", 0.1)

        # Optional subset for quick experiments
        self.subset: bool = data_args.get("subset", False)
        self.subset_ratio: float = data_args.get("subset_ratio", 0.1)

        # Control whether to force re-processing
        self.reprocess: bool = data_args.get("reprocess", False)

        # Cache path
        self.cache_path: str = self.get_cache_path()

        # Normalizers (subclass decides its type and content)
        self.x_normalizer: Optional[Any] = None
        self.y_normalizer: Optional[Any] = None

        # Optional geometry / coordinates (subclass may fill these)
        # Example for grid data:
        #   self.geom = {"dim": 2, "layout": "grid", "spatial_shape": (H, W), ...}
        #   self.coords = torch.Tensor of shape (N_points, d)
        self.geom: Optional[Dict[str, Any]] = None
        self.coords: Optional[torch.Tensor] = None

        # Load or process data
        (
            train_x,
            train_y,
            valid_x,
            valid_y,
            test_x,
            test_y,
            self.x_normalizer,
            self.y_normalizer,
        ) = self.load_or_process(**kwargs)

        # Apply subset if requested
        if self.subset:
            def _apply_subset(
                x: torch.Tensor,
                y: torch.Tensor,
            ) -> Tuple[torch.Tensor, torch.Tensor]:
                n = max(1, int(len(x) * self.subset_ratio))
                return x[:n], y[:n]

            train_x, train_y = _apply_subset(train_x, train_y)
            valid_x, valid_y = _apply_subset(valid_x, valid_y)
            test_x, test_y = _apply_subset(test_x, test_y)

        # Wrap into PyTorch Dataset objects
        self.train_dataset = self.make_dataset(train_x, train_y, mode="train")
        self.valid_dataset = self.make_dataset(valid_x, valid_y, mode="valid")
        self.test_dataset = self.make_dataset(test_x, test_y, mode="test")

    # ------------------------------------------------------------------
    # Hooks for subclasses
    # ------------------------------------------------------------------
    def get_cache_path(self) -> str:
        """
        Build the default cache path for processed data.

        Default: <root>_processed.pt
          e.g. /path/ns.mat -> /path/ns_processed.pt

        Subclasses can override this to encode additional info, e.g.:
          - sample_factor
          - resolution
        """
        if not self.data_path:
            return "dataset_processed.pt"
        root, _ = osp.splitext(self.data_path)
        return root + "_processed.pt"

    def load_raw_data(self, **kwargs: Any) -> Any:
        """
        Subclass MUST implement this.

        Example return types:
          - torch.Tensor of shape (N, ...)
          - list/tuple of samples
        """
        raise NotImplementedError("Subclasses must implement load_raw_data().")

    def split_data(self, raw: Any) -> Tuple[Any, Any, Any]:
        """
        Default split along the first dimension.

        Works for:
          - torch.Tensor with shape (N, ...)
          - numpy array with shape (N, ...)
          - sequence types (list, tuple) with len()

        Subclasses can override if they need a more complex split logic.
        """
        if isinstance(raw, torch.Tensor):
            N = raw.shape[0]
        else:
            # assume __len__ is available
            N = len(raw)

        train_end = int(N * self.train_ratio)
        valid_end = int(N * (self.train_ratio + self.valid_ratio))

        train_block = raw[:train_end]
        valid_block = raw[train_end:valid_end]
        test_block = raw[valid_end:]

        return train_block, valid_block, test_block

    def process_split(
        self,
        data_split: Any,
        mode: str,
        x_normalizer: Optional[Any] = None,
        y_normalizer: Optional[Any] = None,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[Any], Optional[Any]]:
        """
        Subclass SHOULD override this.

        Default behavior:
          - requires subclass implementation
        """
        raise NotImplementedError("Subclasses must implement process_split().")

    def make_dataset(
        self,
        x: torch.Tensor,
        y: torch.Tensor,
        mode: str = "train",
    ) -> TensorPairDataset:
        """
        Subclass can override this if it needs a custom Dataset implementation.
        """
        return TensorPairDataset(x, y, mode=mode)

    def save_to_cache(
        self,
        train_x: torch.Tensor,
        train_y: torch.Tensor,
        valid_x: torch.Tensor,
        valid_y: torch.Tensor,
        test_x: torch.Tensor,
        test_y: torch.Tensor,
        x_normalizer: Optional[Any],
        y_normalizer: Optional[Any],
    ) -> None:
        """
        Save processed tensors, normalizers and optional geometry.
        """
        cache_dir = osp.dirname(self.cache_path)
        if cache_dir:
            os.makedirs(cache_dir, exist_ok=True)

        payload = {
            "train_x": train_x,
            "train_y": train_y,
            "valid_x": valid_x,
            "valid_y": valid_y,
            "test_x": test_x,
            "test_y": test_y,
            "x_normalizer": x_normalizer,
            "y_normalizer": y_normalizer,
            # optional geometry metadata
            "geom": self.geom,
            "coords": self.coords,
        }

        torch.save(payload, self.cache_path)

    def load_from_cache(self) -> Tuple[torch.Tensor, ...]:
        """
        Load processed tensors, normalizers and geometry from cache.
        """
        obj = torch.load(self.cache_path)

        self.geom = obj.get("geom", None)
        self.coords = obj.get("coords", None)

        train_x = obj["train_x"]
        train_y = obj["train_y"]
        valid_x = obj["valid_x"]
        valid_y = obj["valid_y"]
        test_x = obj["test_x"]
        test_y = obj["test_y"]
        x_normalizer = obj.get("x_normalizer", None)
        y_normalizer = obj.get("y_normalizer", None)

        return (
            train_x,
            train_y,
            valid_x,
            valid_y,
            test_x,
            test_y,
            x_normalizer,
            y_normalizer,
        )

    # ------------------------------------------------------------------
    # Internal logic
    # ------------------------------------------------------------------
    def load_or_process(self, **kwargs: Any):
        """
        Main entry:
          - if cache exists and not reprocess -> load_from_cache
          - else:
             * raw = load_raw_data()
             * train/valid/test = split_data(raw)
             * process each via process_split()
             * save_to_cache()
        """
        if osp.exists(self.cache_path) and not self.reprocess:
            print("Loading processed data from", self.cache_path)
            return self.load_from_cache()

        print("Processing data from raw file...")
        raw = self.load_raw_data(**kwargs)
        train_block, valid_block, test_block = self.split_data(raw)

        # Process each split
        train_x, train_y, x_normalizer, y_normalizer = self.process_split(
            train_block, mode="train", x_normalizer=None, y_normalizer=None
        )
        valid_x, valid_y, _, _ = self.process_split(
            valid_block, mode="valid", x_normalizer=x_normalizer, y_normalizer=y_normalizer
        )
        test_x, test_y, _, _ = self.process_split(
            test_block, mode="test", x_normalizer=x_normalizer, y_normalizer=y_normalizer
        )

        self.save_to_cache(
            train_x,
            train_y,
            valid_x,
            valid_y,
            test_x,
            test_y,
            x_normalizer,
            y_normalizer,
        )

        return train_x, train_y, valid_x, valid_y, test_x, test_y, x_normalizer, y_normalizer

    def make_loaders(
        self,
        *,
        ddp: bool = False,
        rank: int = 0,
        world_size: int = 1,
        drop_last: bool = True,
    ) -> Tuple[DataLoader, DataLoader, DataLoader, Optional[DistributedSampler]]:
        """
        Create train/valid/test DataLoaders using self.data_args.

        Args:
            ddp: whether using DistributedDataParallel.
            rank: process rank in DDP.
            world_size: world size in DDP.
            drop_last: whether to drop last batch in training.

        Returns:
            train_loader, valid_loader, test_loader, train_sampler
        """
        train_bs = self.data_args.get("train_batchsize", 10)
        eval_bs = self.data_args.get("eval_batchsize", 10)
        num_workers = self.data_args.get("num_workers", 0)
        pin_memory = self.data_args.get("pin_memory", True)

        # --------- train loader: optional DistributedSampler ---------
        if ddp:
            train_sampler: Optional[DistributedSampler] = DistributedSampler(
                self.train_dataset,
                num_replicas=world_size,
                rank=rank,
                shuffle=True,
                drop_last=drop_last,
            )
            shuffle_train = False
        else:
            train_sampler = None
            shuffle_train = True

        train_loader = DataLoader(
            self.train_dataset,
            batch_size=train_bs,
            shuffle=shuffle_train,
            sampler=train_sampler,
            num_workers=num_workers,
            drop_last=drop_last,
            pin_memory=pin_memory,
        )

        # --------- valid / test loaders: no sampler, no shuffle ---------
        valid_loader = DataLoader(
            self.valid_dataset,
            batch_size=eval_bs,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory,
        )
        test_loader = DataLoader(
            self.test_dataset,
            batch_size=eval_bs,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory,
        )

        return train_loader, valid_loader, test_loader, train_sampler
</file>

<file path="datasets/ns2d.py">
# datasets/ns_2d.py
from __future__ import annotations

from typing import Any, Dict, Optional, Tuple

import os.path as osp
import numpy as np
import torch
import scipy.io as sio
from h5py import File, Dataset as H5Dataset

from .base import BaseDataset
from utils.normalizer import UnitGaussianNormalizer, GaussianNormalizer


class NS2DDataset(BaseDataset):
    """
    2D Navier–Stokes dataset.

    Raw file must contain key 'u':
      - .mat: raw_data['u'] shape (N, H, W, T)
      - .h5:  one of (H, W, T, N) or (T, H, W, N) or (N, H, W, T)

    After loading, we standardize to (N, H, W, T).

    Tasks (data_args['task']):

      - "one_step":
          all splits (train/valid/test) use one-step pairs
          x: (B, N_points, 1), y: (B, N_points, 1).

      - "rollout":
          train split still uses one-step pairs (same as above);
          valid/test can use rollout targets:
            x: (B, N_points, 1)
            y: (B, S, N_points, 1)
          where S = rollout_steps.

    Geometry:

      - self.geom: {"dim": 2, "layout": "grid", "spatial_shape": (H', W'), "sample_factor": sf}
      - self.coords: (H'*W', 2) in [0, 1]^2, order matches flatten layout.
    """

    def __init__(self, data_args: Dict[str, Any], **kwargs: Any) -> None:
        self.sample_factor: int = int(data_args.get("sample_factor", 1))
        self.normalize: bool = bool(data_args.get("normalize", True))
        self.normalizer_type: str = str(data_args.get("normalizer_type", "PGN"))

        self.in_t: int = int(data_args.get("in_t", 5))
        self.out_t: int = int(data_args.get("out_t", 1))
        self.duration: int = int(data_args.get("duration", 10))

        self.task: str = str(data_args.get("task", "one_step"))  # "one_step" | "rollout"

        self.rollout_steps: int = int(data_args.get("rollout_steps", self.duration))
        self.rollout_stride: int = int(data_args.get("rollout_stride", 0))

        super().__init__(data_args, **kwargs)

    def get_cache_path(self) -> str:
        if not self.data_path:
            return "ns2d_processed.pt"

        root, _ = osp.splitext(self.data_path)
        sf = self.sample_factor
        norm_tag = self.normalizer_type if self.normalize else "none"

        task_tag = self.task
        if task_tag == "rollout":
            extra = f"_ro{self.rollout_steps}_rs{self.rollout_stride}"
        else:
            extra = ""

        return (
            f"{root}_sf{sf}"
            f"_it{self.in_t}_ot{self.out_t}_dur{self.duration}"
            f"{extra}_task{task_tag}_norm{norm_tag}_processed.pt"
        )

    def load_raw_data(self, **kwargs: Any) -> torch.Tensor:
        data_path = self.data_path
        if not data_path:
            raise ValueError("data_path is empty.")

        # try .mat
        try:
            raw_data = sio.loadmat(data_path)
            if "u" not in raw_data:
                raise KeyError("Key 'u' not found in .mat file.")
            data = torch.tensor(raw_data["u"], dtype=torch.float32)  # (N,H,W,T)
            if data.ndim != 4:
                raise ValueError(f"Expected 'u' to be 4D in .mat, got {tuple(data.shape)}.")
            return data
        except Exception:
            pass

        # fall back to .h5
        with File(data_path, "r") as raw_data:
            u_node = raw_data.get("u", None)
            if u_node is None:
                raise KeyError("Key 'u' not found in HDF5 file.")
            if not isinstance(u_node, H5Dataset):
                raise TypeError("Expected dataset 'u' in HDF5 file.")
            u_array = np.asarray(u_node[()])

        if u_array.ndim != 4:
            raise ValueError(f"Expected 'u' to be 4D in HDF5, got shape={u_array.shape}.")

        d0, d1, d2, d3 = u_array.shape

        def _is_hw(x: int) -> bool:
            return x in (16, 32, 48, 64, 96, 128, 256) or x >= 16

        def _is_t(x: int) -> bool:
            return 1 <= x <= 256

        def _is_n(x: int) -> bool:
            return x >= 64

        if _is_hw(d0) and _is_hw(d1) and _is_t(d2) and _is_n(d3):
            std = np.transpose(u_array, (3, 0, 1, 2))  # (H,W,T,N)->(N,H,W,T)
        elif _is_t(d0) and _is_hw(d1) and _is_hw(d2) and _is_n(d3):
            std = np.transpose(u_array, (3, 1, 2, 0))  # (T,H,W,N)->(N,H,W,T)
        elif _is_n(d0) and _is_hw(d1) and _is_hw(d2) and _is_t(d3):
            std = u_array  # already (N,H,W,T)
        else:
            raise ValueError(
                f"Unrecognized HDF5 layout for 'u' with shape={u_array.shape}. "
                "Expected one of (H,W,T,N), (T,H,W,N), (N,H,W,T)."
            )

        return torch.tensor(std, dtype=torch.float32)

    def process_split(
        self,
        data_split: torch.Tensor,
        mode: str,
        x_normalizer: Optional[Any] = None,
        y_normalizer: Optional[Any] = None,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[Any], Optional[Any]]:
        """
        Args:
            data_split: (N, H, W, T)
            mode: 'train' | 'valid' | 'test'

        Returns:
            train (any task): x: (B, N_points, 1), y: (B, N_points, 1)
            valid/test, task="one_step":
                x: (B, N_points, 1), y: (B, N_points, 1)
            valid/test, task="rollout":
                x: (B, N_points, 1), y: (B, S, N_points, 1)
        """
        if data_split.ndim != 4:
            raise ValueError(f"Expected (N,H,W,T), got {tuple(data_split.shape)}.")
        if self.out_t <= 0:
            raise ValueError(f"out_t must be > 0, got {self.out_t}.")

        in_t = self.in_t
        out_t = self.out_t
        duration = self.duration

        if in_t < 0:
            raise ValueError(f"in_t must be >= 0, got {in_t}.")
        if duration <= 0:
            raise ValueError(f"duration must be > 0, got {duration}.")

        # train always uses one-step; rollout only used for valid/test
        effective_task = "one_step" if mode == "train" or mode == "valid" else self.task

        # --- temporal windows ---
        T = int(data_split.shape[-1])

        if effective_task == "one_step":
            end_t_task = in_t + duration + out_t
        else:  # rollout for valid/test
            steps = int(self.rollout_steps)
            if steps <= 0:
                raise ValueError(f"rollout_steps must be > 0, got {steps}.")
            end_t_task = in_t + steps * out_t + 1

        if end_t_task > T:
            raise ValueError(f"Temporal window exceeds T: end_t_task={end_t_task} > T={T}.")

        end_t_norm = in_t + duration + out_t
        if end_t_norm > T:
            end_t_norm = end_t_task

        # normalizer window: (N,H,W,Tn)->(N,Tn,H,W)
        data_norm = data_split[..., in_t:end_t_norm].permute(0, 3, 1, 2)
        # task window: (N,H,W,Tt)->(N,Tt,H,W)
        data_task = data_split[..., in_t:end_t_task].permute(0, 3, 1, 2)

        # spatial subsampling
        sf = self.sample_factor
        if sf > 1:
            data_norm = data_norm[:, :, ::sf, ::sf]
            data_task = data_task[:, :, ::sf, ::sf]

        n, t_norm, h, w = data_norm.shape
        _, t_task, _, _ = data_task.shape

        # --- normalization ---
        if self.normalize:
            flat_norm = data_norm.reshape(n * t_norm, -1, 1)

            if mode == "train":
                if self.normalizer_type == "PGN":
                    x_normalizer = UnitGaussianNormalizer(flat_norm)
                    y_normalizer = x_normalizer
                else:
                    x_normalizer = GaussianNormalizer(flat_norm)
                    y_normalizer = x_normalizer
            else:
                if x_normalizer is None or y_normalizer is None:
                    raise RuntimeError("Normalizer is None for non-train split.")

            flat_task = data_task.reshape(n * t_task, -1, 1)
            data_task = x_normalizer.encode(flat_task).reshape(n, t_task, h, w)

        # --- geometry / coords (once) ---
        if self.geom is None or self.coords is None:
            self.geom = {
                "dim": 2,
                "layout": "grid",
                "spatial_shape": (h, w),
                "sample_factor": sf,
            }
            yy = torch.linspace(0.0, 1.0, h, dtype=torch.float32)
            xx = torch.linspace(0.0, 1.0, w, dtype=torch.float32)
            grid_y, grid_x = torch.meshgrid(yy, xx, indexing="ij")
            coords = torch.stack((grid_y, grid_x), dim=-1).view(-1, 2)
            self.coords = coords

        n_points = h * w

        # --- one-step branch (train for all tasks, eval if task="one_step") ---
        if effective_task == "one_step":
            if t_task < duration + out_t:
                raise ValueError(
                    f"Not enough temporal length for one_step: "
                    f"t_task={t_task}, duration={duration}, out_t={out_t}."
                )

            x_u = data_task[:, :duration, :, :]              # (N, duration, H', W')
            y_u = data_task[:, out_t:out_t + duration, :, :] # (N, duration, H', W')

            x_u = x_u.flatten(0, 1)                          # (B, H', W')
            y_u = y_u.flatten(0, 1)                          # (B, H', W')

            B = int(x_u.shape[0])

            x = x_u.view(B, n_points, 1)
            y = y_u.view(B, n_points, 1)

            return x, y, x_normalizer, y_normalizer

        # --- rollout branch (only valid/test when task="rollout") ---
        steps = int(self.rollout_steps)
        max_start = (t_task - 1) - steps * out_t
        if max_start < 0:
            raise ValueError(
                f"Not enough temporal length for rollout: "
                f"t_task={t_task}, steps={steps}, out_t={out_t}."
            )

        if self.rollout_stride and self.rollout_stride > 0:
            starts = list(range(0, max_start + 1, self.rollout_stride))
        else:
            starts = [0]

        xs = []
        ys = []
        for s in starts:
            u0 = data_task[:, s, :, :]  # (N, H', W')
            gt = data_task[:, s + out_t : s + (steps + 1) * out_t : out_t, :, :]  # (N, steps, H', W')
            xs.append(u0)
            ys.append(gt)

        x_seq = torch.cat(xs, dim=0)  # (B', H', W')
        y_seq = torch.cat(ys, dim=0)  # (B', steps, H', W')

        B_total = int(x_seq.shape[0])

        x = x_seq.view(B_total, n_points, 1)
        y = y_seq.view(B_total, steps, n_points, 1)

        return x, y, x_normalizer, y_normalizer
</file>

<file path="utils/__init__.py">
# utils/__init__.py

# ----------------------------------------------------------------------
# helper utilities
# ----------------------------------------------------------------------
from .helper import (
    set_seed,
    set_device,
    load_config,
    save_config,
    get_dir_path,
    set_up_logger,
    save_code,
)

# ----------------------------------------------------------------------
# distributed helpers
# ----------------------------------------------------------------------
from .ddp import (
    init_distributed,
    debug_barrier,
)

# ----------------------------------------------------------------------
# normalizers
# ----------------------------------------------------------------------
from .normalizer import (
    UnitGaussianNormalizer,
    GaussianNormalizer,
)

# ----------------------------------------------------------------------
# loss utilities
# ----------------------------------------------------------------------
from .loss import (
    LOSS_REGISTRY,
    register_loss,
    CompositeLoss,
    LpLoss,
    AverageRecord,
    LossRecord,
)

# ----------------------------------------------------------------------
# metrics utilities
# ----------------------------------------------------------------------
from .metrics import (
    mse,
    rmse,
    psnr,
    ssim,
    METRIC_REGISTRY,
    Evaluator,
)

# ----------------------------------------------------------------------
# Visualization utilities
# ----------------------------------------------------------------------
from .vis import (
    ns2d_vis,
)

# ----------------------------------------------------------------------
# Rollout utilities
# ----------------------------------------------------------------------
from .rollout import (
    autoregressive_rollout,
)

__all__ = [
    # helper
    "set_seed",
    "set_device",
    "load_config",
    "save_config",
    "get_dir_path",
    "set_up_logger",
    "save_code",
    # ddp
    "init_distributed",
    "debug_barrier",
    # normalizer
    "UnitGaussianNormalizer",
    "GaussianNormalizer",
    # loss
    "LOSS_REGISTRY",
    "register_loss",
    "CompositeLoss",
    "LpLoss",
    "AverageRecord",
    "LossRecord",
    # metrics
    "mse",
    "rmse",
    "psnr",
    "ssim",
    "METRIC_REGISTRY",
    "Evaluator",
    # vis
    "ns2d_vis",
    # rollout
    "autoregressive_rollout",
]
</file>

<file path="utils/metrics.py">
# utils/metrics.py
from __future__ import annotations

from typing import Dict, List, Any, Optional, Callable

import torch
import torch.nn.functional as F

from .loss import LossRecord


def _ensure_bnc(x: torch.Tensor) -> torch.Tensor:
    """Ensure tensor is (B, N, C)."""
    if x.ndim == 2:
        # (B, N) -> (B, N, 1)
        return x.unsqueeze(-1)
    if x.ndim == 3:
        return x
    raise ValueError(f"Expected tensor with ndim 2 or 3, got shape={tuple(x.shape)}")


def _bnc_to_bchw(x: torch.Tensor, shape: Optional[List[int]]) -> torch.Tensor:
    """
    Convert (B, N, C) to (B, C, H, W) using spatial shape.

    Only supports 2D grids (len(shape) == 2).
    """
    if shape is None:
        raise ValueError("shape must be provided for 2D SSIM (e.g. [H, W]).")
    if len(shape) != 2:
        raise ValueError(f"SSIM currently supports 2D grids only, got shape={shape}.")

    x = _ensure_bnc(x)  # (B, N, C)
    b, n, c = x.shape
    h, w = shape
    if n != h * w:
        raise ValueError(
            f"shape {shape} implies {h*w} points, but got N={n} in (B, N, C)."
        )

    # (B, N, C) -> (B, H, W, C) -> (B, C, H, W)
    x_hw = x.view(b, h, w, c)
    return x_hw.permute(0, 3, 1, 2).contiguous()


# ----------------------------------------------------------------------
# Metrics
# ----------------------------------------------------------------------


@torch.no_grad()
def mse(
    pred: torch.Tensor,
    target: torch.Tensor,
    shape: Optional[List[int]] = None,  # kept for API compatibility
    **kwargs: Any,
) -> torch.Tensor:
    """
    Mean squared error over all elements.
    Works for any tensor shape (B, ...).
    """
    return F.mse_loss(pred, target, reduction="mean")


@torch.no_grad()
def rmse(
    pred: torch.Tensor,
    target: torch.Tensor,
    shape: Optional[List[int]] = None,
    **kwargs: Any,
) -> torch.Tensor:
    """Root mean squared error."""
    return torch.sqrt(mse(pred, target, shape=shape) + 1e-12)


@torch.no_grad()
def psnr(
    pred: torch.Tensor,
    target: torch.Tensor,
    shape: Optional[List[int]] = None,  # not required, kept for symmetry
    data_range: Optional[float] = None,
    eps: float = 1e-12,
    **kwargs: Any,
) -> torch.Tensor:
    """
    Peak Signal-to-Noise Ratio.

    Works for any shape as long as pred / target are aligned.
    """
    m = mse(pred, target)

    if data_range is None:
        # Use dynamic range from target
        L = (target.max() - target.min()).clamp_min(eps)
    else:
        L = torch.as_tensor(
            data_range, device=pred.device, dtype=pred.dtype
        ).clamp_min(eps)

    return 20.0 * torch.log10(L) - 10.0 * torch.log10(m + eps)


@torch.no_grad()
def ssim(
    pred: torch.Tensor,
    target: torch.Tensor,
    shape: Optional[List[int]] = None,
    data_range: Optional[float] = None,
    K1: float = 0.01,
    K2: float = 0.03,
    eps: float = 1e-12,
    **kwargs: Any,
) -> torch.Tensor:
    """
    Structural Similarity Index (SSIM) with 3x3 average pooling.

    Expects batched 2D grids:
      - inputs: (B, N, C) or (B, H, W, C)
      - shape: [H, W] if inputs are (B, N, C)

    For non-2D data, this metric is not defined.
    """
    # Accept either (B, H, W, C) or (B, N, C)
    if pred.ndim == 4 and target.ndim == 4:
        # Assume BHWC
        b, h, w, c = pred.shape
        pred_chw = pred.permute(0, 3, 1, 2).contiguous()
        target_chw = target.permute(0, 3, 1, 2).contiguous()
    else:
        # Unified representation (B, N, C)
        pred_chw = _bnc_to_bchw(pred, shape)    # (B, C, H, W)
        target_chw = _bnc_to_bchw(target, shape)

    # Dynamic range
    if data_range is None:
        L = (target_chw.max() - target_chw.min()).clamp_min(eps)
    else:
        L = torch.as_tensor(
            data_range, device=pred_chw.device, dtype=pred_chw.dtype
        ).clamp_min(eps)

    C1 = (K1 * L) ** 2
    C2 = (K2 * L) ** 2

    mu_x = F.avg_pool2d(pred_chw, 3, 1, 1)
    mu_y = F.avg_pool2d(target_chw, 3, 1, 1)
    sigma_x = F.avg_pool2d(pred_chw * pred_chw, 3, 1, 1) - mu_x.pow(2)
    sigma_y = F.avg_pool2d(target_chw * target_chw, 3, 1, 1) - mu_y.pow(2)
    sigma_xy = F.avg_pool2d(pred_chw * target_chw, 3, 1, 1) - mu_x * mu_y

    ssim_map = ((2 * mu_x * mu_y + C1) * (2 * sigma_xy + C2)) / (
        (mu_x.pow(2) + mu_y.pow(2) + C1) * (sigma_x + sigma_y + C2) + eps
    )
    return ssim_map.mean()


METRIC_REGISTRY: Dict[str, Callable[..., torch.Tensor]] = {
    "mse": mse,
    "rmse": rmse,
    "psnr": psnr,
    "ssim": ssim,
}


class Evaluator:
    """
    Metric evaluator on unified (B, N, C) fields.

    Args:
        shape: spatial shape; used by metrics that need geometry (e.g. SSIM).
        metrics: custom metric dict; defaults to METRIC_REGISTRY.
        **metric_kwargs: extra keyword args passed to each metric.
    """

    def __init__(
        self,
        shape: Optional[List[int]] = None,
        metrics: Optional[Dict[str, Callable[..., torch.Tensor]]] = None,
        **metric_kwargs: Any,
    ) -> None:
        self.shape = shape
        self.metrics = metrics if metrics is not None else METRIC_REGISTRY
        self.kw = metric_kwargs

    def init_record(self, extra_keys: Optional[List[str]] = None) -> LossRecord:
        keys_raw = (extra_keys or []) + list(self.metrics.keys())
        seen = set()
        keys: List[str] = []
        for k in keys_raw:
            if k not in seen:
                seen.add(k)
                keys.append(k)
        return LossRecord(keys)

    @torch.no_grad()
    def __call__(
        self,
        pred: torch.Tensor,
        target: torch.Tensor,
        record: Optional[LossRecord] = None,
        batch_size: Optional[int] = None,
        **batch: Any,
    ) -> Dict[str, float]:
        """
        Compute all registered metrics.

        Expected default input: (B, N, C).
        For rollout, you can flatten time first, e.g. y -> (B*S, N, C).
        """
        out: Dict[str, float] = {}
        for name, fn in self.metrics.items():
            val = fn(pred, target, shape=self.shape, **self.kw)
            out[name] = float(val.item())

        if record is not None:
            n = batch_size if batch_size is not None else pred.size(0)
            record.update(out, n=n)

        return out
</file>

<file path="main.py">
# main.py
import torch.distributed as dist

from config import parser
from utils import set_up_logger, set_seed, set_device, load_config, save_config, init_distributed
from trainers import TRAINER_REGISTRY


def main():
    # ========= Step 0. Optional: initialize distributed =========
    distributed, local_rank = init_distributed()

    # ========= Step 1. Parse CLI args and load config =========
    args = parser.parse_args()
    args = vars(args)
    args = load_config(args)  # merge CLI args with YAML config

    # Ensure "train" sub-dict exists
    train_cfg = args.setdefault("train", {})
    
    if distributed:
        train_cfg["local_rank"] = local_rank
        train_cfg["world_size"] = dist.get_world_size()
        train_cfg["rank"] = dist.get_rank()
    else:
        train_cfg["local_rank"] = args['train'].get('device_ids', [0])[0]
        train_cfg["world_size"] = 1
        train_cfg["rank"] = args['train'].get('device_ids', [0])[0]

    # ========= Step 2. Setup logger and save config =========
    if distributed:
        # Only rank 0 creates logger and writes config to disk
        if train_cfg["rank"] == 0:
            saving_path, saving_name = set_up_logger(args)
            save_config(args, saving_path)
        else:
            saving_path, saving_name = None, None

        # Broadcast saving_path and saving_name to all ranks
        payload = [saving_path, saving_name]
        dist.broadcast_object_list(payload, src=0)
        saving_path, saving_name = payload
    else:
        # Single-process: just create logger and save config
        saving_path, saving_name = set_up_logger(args)
        save_config(args, saving_path)

    train_cfg["saving_path"] = saving_path
    train_cfg["saving_name"] = saving_name

    # ========= Step 3. Set random seed and (optionally) device =========
    seed = train_cfg.get("seed", 42)
    set_seed(seed)
    set_device(cuda=train_cfg.get("cuda", True), device=train_cfg["local_rank"])

    # ========= Step 4. Build trainer (creates model, dataloader, etc.) =========
    trainer_name = args["model"]["name"]
    trainer_cls = TRAINER_REGISTRY[trainer_name]
    trainer = trainer_cls(args)

    # ========= Step 5. Run training / evaluation loop =========
    trainer.process()

    # ========= Step 6. Clean up distributed environment =========
    if distributed:
        dist.destroy_process_group()


if __name__ == "__main__":
    main()
</file>

<file path="trainers/__init__.py">
# trainers/__init__.py
from .base import BaseTrainer

TRAINER_REGISTRY = {
    'MLP': BaseTrainer,
    'UNet1d': BaseTrainer,
    'UNet2d': BaseTrainer,
    'UNet3d': BaseTrainer,
    'FNO1d': BaseTrainer,
    'FNO2d': BaseTrainer,
    'FNO3d': BaseTrainer,
    'Transformer': BaseTrainer,
    'M2NO2d': BaseTrainer,
    'SwinTransformerV2': BaseTrainer,
    'SwinMLP': BaseTrainer,
    'GalerkinTransformer': BaseTrainer,
    "Transolver": BaseTrainer,
    "GNOT": BaseTrainer,
    "ONO": BaseTrainer,
    "LSM": BaseTrainer,
}

__all__ = ['BaseTrainer', 'TRAINER_REGISTRY']
</file>

<file path="models/__init__.py">
# models/__init__.py
from .mlp import MLP
from .unet import UNet1d, UNet2d, UNet3d
from .transformer import Transformer
from .m2no import M2NO2d
from .swin_transformer import SwinTransformerV2, SwinMLP
from .fno import FNO1d, FNO2d, FNO3d
from .galerkin_transformer import GalerkinTransformer
from .transolver import Transolver
from .gnot import GNOT
from .ono import ONO
from .lsm import LSM


MODEL_REGISTRY = {
    "MLP": MLP,
    "UNet1d": UNet1d,
    "UNet2d": UNet2d,
    "UNet3d": UNet3d,
    "M2NO2d": M2NO2d,
    "FNO1d": FNO1d,
    "FNO2d": FNO2d,
    "FNO3d": FNO3d,
    "Transformer": Transformer,
    "SwinTransformerV2": SwinTransformerV2,
    "SwinMLP": SwinMLP,
    "GalerkinTransformer": GalerkinTransformer,
    "Transolver": Transolver,
    "GNOT": GNOT,
    "ONO": ONO,
    "LSM": LSM,
}

__all__ = [
    "MODEL_REGISTRY", 
    "MLP", "UNet1d", "UNet2d", "UNet3d", 
    "FNO1d", "FNO2d", "FNO3d",
    "Transformer", "M2NO2d", "SwinTransformerV2", "SwinMLP",
    "GalerkinTransformer", "Transolver", "GNOT", "ONO", "LSM"
    ]
</file>

<file path="trainers/base.py">
# trainers/base.py
import os
import logging
from functools import partial
from typing import Any, Dict, Optional

import inspect
import torch
import torch.distributed as dist
import wandb

from torch import nn
from torch.nn.parallel import DistributedDataParallel as DDP

from tqdm import tqdm

from utils import LossRecord, LpLoss, Evaluator, autoregressive_rollout

from models import MODEL_REGISTRY
from datasets import DATASET_REGISTRY


class BaseTrainer:
    """
    Base trainer with single-GPU / DDP support and common training loop.
    """

    def __init__(self, args: Dict[str, Any]) -> None:
        self.args = args
        self.model_args = args["model"]
        self.data_args = args["data"]
        self.optim_args = args["optimize"]
        self.scheduler_args = args["schedule"]
        self.train_args = args["train"]
        self.log_args = args["log"]

        # ------------------------------------------------------------------
        # Distributed setup
        # ------------------------------------------------------------------
        self._setup_distribute()

        # Optional geometry / coordinates from dataset
        self.geom: Optional[Dict[str, Any]] = None
        self.coords: Optional[torch.Tensor] = None

        # Flags describing which extra arguments model.forward accepts
        self._model_accepts_coords: bool = False
        self._model_accepts_geom: bool = False
        self._model_accepts_y: bool = False

        # Logger & wandb
        self.logger = logging.info if self.log_args.get("log", True) else print
        self.wandb = self.log_args.get("wandb", False)

        if self._is_main_process() and self.wandb:
            wandb.init(
                project=self.log_args.get("wandb_project", "default"),
                name=self.train_args.get("saving_name", "experiment"),
                tags=[
                    self.model_args.get("name", "model"),
                    self.data_args.get("name", "dataset"),
                ],
                config=args,
            )

        # ------------------------------------------------------------------
        # Build model / optimizer / scheduler
        # ------------------------------------------------------------------
        self.model_name = self.model_args["name"]
        self.main_log(f"Building {self.model_name} model")
        self.model = self.build_model()
        self.apply_init()

        self.model = self.model.to(self.device)

        # Wrap with DP / DDP if needed
        if self.dp:
            self.device_ids = self.train_args.get(
                "device_ids", list(range(torch.cuda.device_count()))
            )
            self.model = nn.DataParallel(self.model, device_ids=self.device_ids)
            self.main_log(f"Using DataParallel with GPUs: {self.device_ids}")
        elif self.ddp:
            # local_rank/device is set in _setup_distribute
            self.model = DDP(
                self.model,
                device_ids=[self.local_rank],
                output_device=self.local_rank,
            )

        # Ensure all params are contiguous
        for p in self.model.parameters():
            if not p.is_contiguous():
                p.data = p.data.contiguous()

        # Inspect model.forward signature once
        self._inspect_model_signature()

        self.start_epoch = 0
        self.optimizer = self.build_optimizer()
        self.scheduler = self.build_scheduler()

        # Optionally resume from checkpoint
        if self.train_args.get("load_ckpt", False):
            self.load_ckpt(self.train_args["ckpt_path"])

        self.loss_fn = self.build_loss()
        self.evaluator = self.build_evaluator()

        self.main_log(f"Model: {self.model}")
        self.main_log(
            "Model parameters: {:.2f}M".format(
                sum(p.numel() for p in self._unwrap().parameters()) / 1e6
            )
        )
        self.main_log(f"Optimizer: {self.optimizer}")
        self.main_log(f"Scheduler: {self.scheduler}")

        # ------------------------------------------------------------------
        # Data
        # ------------------------------------------------------------------
        self.data = self.data_args["name"]
        self.main_log(f"Loading {self.data} dataset")
        self.build_data()
        self.main_log(f"Train dataset size: {self.train_length}")
        self.main_log(f"Valid dataset size: {self.valid_length}")
        self.main_log(f"Test  dataset size: {self.test_length}")

        # ------------------------------------------------------------------
        # Training hyperparameters
        # ------------------------------------------------------------------
        self.epochs = self.train_args["epochs"]
        self.eval_freq = self.train_args["eval_freq"]
        self.patience = self.train_args["patience"]

        self.saving_best = self.train_args.get("saving_best", True)
        self.saving_ckpt = self.train_args.get("saving_ckpt", False)
        self.ckpt_freq = self.train_args.get("ckpt_freq", 100)
        self.ckpt_max = self.train_args.get("ckpt_max", 5)
        self.saving_path = self.train_args.get("saving_path", None)

    # ----------------------------------------------------------------------
    # Distributed helpers
    # ----------------------------------------------------------------------
    def _setup_distribute(self) -> None:
        """
        Decide whether we are in DDP / DP / single-GPU mode and set self.device.
        """
        self.world_size = self.train_args.get("world_size", 1)
        self.dist_mode = self.train_args.get("distribute_mode", "DDP")

        self.ddp = (
            self.world_size > 1
            and dist.is_available()
            and dist.is_initialized()
            and self.dist_mode == "DDP"
        )
        # DataParallel: single process, multiple GPUs
        self.dp = (
            self.dist_mode == "DP"
            and not self.ddp
            and torch.cuda.is_available()
            and torch.cuda.device_count() > 1
        )
        self.dist = self.ddp or self.dp

        if self.ddp:
            self.local_rank = self.train_args.get("local_rank", 0)
            self.device = torch.device(
                f"cuda:{self.local_rank}" if torch.cuda.is_available() else "cpu"
            )
        else:
            self.local_rank = self.train_args.get("device_ids", [0])[0]
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def _unwrap(self) -> nn.Module:
        """
        Return the underlying model (strip DDP / DataParallel).
        """
        if isinstance(self.model, (DDP, nn.DataParallel)):
            return self.model.module
        return self.model

    def _is_main_process(self) -> bool:
        """
        True only for rank 0 in DDP; always True for DP / single-GPU.
        """
        if not self.dist:
            return True
        if self.dp:  # single process, multiple GPUs
            return True
        # DDP: only rank 0
        return self.local_rank == 0

    # ----------------------------------------------------------------------
    # Model signature inspection / unified forward
    # ----------------------------------------------------------------------
    def _inspect_model_signature(self) -> None:
        """
        Inspect model.forward once and record which extra arguments
        are supported (coords / geom / y).
        """
        base_model = self._unwrap()
        try:
            sig = inspect.signature(base_model.forward)
        except (TypeError, ValueError):
            # Fallback: assume no extra args
            self._model_accepts_coords = False
            self._model_accepts_geom = False
            self._model_accepts_y = False
            return

        params = sig.parameters
        self._model_accepts_coords = "coords" in params
        self._model_accepts_geom = "geom" in params
        self._model_accepts_y = "y" in params

        self.main_log(
            f"Model forward supports: "
            f"coords={self._model_accepts_coords}, "
            f"geom={self._model_accepts_geom}, "
            f"y={self._model_accepts_y}"
        )

    def _forward_model(
        self,
        x: torch.Tensor,
        y: Optional[torch.Tensor] = None,
        **extra_kwargs: Any,
    ) -> torch.Tensor:
        """
        Unified forwarding entry for training / evaluation.

        It automatically injects:
          - coords: batched coordinates if model.forward(coords=...) exists.
          - geom: geometry dict if model.forward(geom=...) exists.
          - y: target tensor if model.forward(y=...) exists.

        Args:
            x:  input tensor (e.g. B x N x C)
            y:  optional target tensor
            extra_kwargs: extra keyword args passed through to model.forward

        Returns:
            Model prediction tensor.
        """
        kwargs: Dict[str, Any] = dict(extra_kwargs)

        # Inject coordinates (if available and requested by the model)
        if self._model_accepts_coords and self.coords is not None:
            B = x.size(0)
            coords = self.coords
            # coords is typically (N_points, d); broadcast to (B, N_points, d)
            if coords.dim() == 2:
                coords_batched = coords.unsqueeze(0).expand(B, -1, -1)
            else:
                coords_batched = coords
            kwargs["coords"] = coords_batched

        # Inject geometry (if requested)
        if self._model_accepts_geom and self.geom is not None:
            kwargs["geom"] = self.geom

        # Inject y if the model wants it (e.g. diffusion-style models)
        if self._model_accepts_y and (y is not None):
            kwargs["y"] = y

        return self.model(x, **kwargs)

    # ----------------------------------------------------------------------
    # Init / build components
    # ----------------------------------------------------------------------
    def get_initializer(self, name: Optional[str]):
        if name is None:
            return None
        if name == "xavier_normal":
            return partial(torch.nn.init.xavier_normal_)
        if name == "kaiming_uniform":
            return partial(torch.nn.init.kaiming_uniform_)
        if name == "kaiming_normal":
            return partial(torch.nn.init.kaiming_normal_)
        raise ValueError(f"Unknown initializer: {name}")

    def apply_init(self) -> None:
        initializer = self.get_initializer(self.train_args.get("initializer", None))
        if initializer is None:
            return

        def init_module(module: nn.Module) -> None:
            weight = getattr(module, "weight", None)
            if isinstance(weight, torch.Tensor) and weight.dim() > 1:
                with torch.no_grad():
                    initializer(weight)

        self._unwrap().apply(init_module)
        self.main_log(f"Apply {self.train_args.get('initializer')} initializer")

    def build_model(self, **kwargs: Any) -> nn.Module:
        if self.model_args["name"] not in MODEL_REGISTRY:
            raise NotImplementedError(
                f"Model {self.model_args['name']} not implemented"
            )
        model_cls = MODEL_REGISTRY[self.model_args["name"]]
        return model_cls(self.model_args)

    def build_optimizer(self, **kwargs: Any):
        opt_name = self.optim_args["optimizer"]
        if opt_name == "Adam":
            return torch.optim.Adam(
                self._unwrap().parameters(),
                lr=self.optim_args["lr"],
                weight_decay=self.optim_args["weight_decay"],
            )
        if opt_name == "SGD":
            return torch.optim.SGD(
                self._unwrap().parameters(),
                lr=self.optim_args["lr"],
                momentum=self.optim_args.get("momentum", 0.9),
                weight_decay=self.optim_args["weight_decay"],
            )
        if opt_name == "AdamW":
            return torch.optim.AdamW(
                self._unwrap().parameters(),
                lr=self.optim_args["lr"],
                weight_decay=self.optim_args["weight_decay"],
            )
        raise NotImplementedError(f"Optimizer {opt_name} not implemented")

    def build_scheduler(self, **kwargs: Any):
        sch_name = self.scheduler_args.get("scheduler", None)
        if sch_name is None:
            return None

        if sch_name == "MultiStepLR":
            return torch.optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=self.scheduler_args["milestones"],
                gamma=self.scheduler_args["gamma"],
            )
        if sch_name == "OneCycleLR":
            return torch.optim.lr_scheduler.OneCycleLR(
                self.optimizer,
                max_lr=self.optim_args["lr"],
                div_factor=self.scheduler_args["div_factor"],
                final_div_factor=self.scheduler_args["final_div_factor"],
                pct_start=self.scheduler_args["pct_start"],
                steps_per_epoch=self.scheduler_args["steps_per_epoch"],
                epochs=self.train_args["epochs"],
            )
        if sch_name == "StepLR":
            return torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.scheduler_args["step_size"],
                gamma=self.scheduler_args["gamma"],
            )

        raise NotImplementedError(f"Scheduler {sch_name} not implemented")

    def build_loss(self, **kwargs: Any):
        # Relative Lp loss, averaged over batch
        return LpLoss(size_average=True)

    def build_evaluator(self):
        # For now we keep using shape from config; if you want, you can
        # manually set data.shape to match geom["spatial_shape"].
        return Evaluator(shape=self.data_args["shape"])

    def build_data(self, **kwargs: Any) -> None:
        if self.data_args["name"] not in DATASET_REGISTRY:
            raise NotImplementedError(
                f"Dataset {self.data_args['name']} not implemented"
            )
        dataset_cls = DATASET_REGISTRY[self.data_args["name"]]
        dataset = dataset_cls(self.data_args)

        # Geometry / coordinates (if dataset provides them)
        self.geom = getattr(dataset, "geom", None)
        self.coords = getattr(dataset, "coords", None)
        if self.coords is not None:
            # Move coordinates to current device once
            self.coords = self.coords.to(self.device)
        else:
            self.main_log("Dataset does not provide coordinates tensor")

        if self.geom is not None:
            self.main_log(f"Dataset geometry: {self.geom}")
        else:
            self.main_log("Dataset does not provide geometry dict")

        self.train_loader, self.valid_loader, self.test_loader, self.train_sampler = dataset.make_loaders(
            ddp=self.ddp,
            rank=self.local_rank,
            world_size=self.world_size,
            drop_last=True,
        )

        self.train_length = len(dataset.train_dataset)
        self.valid_length = len(dataset.valid_dataset)
        self.test_length = len(dataset.test_dataset)

        self.x_normalizer = dataset.x_normalizer
        self.y_normalizer = dataset.y_normalizer

    # ----------------------------------------------------------------------
    # Checkpoint IO
    # ----------------------------------------------------------------------
    def _get_state_dict_cpu(self) -> Dict[str, torch.Tensor]:
        model_to_save = self._unwrap()
        return {k: v.detach().cpu() for k, v in model_to_save.state_dict().items()}

    def save_ckpt(self, epoch: int) -> None:
        if self.saving_path is None:
            return
        os.makedirs(self.saving_path, exist_ok=True)
        state_dict_cpu = self._get_state_dict_cpu()

        state = {
            "epoch": epoch,
            "model_state_dict": state_dict_cpu,
            "optimizer_state_dict": self.optimizer.state_dict(),
        }
        if self.scheduler is not None:
            state["scheduler_state_dict"] = self.scheduler.state_dict()

        ckpt_path = os.path.join(self.saving_path, f"model_epoch_{epoch}.pth")
        torch.save(state, ckpt_path)

        # Keep only the last N checkpoints
        if self.ckpt_max is not None and self.ckpt_max > 0:
            ckpt_list = [
                f
                for f in os.listdir(self.saving_path)
                if f.startswith("model_epoch_") and f.endswith(".pth")
            ]
            if len(ckpt_list) > self.ckpt_max:
                ckpt_list.sort(
                    key=lambda x: int(x.split("_")[-1].split(".")[0])
                )
                os.remove(os.path.join(self.saving_path, ckpt_list[0]))

    def save_model(self, model_path: str) -> None:
        state_dict_cpu = self._get_state_dict_cpu()
        torch.save(state_dict_cpu, model_path)
        self.main_log(f"Save model to {model_path}")

    def load_model(self, model_path: str) -> None:
        state = torch.load(model_path, map_location="cpu")
        self._unwrap().load_state_dict(state)
        self.main_log(f"Load model from {model_path}")

    def load_ckpt(self, ckpt_path: str) -> None:
        """
        Load checkpoint (model + optimizer + scheduler + epoch).
        """
        state = torch.load(ckpt_path, map_location="cpu")

        model_state = state.get("model_state_dict", None)
        if model_state is not None:
            self._unwrap().load_state_dict(model_state)

        if "optimizer_state_dict" in state:
            self.optimizer.load_state_dict(state["optimizer_state_dict"])
            # Move optimizer state tensors to correct device
            for group in self.optimizer.state.values():
                for k, v in group.items():
                    if isinstance(v, torch.Tensor):
                        group[k] = v.to(self.device)

        if "scheduler_state_dict" in state and self.scheduler is not None:
            self.scheduler.load_state_dict(state["scheduler_state_dict"])

        self.start_epoch = state.get("epoch", 0) + 1
        self.main_log(
            f"Load checkpoint from {ckpt_path}, epoch {state.get('epoch', 'N/A')}"
        )

    # ----------------------------------------------------------------------
    # Logging helpers
    # ----------------------------------------------------------------------
    def main_log(self, msg: str) -> None:
        if self._is_main_process():
            self.logger(msg)

    # ----------------------------------------------------------------------
    # Core training loop
    # ----------------------------------------------------------------------
    def process(self, **kwargs: Any) -> None:
        self.main_log("Start training")
        best_epoch = 0
        best_metrics: Optional[Dict[str, float]] = None
        best_path = (
            os.path.join(self.saving_path, "best_model.pth")
            if self.saving_path is not None
            else "best_model.pth"
        )
        counter = 0

        if dist.is_available() and dist.is_initialized():
            dist.barrier()

        bar = (
            tqdm(total=self.epochs - self.start_epoch)
            if self._is_main_process()
            else None
        )

        for epoch in range(self.start_epoch, self.epochs):
            train_loss_record = self.train(epoch)
            self.main_log(
                "Epoch {} | {} | lr: {:.4e}".format(
                    epoch,
                    train_loss_record,
                    self.optimizer.param_groups[0]["lr"],
                )
            )
            if self._is_main_process() and self.wandb:
                wandb.log(
                    {
                        **train_loss_record.to_dict(),
                        "epoch": epoch,
                        "lr": self.optimizer.param_groups[0]["lr"],
                    }
                )

            if self._is_main_process() and self.saving_ckpt and (epoch + 1) % self.ckpt_freq == 0:
                self.save_ckpt(epoch)
                self.main_log(f"Epoch {epoch} | save checkpoint in {self.saving_path}")

            if (epoch + 1) % self.eval_freq == 0:
                valid_loss_record = self.evaluate(split="valid")
                self.main_log(f"Epoch {epoch} | {valid_loss_record}")
                valid_metrics = valid_loss_record.to_dict()

                if self._is_main_process() and self.wandb:
                    wandb.log(
                        {**valid_metrics, "epoch": epoch, "phase": "valid"}
                    )

                if (not best_metrics) or (
                    valid_metrics["valid_loss"] < best_metrics["valid_loss"]
                ):
                    counter = 0
                    best_epoch = epoch
                    best_metrics = valid_metrics
                    if self._is_main_process() and self.saving_best:
                        self.save_model(best_path)
                elif self.patience != -1:
                    counter += 1
                    if counter >= self.patience:
                        self.main_log(f"Early stop at epoch {epoch}")
                        # Early stop across all ranks
                        stop_flag = torch.tensor(
                            [1 if self._is_main_process() else 0],
                            device=self.device,
                            dtype=torch.int32,
                        )
                        if self.ddp and dist.is_initialized():
                            dist.broadcast(stop_flag, src=0)
                        if stop_flag.item() > 0:
                            break

            if self._is_main_process() and bar is not None:
                bar.update(1)

        if self._is_main_process() and bar is not None:
            bar.close()
        self.main_log("Optimization Finished!")

        # No validation ever run -> save final model
        if self._is_main_process() and not best_metrics:
            self.save_model(best_path)

        if self.ddp and dist.is_initialized():
            dist.barrier()

        # Load best model for final evaluation (all ranks)
        self.load_model(best_path)

        valid_loss_record = self.evaluate(split="valid")
        self.main_log(f"Valid metrics: {valid_loss_record}")
        test_loss_record = self.evaluate(split="test")
        self.main_log(f"Test metrics: {test_loss_record}")

        if self._is_main_process() and self.wandb:
            run = wandb.run
            if run is not None:
                run.summary["best_epoch"] = best_epoch
                run.summary.update(test_loss_record.to_dict())
            wandb.finish()

        if self.ddp and dist.is_initialized():
            dist.barrier()

    # ----------------------------------------------------------------------
    # Train / eval
    # ----------------------------------------------------------------------
    def train(self, epoch: int, **kwargs: Any) -> LossRecord:
        loss_record = LossRecord(["train_loss"])

        if self.train_sampler is not None:
            self.train_sampler.set_epoch(epoch)

        self.model.train()
        for x, y in self.train_loader:
            x = x.to(self.device, non_blocking=True)
            y = y.to(self.device, non_blocking=True)

            # Unified forward (auto-inject coords / geom / y)
            y_pred = self._forward_model(x, y)

            loss = self.loss_fn(y_pred, y)  # scalar

            loss_record.update(
                {"train_loss": float(loss.item())}, n=x.size(0)
            )

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        if self.scheduler is not None:
            self.scheduler.step()

        # Global average over DDP
        if self.ddp and dist.is_initialized():
            loss_record.dist_reduce(device=self.device)

        return loss_record

    def _one_step(self, x: torch.Tensor, y: Optional[torch.Tensor] = None, **kwargs: Any) -> torch.Tensor:
        """
        One-step inference: unified forward.
        Override in subclasses if needed.
        """
        y_pred = self._forward_model(x, y, **kwargs)
        return y_pred
    
    def inference(self, x: torch.Tensor, y: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        """
        Default inference: unified forward, reshaped to match y.
        Override in subclasses if needed.
        x: (B, N, C_in)
        y: one-step -> (B, N, C_out)
           rollout -> (B, S, N, C_out)
        """
        if y.ndim == 3: # one_step
            y_pred = self._one_step(x, y, **kwargs)
            return y_pred.reshape_as(y)
        elif y.ndim == 4: # autoregressive rollout
            b, s, n, c = y.shape
            u0 = x[..., -1:].contiguous() # (B, N, C_in)
            seq = autoregressive_rollout(self._one_step, u0, y, steps=s)
            return seq.reshape_as(y)
        
        raise ValueError(f"Unsupported y shape for inference: {tuple(y.shape)}")

    def evaluate(self, split: str = "valid", **kwargs: Any) -> LossRecord:
        if split == "valid":
            eval_loader = self.valid_loader
        elif split == "test":
            eval_loader = self.test_loader
        else:
            raise ValueError("split must be 'valid' or 'test'")

        loss_record = self.evaluator.init_record([f"{split}_loss"])

        all_y = []
        all_y_pred = []

        self.model.eval()
        with torch.no_grad():
            for x, y in eval_loader:
                x = x.to(self.device, non_blocking=True)
                y = y.to(self.device, non_blocking=True)

                y_pred = self.inference(x, y, **kwargs)

                all_y.append(y)
                all_y_pred.append(y_pred)

        y = torch.cat(all_y, dim=0)
        y_pred = torch.cat(all_y_pred, dim=0)

        loss = self.loss_fn(y_pred, y)
        total_samples = y.size(0)
        loss_record.update({f"{split}_loss": float(loss.item())}, n=total_samples)

        if self.y_normalizer is not None and hasattr(self.y_normalizer, "decode"):
            y_pred = self.y_normalizer.decode(y_pred)
            y = self.y_normalizer.decode(y)
        self.evaluator(y_pred, y, record=loss_record, batch_size=total_samples)

        if self.ddp and dist.is_initialized():
            loss_record.dist_reduce(device=self.device)

        return loss_record
</file>

</files>
